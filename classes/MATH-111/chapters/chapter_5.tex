\chapter{Eigenvalues and Eigenvectors}
Some vectors have the special property that when a linear transformation is applied to them, the resulting vector is simply a scalar multiple of the original vector i.e. $A \vec{x} = \lambda \vec{x}$.

\begin{eg}
    Let $A = \begin{bmatrix}
        3 & -2 \\
        1 & 0
    \end{bmatrix}$ and two vectors $\vec{u} = \begin{bmatrix}
        2 \\ 1
    \end{bmatrix}$ and $\vec{v} = \begin{bmatrix}
        1 \\ 1
    \end{bmatrix}$. We can compute:
    \[A \vec{u} = \begin{bmatrix}
        3 & -2 \
        1 & 0
    \end{bmatrix} \begin{bmatrix}
        2 \\ 1
    \end{bmatrix} = \begin{bmatrix}
        4 \\ 2
    \end{bmatrix} = 2 \begin{bmatrix}
        2 \\ 1
    \end{bmatrix} = 2 \vec{u}\]
    and
    \[A \vec{v} = \begin{bmatrix}
        3 & -2 \\
        1 & 0
    \end{bmatrix} \begin{bmatrix}
        1 \\ 1
    \end{bmatrix} = \begin{bmatrix}
        1 \\ 1
    \end{bmatrix} = 1 \begin{bmatrix}
        1 \\ 1
    \end{bmatrix} = 1 \vec{v}\]
    Thus:
    \[
        A^k \vec{u} = 2^k \vec{u} \quad \text{and} \quad A^k \vec{v} = 1^k \vec{v} = \vec{v}
    \]
\end{eg}

\begin{definition}[Eigenvalues and Eigenvectors]
    Let $A$ be an $n \times n$ matrix. A non-zero vector $\vec{x} \in \mathbb{R}^n$ is called an eigenvector of $A$ if there exists a scalar $\lambda \in \mathbb{R}$ such that:
    \[
        A \vec{x} = \lambda \vec{x}
    \]
    The scalar $\lambda$ is called the eigenvalue corresponding to the eigenvector $\vec{x}$.
\end{definition}

\begin{eg}
    Let $A = \begin{bmatrix}
        1 & 6 \\ 5 & 2
    \end{bmatrix}$. Are $\vec{x} = \begin{bmatrix}
        6 \\ -5
    \end{bmatrix}$ and $\vec{y} = \begin{bmatrix}
        3 \\ -2
    \end{bmatrix}$ an eigenvector of $A$? If so, find the corresponding eigenvalue. We compute:
    \[A \vec{x} = \begin{bmatrix}
        1 & 6 \\ 5 & 2
    \end{bmatrix} \begin{bmatrix}
        6 \\ -5
    \end{bmatrix} = \begin{bmatrix}
        -24 \\ 20
    \end{bmatrix} = -4 \begin{bmatrix}
        6 \\ -5
    \end{bmatrix} = -4 \vec{x}\]
    and
    \[A \vec{y} = \begin{bmatrix}
        1 & 6 \\ 5 & 2
    \end{bmatrix} \begin{bmatrix}
        3 \\ -2
    \end{bmatrix} = \begin{bmatrix}
        -9 \\ 11
    \end{bmatrix}\]
    Since $A \vec{x} = -4 \vec{x}$, $\vec{x}$ is an eigenvector of $A$ with corresponding eigenvalue $-4$. However, since $A \vec{y} \neq \lambda \vec{y}$ for any scalar $\lambda$, $\vec{y}$ is not an eigenvector of $A$.
\end{eg}

\begin{eg}
    Let $A = \begin{bmatrix}
        1 & 6 \\ 5 & 2
    \end{bmatrix}$. Is $7$ an eigenvalue of $A$? If so, find a corresponding eigenvector. We compute:
    \[A \vec{v} = 7 \vec{v} \quad \implies \quad \begin{bmatrix}
        1 & 6 \\ 5 & 2
    \end{bmatrix} \vec{v} = 7 \vec{v} \quad \implies \quad \begin{bmatrix}
        1 - 7 & 6 \\ 5 & 2 - 7
    \end{bmatrix} \vec{v} = \begin{bmatrix}
        -6 & 6 \\ 5 & -5
    \end{bmatrix} \vec{v} = \vec{0}\]
    We can row reduce the matrix (because we want to find the kernel of the matrix):
    \[\begin{bmatrix}
        -6 & 6 \\ 5 & -5
    \end{bmatrix} \sim \begin{bmatrix}
        1 & -1 \\ 0 & 0
    \end{bmatrix}\]
    Thus, we have the equation $x_1 - x_2 = 0 \implies x_1 = x_2$. Therefore, any non-zero scalar multiple of the vector $\begin{bmatrix}
        1 \\ 1
    \end{bmatrix}$ is an eigenvector corresponding to the eigenvalue $7$.
\end{eg}
More generally, for a matrix $A$ and a scalar $\lambda$, all of the propositions below are equivalent:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $\lambda$ is an eigenvalue of $A$.
    \item $A \vec{x} = \lambda \vec{x}$ has a non-trivial solution.
    \item $(A - \lambda I_n) \vec{x} = \vec{0}$ has a non-trivial solution.
    \item $A - \lambda I_n$ is not invertible.
    \item $\det(A - \lambda I_n) = 0$
\end{itemize}

\begin{eg}
    Let $A = \begin{bmatrix}
        1 & 6 \\ 5 & 2
    \end{bmatrix}$. Let's find all eigenvalues of $A$. We compute:
    \[\det(A - \lambda I_2) = \det\begin{bmatrix}
        1 - \lambda & 6 \\ 5 & 2 - \lambda
    \end{bmatrix} = (1 - \lambda)(2 - \lambda) - 30 = \lambda^2 - 3 \lambda - 28\]
    Setting this equal to zero, we have:
    \[\lambda^2 - 3 \lambda - 28 = 0 \quad \implies \quad (\lambda - 7)(\lambda + 4) = 0\]
    Thus, the eigenvalues of $A$ are $7$ and $-4$. We can find the eigenvectors corresponding to each eigenvalue as follows: \\
    \textbf{For $\lambda = 7$:}
    \[\begin{bmatrix}
        1 - 7 & 6 \\ 5 & 2 - 7
    \end{bmatrix} = \begin{bmatrix}
        -6 & 6 \\ 5 & -5
    \end{bmatrix} \sim \begin{bmatrix}
        1 & -1 \\ 0 & 0
    \end{bmatrix}\]
    Thus, any non-zero scalar multiple of $\begin{bmatrix}
        1 \\ 1
    \end{bmatrix}$ is an eigenvector corresponding to the eigenvalue $7$. \\
    \textbf{For $\lambda = -4$:}
    \[\begin{bmatrix}
        1 + 4 & 6 \\ 5 & 2 + 4
    \end{bmatrix} = \begin{bmatrix}
        5 & 6 \\ 5 & 6
    \end{bmatrix} \sim \begin{bmatrix}
        1 & \frac{6}{5} \\ 0 & 0
    \end{bmatrix}\]
    Thus, any non-zero scalar multiple of $\begin{bmatrix}
        6 \\ -5
    \end{bmatrix}$ is an eigenvector corresponding to the eigenvalue $-4$.
\end{eg}
\begin{definition}[Eigenspace]
    Let $A$ be an $n \times n$ matrix and let $\lambda$ be an eigenvalue of $A$. The eigenspace of $A$ corresponding to the eigenvalue $\lambda$ is defined as:
    \[
        E_\lambda = \{\vec{x} \in \mathbb{R}^n : A \vec{x} = \lambda \vec{x}\}
    \]
    Equivalently, the eigenspace can be expressed as the kernel of the matrix $A - \lambda I_n$:
    \[
        E_\lambda = \text{ker}(A - \lambda I_n)
    \]
\end{definition}

\begin{eg}
    Let $A = \begin{bmatrix}
        4 & -1 & 6 \\
        2 & 1 & 6 \\
        2 & -1 & 8
    \end{bmatrix}$. Find the eigenvalues of $A$. We compute:
    \[\det(A - \lambda I_3) = \det\begin{bmatrix}
        4 - \lambda & -1 & 6 \\
        2 & 1 - \lambda & 6 \\
        2 & -1 & 8 - \lambda
    \end{bmatrix}\]
    Expanding along the first row, we have:
    \begin{align*}
        \det(A - \lambda I_3) &= (4 - \lambda)(1 - \lambda)(2 - \lambda) - 24  + 6(4 - \lambda) + 2 (8-\lambda) - 12 (1 - \lambda) \\
        &= -\lambda^3 + 13 \lambda^2 - 40 \lambda + 36 \\
        &= -(\lambda - 9)(\lambda - 2)^2
    \end{align*}
    Thus, the eigenvalues of $A$ are $9$ and $2$ (with a multiplicity of $2$). Let's find the eigenspaces corresponding to each eigenvalue: \\
    \textbf{For $\lambda = 9$:}
    \[\begin{bmatrix}
        4 - 9 & -1 & 6 \\
        2 & 1 - 9 & 6 \\
        2 & -1 & 8 - 9
    \end{bmatrix} = \begin{bmatrix}
        -5 & -1 & 6 \\
        2 & -8 & 6 \\
        2 & -1 & -1
    \end{bmatrix} \sim \begin{bmatrix}
        1 & 0 & -1 \\
        0 & 1 & -1 \\
        0 & 0 & 0
    \end{bmatrix}\]
    Thus, any non-zero scalar multiple of $\begin{bmatrix}
        1 \\ 1 \\ 1
    \end{bmatrix}$ is an eigenvector corresponding to the eigenvalue $9$. \\
    \textbf{For $\lambda = 2$:}
    \[\begin{bmatrix}
        4 - 2 & -1 & 6 \\
        2 & 1 - 2 & 6 \\
        2 & -1 & 8 - 2
    \end{bmatrix} = \begin{bmatrix}
        2 & -1 & 6 \\
        2 & -1 & 6 \\
        2 & -1 & 6
    \end{bmatrix} \sim \begin{bmatrix}
        1 & -\frac{1}{2} & 3 \\
        0 & 0 & 0 \\
        0 & 0 & 0
    \end{bmatrix}\]
    Thus, any non-zero scalar multiple of $\begin{bmatrix}
        1 \\ 2 \\ 0
    \end{bmatrix}$ and $\begin{bmatrix}
        3 \\ 0 \\ -1
    \end{bmatrix}$ are eigenvectors corresponding to the eigenvalue $2$.
\end{eg}

\begin{eg}
    Let $A = \begin{bmatrix}
        1 & 4 & 6 \\ 0 & 3 & 9 \\ 0 & 0 & 8
    \end{bmatrix}$. Find the eigenvalues of $A$. We compute:
    \[\det(A - \lambda I_3) = \det\begin{bmatrix}
        1 - \lambda & 4 & 6 \\ 0 & 3 - \lambda & 9 \\ 0 & 0 & 8 - \lambda
    \end{bmatrix} = (1 - \lambda)(3 - \lambda)(8 - \lambda)\]
    Thus, the eigenvalues of $A$ are $1$, $3$, and $8$. Since $A$ is an upper triangular matrix, the eigenvalues are simply the entries on the main diagonal.
\end{eg}

\begin{theorem}
    Let $A$ be an $n \times n$ upper or lower triangular matrix. The eigenvalues of $A$ are the entries on the main diagonal of $A$.
\end{theorem}

\begin{eg}[Fibonacci Sequence]
    The Fibonacci sequence is defined as follows:
    \[
        F_0 = 1, \quad F_1 = 1, \quad F_n = F_{n-1} + F_{n-2} \text{ for } n \geq 2
    \]
    We can express this recurrence relation in matrix form:
    \[
        \begin{bmatrix}
            F_1 \\ F_2
        \end{bmatrix} = \begin{bmatrix}
            F_1 \\ F_0 + F_1
        \end{bmatrix} = \begin{bmatrix}
            0 & 1 \\ 1 & 1
        \end{bmatrix} \begin{bmatrix}
            F_0 \\ F_1
        \end{bmatrix}
    \]
    and:
    \[
        \begin{bmatrix}
            F_2 \\ F_3
        \end{bmatrix} = \begin{bmatrix}
            F_2 \\ F_1 + F_2
        \end{bmatrix} = \begin{bmatrix}
            0 & 1 \\ 1 & 1
        \end{bmatrix} \begin{bmatrix}
            F_1 \\ F_2
        \end{bmatrix} = \begin{bmatrix}
            0 & 1 \\ 1 & 1
        \end{bmatrix}^2 \begin{bmatrix}
            F_0 \\ F_1
        \end{bmatrix}
    \]
    More generally, we have:
    \[\begin{bmatrix}
        F_n \\ F_{n+1}
    \end{bmatrix} = \begin{bmatrix}
        0 & 1 \\ 1 & 1
    \end{bmatrix}^n \begin{bmatrix}
        F_0 \\ F_1
    \end{bmatrix}\]
    Thus we want to compute powers of the matrix $A = \begin{bmatrix}
        0 & 1 \\ 1 & 1
    \end{bmatrix}$. Let's find the eigenvalues of $A$:
    \[\det(A - \lambda I_2) = \det\begin{bmatrix}
        -\lambda & 1 \\ 1 & 1 - \lambda
    \end{bmatrix} = \lambda^2 - \lambda - 1 = 0 \quad \implies \quad (\lambda - \frac{1 + \sqrt{5}}{2})(\lambda - \frac{1 - \sqrt{5}}{2}) = 0\]
    Thus, the eigenvalues of $A$ are $\lambda_1 = \frac{1 + \sqrt{5}}{2} = \phi$ and $\lambda_2 = \frac{1 - \sqrt{5}}{2} = \bar{\phi}$, where $\phi$ is the golden ratio, $\phi = \frac{1 + \sqrt{5}}{2} \approx 1.618$. Let's find the eigenvectors corresponding to each eigenvalue: \\
    \textbf{For $\lambda_1 = \frac{1 + \sqrt{5}}{2}$:}
    \[\begin{bmatrix}
        -\frac{1 + \sqrt{5}}{2} & 1 \\ 1 & 1 - \frac{1 + \sqrt{5}}{2}
    \end{bmatrix} \sim \begin{bmatrix}
        1 & -\frac{1 + \sqrt{5}}{2} \\ 0 & 0
    \end{bmatrix}\]
    Thus, any non-zero scalar multiple of $\begin{bmatrix}
        1 \\ \frac{1 + \sqrt{5}}{2}
    \end{bmatrix} = \begin{bmatrix}
        1 \\ \phi
    \end{bmatrix} = \vec{v_1}$ is an eigenvector corresponding to the eigenvalue $\frac{1 + \sqrt{5}}{2}$. \\
    \textbf{For $\lambda_2 = \frac{1 - \sqrt{5}}{2}$:}
    \[\begin{bmatrix}
        -\frac{1 - \sqrt{5}}{2} & 1 \\ 1 & 1 - \frac{1 - \sqrt{5}}{2}
    \end{bmatrix} \sim \begin{bmatrix}
        1 & -\frac{1 - \sqrt{5}}{2} \\ 0 & 0
    \end{bmatrix}\]
    Thus, any non-zero scalar multiple of $\begin{bmatrix}
        1 \\ \frac{1 - \sqrt{5}}{2}
    \end{bmatrix}$ is an eigenvector corresponding to the eigenvalue $\frac{1 - \sqrt{5}}{2}$. 
    To see the equivalence with $\begin{bmatrix} -\phi \\ 1 \end{bmatrix}$, note that:
    \[
        \begin{bmatrix}
            1 \\ \frac{1 - \sqrt{5}}{2}
        \end{bmatrix}
        = \frac{1}{-\phi} \begin{bmatrix}
            -\phi \\ 1
        \end{bmatrix}
    \]
    where $\phi = \frac{1 + \sqrt{5}}{2}$.
    Therefore, $\vec{v_2} = \begin{bmatrix} -\phi \\ 1 \end{bmatrix}$ is also an eigenvector corresponding to the eigenvalue $\frac{1 - \sqrt{5}}{2}$. \\
    We remark that the eigenvalues are distinct, so the eigenvectors form a basis of $\mathbb{R}^2$. We want to write the initial vector $\begin{bmatrix}
        F_0 \\ F_1
    \end{bmatrix} = \begin{bmatrix}
        1 \\ 1
    \end{bmatrix}$ as a linear combination of the eigenvectors:
    \[
        c_1 \vec{v_1} + c_2 \vec{v_2} = \begin{bmatrix}
            1 \\ 1
        \end{bmatrix} \quad \iff \quad \begin{bmatrix}
            1 & -\phi \\ \phi & 1
        \end{bmatrix} \begin{bmatrix}
            c_1 \\ c_2
        \end{bmatrix} = \begin{bmatrix}
            1 \\ 1
        \end{bmatrix}
    \]
    We can solve this system to find $c_1$ and $c_2$:
    \[
        c_1 = \frac{1 + \phi}{1 + \phi^2}, \quad c_2 = \frac{1 - \phi}{1 + \phi^2}
    \]
    Thus we have:
    \[
        \begin{bmatrix}
            F_n \\ F_{n+1}
        \end{bmatrix} = \begin{bmatrix}
            0 & 1 \\ 1 & 1
        \end{bmatrix}^n \begin{bmatrix}
            F_0 \\ F_1
        \end{bmatrix} = A^n (c_1 \vec{v_1} + c_2 \vec{v_2}) = c_1 A^n \vec{v_1} + c_2 A^n \vec{v_2}
    \]
    Since:
    \[
        A^n \vec{v_1} = \lambda_1^n \vec{v_1} = \phi^n \begin{bmatrix}
            1 \\ \phi
        \end{bmatrix} \quad \text{and} \quad A^n \vec{v_2} = \lambda_2^n \vec{v_2} = \left(\frac{-1}{\phi}\right)^n \begin{bmatrix}
            -\phi \\ 1
        \end{bmatrix}
    \]
    Therefore, we have:
    \[        \begin{bmatrix}
            F_n \\ F_{n+1}
        \end{bmatrix} = c_1 \phi^n \begin{bmatrix}
            1 \\ \phi
        \end{bmatrix} + c_2 \left(\frac{-1}{\phi}\right)^n \begin{bmatrix}
            -\phi \\ 1
        \end{bmatrix}\]
    From this, we can extract an explicit formula for the $n$-th Fibonacci number:
    \[
        F_n = c_1 \phi^n + c_2 (-\phi) \left(\frac{-1}{\phi}\right)^n = \frac{1 + \phi}{1 + \phi^2} \phi^n + \frac{1}{1 + \phi^2} \left(\frac{-1}{\phi}\right)^n
    \]
\end{eg}

\begin{theorem}
    Let $A$ be a matrix of size $n \times n$ and $\vec{v_1}, \ldots, \vec{v_r}$ be eigenvectors of $A$ corresponding to distinct eigenvalues $\lambda_1, \ldots, \lambda_r$. Then, the set $\{\vec{v_1}, \ldots, \vec{v_r}\}$ is linearly independent.
\end{theorem}

\begin{eg}
    Let $A$ be a matrix, $\lambda_1, \ldots, \lambda_p$ be the eigenvalues of $A$ and $\vec{v_1}, \ldots, \vec{v_p}$ be the corresponding eigenvectors. If we now take a new matrix $B = 2A$, what are the eigenvalues and eigenvectors of $B$? We have:
    \[B \vec{v_i} = 2A \vec{v_i} = 2 \lambda_i \vec{v_i}\]
    Thus, the eigenvalues of $B$ are $2 \lambda_1, \ldots, 2 \lambda_p$ and the eigenvectors are the same as those of $A$, i.e. $\vec{v_1}, \ldots, \vec{v_p}$.
\end{eg}

\begin{definition}[Caracteristic Polynomial]
    Let $A$ be an $n \times n$ matrix. The characteristic polynomial of $A$ is defined as:
    \[
        p_A(\lambda) = \det(A - \lambda I_n)
    \]
    for $\lambda \in \mathbb{R}$.
\end{definition}
Remark that for any matrix $A$ of size $n \times n$, the constant term of the characteristic polynomial is given by $p_A(0) = \det(A)$ (since in this case $\lambda = 0$ and thus $A - 0 \cdot I_n = A$).

\begin{theorem}
    Let $A$ be an $n \times n$ matrix. The characteristic polynomial $p_A(\lambda)$ is a polynomial of degree $n$ in the variable $\lambda$.
\end{theorem}
Remark that since the characteristic polynomial is of degree $n$, it has exactly $n$ roots (counting multiplicities) in the complex numbers (by the Fundamental Theorem of Algebra), thus it can always be factored as:
\[p_A(\lambda) = (-1)^n (\lambda - \lambda_1)^{m_1} (\lambda - \lambda_2)^{m_2} \cdots (\lambda - \lambda_k)^{m_k}\]
where $\lambda_1, \ldots, \lambda_k$ are the distinct eigenvalues of $A$ and $m_1, \ldots, m_k$ are their respective algebraic multiplicities (with $m_1 + m_2 + \cdots + m_k = n$).

\begin{eg}
    Let $A = \begin{bmatrix}
        3 & 9 & 3 & 1 \\
        0 & 6 & -2 & 0 \\
        0 & 0 & 0 & -2 \\
        0 & 0 & 0 & 6
    \end{bmatrix}$. Find the eigenvalues of $A$. We compute:
    \[\det(A - \lambda I_4) = \det\begin{bmatrix}
        3 - \lambda & 9 & 3 & 1 \\
        0 & 6 - \lambda & -2 & 0 \\
        0 & 0 & -\lambda & -2 \\
        0 & 0 & 0 & 6 - \lambda
    \end{bmatrix} = (3 - \lambda)(6 - \lambda)^2 (-\lambda)\]
    Thus, the eigenvalues of $A$ are $3$, $6$ (with a multiplicity of $2$), and $0$.
\end{eg}
Remark that the sum of the algebraic multiplicities of the eigenvalues is equal to $4$, which is the size of the matrix $A$.

\begin{eg}
    Let $A = \begin{bmatrix}
        0 & -1 \\ 1 & 0
    \end{bmatrix}$. Find the eigenvalues of $A$. We compute:
    \[\det(A - \lambda I_2) = \det\begin{bmatrix}
        -\lambda & -1 \\ 1 & -\lambda
    \end{bmatrix} = \lambda^2 + 1 = (i - \lambda)(-i - \lambda) = 0\]
    Thus, the eigenvalues of $A$ are $\lambda_1 = i$ and $\lambda_2 = -i$. Geometrically, this matrix represents a rotation by $90^\circ$ in the counter-clockwise direction, which does not have any real eigenvalues or eigenvectors.
\end{eg}
More generally, if a matrix $A$ has complex eigenvalues, they will always come in conjugate pairs. That is, if $\lambda = a + bi$ is an eigenvalue of $A$ (where $a,b \in \mathbb{R}$ and $b \neq 0$), then its complex conjugate $\bar{\lambda} = a - bi$ is also an eigenvalue of $A$ and both eigenvalues have the same algebraic multiplicity.

\section{Similarity of Matrices}
\begin{definition}[Similar Matrices]
    Let $A$ and $B$ be two $n \times n$ matrices. We say that $A$ is similar to $B$ if there exists an invertible matrix $P$ such that:
    \[B = P^{-1} A P \quad \iff \quad A = P B P^{-1}\]
\end{definition}
Remark that similarity of matrices is an equivalence relation, i.e. for any $n \times n$ matrices $A$, $B$, and $C$:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $A$ is similar to $A$ (reflexivity).
    \item If $A$ is similar to $B$, then $B$ is similar to $A$ (symmetry).
    \item If $A$ is similar to $B$ and $B$ is similar to $C$, then $A$ is similar to $C$ (transitivity).
\end{itemize}
\begin{proof}
    \textbf{1. Property}: Let $A$ be an $n \times n$ matrix. We have:
    \[A = I_n A I_n\]
    where $I_n$ is the identity matrix of size $n \times n$, which is invertible. Thus, $A$ is similar to itself. \\
    \textbf{2. Property}: Let $A$ and $B$ be two $n \times n$ matrices such that $A$ is similar to $B$. Then, there exists an invertible matrix $P$ such that:
    \[B = P^{-1} A P\]
    Multiplying both sides by $P$ on the left and by $P^{-1}$ on the right, we have:
    \[A = P B P^{-1}\]
    Thus, $B$ is similar to $A$. \\
    \textbf{3. Property}: Let $A$, $B$, and $C$ be three $n \times n$ matrices such that $A$ is similar to $B$ and $B$ is similar to $C$. Then, there exist invertible matrices $P$ and $Q$ such that:
    \[B = P^{-1} A P \quad \text{and} \quad C = Q^{-1} B Q\]
    Substituting the first equation into the second, we have:
    \[C = Q^{-1} (P^{-1} A P) Q = (Q^{-1} P^{-1}) A (P Q)\]
    Since the product of two invertible matrices is also invertible, $Q^{-1} P^{-1}$ is invertible and $P Q$ is invertible. Thus, $A$ is similar to $C$.
\end{proof}

\begin{theorem}[Similarity and Eigenvalues]
    Let $A$ and $B$ be two similar $n \times n$ matrices. Then:
    \[
        p_B(\lambda) = p_A(\lambda)
    \]
    Thus, $A$ and $B$ have the same eigenvalues (counting algebraic multiplicities).
\end{theorem}
\begin{proof}
    Let $A$ and $B$ be two similar $n \times n$ matrices. Then, there exists an invertible matrix $P$ such that:
    \[B = P^{-1} A P\]
    We compute the characteristic polynomial of $B$:
    \begin{align*}
        p_B(\lambda) &= \det(B - \lambda I_n) \\
        &= \det(P^{-1} A P - \lambda I_n) = \det(P^{-1} A P - \lambda P^{-1} I_n P) = \det(P^{-1} (A - \lambda I_n) P) \\
        &= \det(P^{-1}) \det(A - \lambda I_n) \det(P) = \det(A - \lambda I_n) = p_A(\lambda)
    \end{align*}
    Thus, $p_B(\lambda) = p_A(\lambda)$, which implies that $A$ and $B$ have the same eigenvalues (counting algebraic multiplicities).
\end{proof}
Remark that the converse of this theorem is not necessarily true, i.e. two matrices can have the same eigenvalues (counting algebraic multiplicities) without being similar. For example, the matrices
\[
A = \begin{bmatrix}
    1 & 0 \\ 0 & 0
\end{bmatrix}, \quad B = \begin{bmatrix}
    1 & 1 \\ 0 & 1
\end{bmatrix}
\]
both have the same characteristic polynomial $p(\lambda) = \lambda^2 - 1$, but they are not similar since $A$ is diagonal while $B$ is not. \\

\subsection{Link between Similarity and Change of Basis}
\begin{eg}
    Let $T: \mathbb{R}^3 \to \mathbb{R}^3$ be a linear transformation and $A$ be the matrix representation of $T$ with respect to the standard basis $\mathcal{E} = \{\vec{e_1}, \vec{e_2}, \vec{e_3}\}$. If we have a new basis $\mathcal{B} = \{\vec{v_1}, \vec{v_2}, \vec{v_3}\}$ of $\mathbb{R}^3$, what is the matrix representation of $T$ with respect to the basis $\mathcal{B}$? Let $P$ be the change of basis matrix from $\mathcal{B}$ to $\mathcal{E}$ such that for all $\vec{x} \in \mathbb{R}^3$:
    \[ [\vec{x}]_{\mathcal{E}} = P [\vec{x}]_{\mathcal{B}} \]
    Thus, we have:
    \[
        [T(\vec{x})]_{\mathcal{B}} = P^{-1} T(\vec{x}) = P^{-1} A \vec{x} = P^{-1} A P [\vec{x}]_{\mathcal{B}} 
    \]
    Therefore, the matrix representation of $T$ with respect to the basis $\mathcal{B}$ is given by:
    \[B = P^{-1} A P\]
    which shows that the matrix representations of a linear transformation with respect to different bases are similar matrices.
\end{eg}

\subsection{Diagonalization of Matrices}
\begin{definition}[Diagonalizable Matrix]
    An $n \times n$ matrix $A$ is said to be diagonalizable if it is similar to a diagonal matrix, i.e. there exists an invertible matrix $P$ and a diagonal matrix $D$ such that:
    \[A = P D P^{-1} \quad \iff \quad D = P^{-1} A P\]
\end{definition}
Remark that if $A$ is diagonal, then the eigenvalues, rank and determinant of $A$ can be easily computed from the entries on the main diagonal of $A$ but also that computing powers of $A$ is straightforward since:
\[A^k = \begin{bmatrix}
    d_1 & 0 & \cdots & 0 \\
    0 & d_2 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & d_n
\end{bmatrix}^k = \begin{bmatrix}
    d_1^k & 0 & \cdots & 0 \\
    0 & d_2^k & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & d_n^k
\end{bmatrix}\]
Thus, if a matrix $A$ is diagonalizable, we can compute its powers as follows:
\[A^k = (P D P^{-1})^k = P D^k P^{-1}\]
which is much easier to compute than $A^k$ directly. In one of the previous examples, we saw how diagonalization can be used to compute the $n$-th Fibonacci number efficiently.

\begin{eg}
    Let $A = \begin{bmatrix}
        7 & 2 \\ -4 & 1
    \end{bmatrix}, P = \begin{bmatrix}
        1 & 1 \\ -1 & -2
    \end{bmatrix}$ and $D = \begin{bmatrix}
        5 & 0 \\ 0 & 3
    \end{bmatrix}$. Let's verify that $A$ is diagonalizable by checking if $A = P D P^{-1}$. We first compute $P^{-1}$:
    \[P^{-1} = \frac{1}{(1)(-2) - (1)(-1)} \begin{bmatrix}
        -2 & -1 \\ 1 & 1
    \end{bmatrix} = \begin{bmatrix}
        -2 & -1 \\ 1 & 1
    \end{bmatrix}\]
    Now, we compute $P D P^{-1}$:
    \[P D P^{-1} = \begin{bmatrix}
        1 & 1 \\ -1 & -2
    \end{bmatrix} \begin{bmatrix}
        5 & 0 \\ 0 & 3
    \end{bmatrix} \begin{bmatrix}
        -2 & -1 \\ 1 & 1
    \end{bmatrix} = \begin{bmatrix}
        7 & 2 \\ -4 & 1
    \end{bmatrix} = A\]
    Thus, $A$ is diagonalizable.
\end{eg}
Remark that the columns of the matrix $P$ used to diagonalize $A$ are the eigenvectors of $A$ and the entries on the main diagonal of the matrix $D$ are the corresponding eigenvalues of $A$.

\begin{eg}
    If we complete the previous example, let's show that the $P$ is made of the eigenvectors of $A$ and that the entries on the main diagonal of $D$ are the corresponding eigenvalues of $A$. We have:
    \[
        A = P D P^{-1} \quad \iff \quad A P = P D
    \]
    Thus, we have:
    \[ AP = A \begin{bmatrix}
        1 & 1 \\ -1 & -2
    \end{bmatrix} = \begin{bmatrix}
        A\begin{bmatrix}
        1 \\ -1
        \end{bmatrix}
        & A\begin{bmatrix}
        1 \\ -2
        \end{bmatrix}
    \end{bmatrix}\]
    and:
    \[PD = \begin{bmatrix}
        1 & 1 \\ -1 & -2
    \end{bmatrix} \begin{bmatrix}
        5 & 0 \\ 0 & 3
    \end{bmatrix} = \begin{bmatrix}
        5 \begin{bmatrix}
        1 \\ -1
        \end{bmatrix}
        & 3 \begin{bmatrix}
        1 \\ -2
        \end{bmatrix}
    \end{bmatrix}\]
    Therefore, we have:
    \[A \begin{bmatrix}
        1 \\ -1
    \end{bmatrix} = 5 \begin{bmatrix}
        1 \\ -1
    \end{bmatrix} \quad \text{and} \quad A \begin{bmatrix}
        1 \\ -2
    \end{bmatrix} = 3 \begin{bmatrix}
        1 \\ -2
    \end{bmatrix}\]
    which shows that $\begin{bmatrix}
        1 \\ -1
    \end{bmatrix}$ and $\begin{bmatrix}
        1 \\ -2
    \end{bmatrix}$ are eigenvectors of $A$ corresponding to the eigenvalues $5$ and $3$, respectively.
\end{eg}

\begin{theorem}
    An $n \times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$ linearly independent eigenvectors.
\end{theorem}
\begin{proof}
    \textbf{($\Rightarrow$) Direction}: Suppose that $A$ is diagonalizable. Then, there exists an invertible matrix $P$ and a diagonal matrix $D$ such that:
    \[A = P D P^{-1}\]
    Let the columns of $P$ be denoted as $\vec{v_1}, \vec{v_2}, \ldots, \vec{v_n}$. Since $P$ is invertible, the set $\{\vec{v_1}, \vec{v_2}, \ldots, \vec{v_n}\}$ is linearly independent. Now, we have:
    \[A P = P D\]
    which implies that:
    \[A \vec{v_i} = d_{ii} \vec{v_i}\]
    for each $i = 1, 2, \ldots, n$, where $d_{ii}$ is the $i$-th diagonal entry of $D$. Thus, each $\vec{v_i}$ is an eigenvector of $A$ corresponding to the eigenvalue $d_{ii}$. Therefore, $A$ has $n$ linearly independent eigenvectors. \\
    \textbf{($\Leftarrow$) Direction}: Suppose that $A$ has $n$ linearly independent eigenvectors $\vec{v_1}, \vec{v_2}, \ldots, \vec{v_n}$ corresponding to eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$. Let $P$ be the matrix whose columns are the eigenvectors of $A$:
    \[P = [\vec{v_1} \quad \vec{v_2} \quad \cdots \quad \vec{v_n}]\]
    Since the eigenvectors are linearly independent, $P$ is invertible. Now, let $D$ be the diagonal matrix with the eigenvalues on the main diagonal:
    \[D = \begin{bmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{bmatrix}\]
    We have:
    \[A P = [A \vec{v_1} \quad A \vec{v_2} \quad \cdots \quad A \vec{v_n}] = [\lambda_1 \vec{v_1} \quad \lambda_2 \vec{v_2} \quad \cdots \quad \lambda_n \vec{v_n}] = P D\]
    Multiplying both sides by $P^{-1}$, we obtain:
    \[A = P D P^{-1}\]
    Thus, $A$ is diagonalizable.
\end{proof}
Remark that both $P$ and $D$ can be constructed from the eigenvectors and eigenvalues of $A$.

% \begin{theorem}
%     Let $A$ be a $n \times n$ matrix and let $\vec{v_1}, \ldots, \vec{v_k}$ be eigenvectors of $A$ corresponding to distinct eigenvalues $\lambda_1, \ldots, \lambda_k$. Then these eigenvectors are linearly independent.
% \end{theorem}

\begin{theorem}
    Let $A$ be an $n \times n$ matrix with $n$ distinct eigenvalues. Then, $A$ is diagonalizable.
\end{theorem}
\begin{eg}
    Let $A = \begin{bmatrix}
        -1 & 7 & 9 \\
        0 & 2 & -2 \\
        0 & 0 & 0
    \end{bmatrix}$. We easily see that the eigenvalues of $A$ are $-1$, $2$, and $0$ (since $A$ is an upper triangular matrix). Since $A$ has $3$ distinct eigenvalues, it is diagonalizable.
\end{eg}

\begin{eg}
    Let $A = \begin{bmatrix}
        1 & 3 & 3 \\ -3 & -5 & -3 \\ 3 & 3 & 1
    \end{bmatrix}$. Let's show that $A$ is diagonalizable. We compute the characteristic polynomial of $A$:
    \[\det(A - \lambda I_3) = \det\begin{bmatrix}
        1 - \lambda & 3 & 3 \\ -3 & -5 - \lambda & -3 \\ 3 & 3 & 1 - \lambda
    \end{bmatrix} = (1 - \lambda)(2 + \lambda)^2\]
    Thus, the eigenvalues of $A$ are $\lambda_1 = 1$ and $\lambda_2 = -2$ (with a multiplicity of $2$). Let's find the eigenvectors corresponding to each eigenvalue: \\
    \textbf{For $\lambda = 1$:}
    \[\begin{bmatrix}
        1 - 1 & 3 & 3 \\ -3 & -5 - 1 & -3 \\ 3 & 3 & 1 - 1
    \end{bmatrix} = \begin{bmatrix}
        0 & 3 & 3 \\ -3 & -6 & -3 \\ 3 & 3 & 0
    \end{bmatrix} \sim \begin{bmatrix}
        1 & 0 & -1 \\ 0 & 1 & 1 \\ 0 & 0 & 0
    \end{bmatrix}\]
    Thus, any non-zero scalar multiple of $\begin{bmatrix}
        1 \\ -1 \\ 1
    \end{bmatrix}$ is an eigenvector corresponding to the eigenvalue $1$. \\
    \textbf{For $\lambda = -2$:}
    \[\begin{bmatrix}
        1 + 2 & 3 & 3 \\ -3 & -5 + 2 & -3 \\ 3 & 3 & 1 + 2
    \end{bmatrix} = \begin{bmatrix}
        3 & 3 & 3 \\ -3 & -3 & -3 \\ 3 & 3 & 3
    \end{bmatrix} \sim \begin{bmatrix}
        1 & 1 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0
    \end{bmatrix}\]
    Thus, any non-zero scalar multiple of $\begin{bmatrix}
        1 \\ -1 \\ 0
    \end{bmatrix}$ and $\begin{bmatrix}
        1 \\ 0 \\ -1
    \end{bmatrix}$ are eigenvectors corresponding to the eigenvalue $-2$.
    Since we have found $3$ linearly independent eigenvectors of $A$, we conclude that $A$ is diagonalizable. We can construct the matrices $P$ and $D$ as follows:
    \[P = \begin{bmatrix}
        1 & 1 & 1 \\ -1 & -1 & 0 \\ 1 & 0 & -1
    \end{bmatrix}, \quad D = \begin{bmatrix}
        1 & 0 & 0 \\ 0 & -2 & 0 \\ 0 & 0 & -2
    \end{bmatrix}\]
\end{eg}

\begin{eg}
    Let $A = \begin{bmatrix}
        2 & 4 & 3 \\ -4   & -6 & -3 \\ 3 & 3 & 1
    \end{bmatrix}$. Let's show that $A$ is not diagonalizable. We compute the characteristic polynomial of $A$:
    \[\det(A - \lambda I_3) = \det\begin{bmatrix}
        2 - \lambda & 4 & 3 \\ -4 & -6 - \lambda & -3 \\ 3 & 3 & 1 - \lambda
    \end{bmatrix} = (1 - \lambda)^2 ( 2 + \lambda )\]
    Thus, the eigenvalues of $A$ are $\lambda_1 = 1$ (with a multiplicity of $2$) and $\lambda_2 = -2$. Let's find the eigenvectors corresponding to each eigenvalue: \\
    \textbf{For $\lambda = 1$:}
    \[\begin{bmatrix}
        2 - 1 & 4 & 3 \\ -4 & -6 - 1 & -3 \\ 3 & 3 & 1 - 1
    \end{bmatrix} = \begin{bmatrix}
        1 & 4 & 3 \\ -4 & -7 & -3 \\ 3 & 3 & 0
    \end{bmatrix} \sim \begin{bmatrix}
        1 & 0 & -1 \\ 0 & 1 & 1 \\ 0 & 0 & 0
    \end{bmatrix}\]
    Thus, any non-zero scalar multiple of $\begin{bmatrix}
        1 \\ -1 \\ 1
    \end{bmatrix}$ is an eigenvector corresponding to the eigenvalue $1$. \\
    \textbf{For $\lambda = -2$:}
    \[\begin{bmatrix}
        2 + 2 & 4 & 3 \\ -4 & -6 + 2 & -3 \\ 3 & 3 & 1 + 2
    \end{bmatrix} = \begin{bmatrix}
        4 & 4 & 3 \\ -4 & -4 & -3 \\ 3 & 3 & 3
    \end{bmatrix} \sim \begin{bmatrix}
        1 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0
    \end{bmatrix}\]
    Thus, any non-zero scalar multiple of $\begin{bmatrix}
        -1 \\ 1 \\ 0
    \end{bmatrix}$ is an eigenvector corresponding to the eigenvalue $-2$.
    Since we have found only $2$ linearly independent eigenvectors of $A$, we conclude that $A$ is not diagonalizable.
\end{eg}
% For a matrix $A$ to be diagonalizable, the following conditions must be met:
% \begin{itemize}[itemsep=1pt,label=$\circ$]
%     \item $n$ linearly independent eigenvectors.
%     \item 

\subsection{Geometric Multiplicity of Eigenvalues}
\begin{definition}[Geometric Multiplicity]
    Let $A$ be an $n \times n$ matrix and $\lambda$ be an eigenvalue of $A$. The geometric multiplicity of $\lambda$ is defined as the dimension of the eigenspace corresponding to $\lambda$ ($\ker(A - \lambda I_n)$), i.e. the geometric multiplicity of $\lambda$ is given by:
    \[
        \dim(\ker(A - \lambda I_n))
    \]
\end{definition}
Note that the geometric multiplicity of an eigenvalue represents the number of linearly independent eigenvectors associated with that eigenvalue.

\begin{theorem}
    Let $A$ be an $n \times n$ matrix and $\lambda_i$ and $\lambda_j$ be two distinct eigenvalues of $A$. Then, the eigenspaces corresponding to $\lambda_i$ and $\lambda_j$ intersect only at the zero vector, i.e.:
    \[\ker(A - \lambda_i I_n) \cap \ker(A - \lambda_j I_n) = \{\vec{0}\}\]
    Thus, the eigenvectors corresponding to distinct eigenvalues are linearly independent.
\end{theorem}
\begin{proof}
    Let $\vec{v}$ be a vector in the intersection of the eigenspaces corresponding to $\lambda_i$ and $\lambda_j$. Then, we have:
    \[(A - \lambda_i I_n) \vec{v} = \vec{0} \quad \text{and} \quad (A - \lambda_j I_n) \vec{v} = \vec{0}\]
    From the first equation, we have:
    \[A \vec{v} = \lambda_i \vec{v}\]
    Substituting this into the second equation, we get:
    \[(\lambda_i - \lambda_j) \vec{v} = \vec{0}\]
    Since $\lambda_i$ and $\lambda_j$ are distinct eigenvalues, we have $\lambda_i - \lambda_j \neq 0$. Therefore, the only solution to this equation is $\vec{v} = \vec{0}$. Thus, the intersection of the eigenspaces corresponding to $\lambda_i$ and $\lambda_j$ contains only the zero vector. This implies that the eigenvectors corresponding to distinct eigenvalues are linearly independent.
\end{proof}

\begin{theorem}
    The geometric multiplicity of an eigenvalue $\lambda$ of an $n \times n$ matrix $A$ is always less than or equal to its algebraic multiplicity.
\end{theorem}

\begin{theorem}
    Let $A$ be a matrix of size $n \times n$ and let $\lambda_1, \ldots, \lambda_k$ be its distinct eigenvalues with $m_1, \ldots, m_k$ as their respective algebraic multiplicities. Then:
    \[
        \sum_{i=1}^k \dim(\ker(A - \lambda_i I_n)) \leq \sum_{i=1}^k m_i = n
    \]
\end{theorem}
Remark that $A$ is diagonalizable if and only if the equality holds in the previous theorem, i.e. if and only if:
\[\sum_{i=1}^k \dim(\ker(A - \lambda_i I_n)) = \sum_{i=1}^k m_i = n\]
Or equivalently, if and only if the geometric multiplicity of each eigenvalue is equal to its algebraic multiplicity.