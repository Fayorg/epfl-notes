\chapter{Vectorial Spaces and Subspaces}
With this chapter, we add another abstraction layer to our mathematical framework. We will get new mathematical tools that will allow us to work with more complex objects in a more general way.

\section{Vectorial Spaces}
\begin{definition}[Vectorial Space]
    A vectorial space (or vector space) is a set $V$ together with two operations: vector addition and scalar multiplication, satisfying these properties for all $\vec{u}, \vec{v}, \vec{w} \in V$ and $c,d \in \mathbb{R}$:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $\vec{u} + \vec{v} = \vec{v} + \vec{u}$ (Commutativity of vector addition)
        \item $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$ (Associativity of vector addition)
        \item There exists a zero vector $\vec{0} \in \mathbb{R}^n$ such that $\vec{u} + \vec{0} = \vec{u}$ (Existence of additive identity)
        \item For each $\vec{u} \in \mathbb{R}^n$, there exists an additive inverse $-\vec{u} \in \mathbb{R}^n$ such that $\vec{u} + (-\vec{u}) = \vec{0}$ (Existence of additive inverse)
        \item $c(\vec{u} + \vec{v}) = c\vec{u} + c\vec{v}$ (Distributivity of scalar multiplication over vector addition)
        \item $(c + d)\vec{u} = c\vec{u} + d\vec{u}$ (Distributivity of scalar addition over scalar multiplication)
        \item $c(d\vec{u}) = (cd)\vec{u}$ (Associativity of scalar multiplication)
        \item $1\vec{u} = \vec{u}$ (Existence of multiplicative identity)
    \end{itemize}
\end{definition}
Instead of working with specific objects like $\mathbb{R}^n$, we can now work with any set $V$ that satisfies the properties of a vectorial space. This abstraction allows us to apply the same mathematical tools and techniques previously seen to a wider variety of objects.

\begin{definition}[Vector]
    Previously defined vectors in $\mathbb{R}^n$ are now called vectors in a vectorial space $V$. The operations of vector addition and scalar multiplication are defined as per the properties of the vectorial space.
\end{definition}

\begin{eg}
    Let $V = \mathbb{R}^n$ with the usual operations of vector addition and scalar multiplication in $\mathbb{R}^n$. 
\end{eg}

\begin{eg}
    Let $V = \mathbb{R}^{n \times n}$ with the usual operations of matrix addition and scalar multiplication in $\mathbb{R}^{n \times n}$. \\
    Note that in this case, the elements of $V$ are matrices but also vectors in the vectorial space $V$.
\end{eg}

\subsection{Polynomial Spaces}
\begin{definition}[Polynomial Space]
    Let $P_n$ be the set of all polynomials of degree less than or equal to $n$. The operations of polynomial addition and scalar multiplication are defined as follows:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Polynomial Addition: For $p(x), q(x) \in P_n$, their sum is defined as $(p + q)(x) = p(x) + q(x)$.
        \item Scalar Multiplication: For a scalar $c \in \mathbb{R}$ and a polynomial $p(x) \in P_n$, the scalar multiplication is defined as $(cp)(x) = c \cdot p(x)$.
    \end{itemize}
    With these operations, we can easily verify that $P_n$ satisfies all the properties of a vectorial space. Thus, $P_n$ is a vectorial space.
\end{definition}
\begin{proof}
    Let's prove that for any two polynomials $p(x), q(x) \in P_n$ and any scalar $c \in \mathbb{R}$, the operations of polynomial are well-defined. We have:
    \[
        p(x) = a_0 + a_1 x + a_2 x^2 + ... + a_n x^n \quad \text{and} \quad q(x) = b_0 + b_1 x + b_2 x^2 + ... + b_n x^n
    \]
    where $a_0, a_1, ..., a_n, b_0, b_1, ..., b_n \in \mathbb{R}$. \\
    \textbf{Polynomial Addition:}
    \[(p + q)(x) = p(x) + q(x) = (a_0 + b_0) + (a_1 + b_1)x + (a_2 + b_2)x^2 + ... + (a_n + b_n)x^n\]
    The resulting polynomial $(p + q)(x)$ is also of degree less than or equal to $n$, so it belongs to $P_n$. \\
    \textbf{Scalar Multiplication:}
    \[(cp)(x) = c \cdot p(x) = c \cdot (a_0 + a_1 x + a_2 x^2 + ... + a_n x^n) = (c a_0) + (c a_1)x + (c a_2)x^2 + ... + (c a_n)x^n\]
    The resulting polynomial $(cp)(x)$ is also of degree less than or equal to $n$, so it belongs to $P_n$. \\
    Thus both operations are well-defined, and we can easily verify that $P_n$ satisfies all the properties of a vectorial space.
\end{proof}

\begin{eg}
    Let $V$ a vectorial space. Let's show that (1) the zero vector is unique, (2) the additive inverse of each vector is unique and (3) $0 \vec{u} = \vec{0}$ and $c \vec{0} = \vec{0}$ for all $c \in \mathbb{R}$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Let's assume that there are two vectors $\vec{0}$ and $\vec{0'}$ that both satisfy the property of the additive identity. Then, we have:
        \[\vec{u} + \vec{0} = \vec{u} \quad \text{and} \quad \vec{u} + \vec{0'} = \vec{u}\]
        By subtracting $\vec{u}$ from both sides of the second equation, we get:
        \[ \vec{u} + \vec{0} = \vec{u} = \vec{u} + \vec{0'} \implies \vec{0'} = \vec{0}\]
        Thus, the zero vector is unique.
        \item Let's assume that there are two vectors $\vec{v}$ and $\vec{v'}$ that both satisfy the property of the additive inverse for a vector $\vec{u}$. Then, we have:
        \[\vec{u} + \vec{v} = \vec{0} \quad \text{and} \quad \vec{u} + \vec{v'} = \vec{0}\]
        By subtracting $\vec{u}$ from both sides of the second equation, we get:
        \[ \vec{u} + \vec{v} = \vec{0} = \vec{u} + \vec{v'} \implies \vec{v'} = \vec{v}\]
        Thus, the additive inverse of each vector is unique.
        \item To show that $0 \vec{u} = \vec{0}$, we can use the distributive property of scalar multiplication over scalar addition:
        \[0 \vec{u} = (0 + 0) \vec{u} = 0 \vec{u} + 0 \vec{u}\]
        By subtracting $0 \vec{u}$ from both sides, we get:
        \[0 \vec{u} = \vec{0}\]
        To show that $c \vec{0} = \vec{0}$ for any scalar $c \in \mathbb{R}$, we can use the distributive property of scalar multiplication over vector addition:
        \[c \vec{0} = c (\vec{0} + \vec{0}) = c \vec{0} + c \vec{0}\]
        By subtracting $c \vec{0}$ from both sides, we get:
        \[c \vec{0} = \vec{0}\]
        Thus, we have shown that $0 \vec{u} = \vec{0}$ and $c \vec{0} = \vec{0}$ for all $c \in \mathbb{R}$.
    \end{itemize}
\end{eg}

\section{Subspaces}
\begin{definition}[Subspace]
    A subspace $W$ of a vectorial space $V$ is a subset of $V$ that is itself a vectorial space under the same operations of vector addition and scalar multiplication defined on $V$. In other words, $W$ is a subspace of $V$ if:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector of $V$ is in $W$.
        \item $W$ is closed under vector addition: For any $\vec{u}, \vec{v} \in W$, the sum $\vec{u} + \vec{v}$ is also in $W$.
        \item $W$ is closed under scalar multiplication: For any $\vec{u} \in W$ and any scalar $c \in \mathbb{R}$, the product $c\vec{u}$ is also in $W$.
    \end{itemize}
\end{definition}

\begin{eg}
    Let $V = \mathbb{R}^n$ and $H = \{\vec{x} \in \mathbb{R}^n : x_1 + \ldots + x_n = 0\}$. Let's show that $H$ is a subspace of $V$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector $\vec{0} = (0, 0, \ldots, 0)$ is in $H$ since $0 + 0 + \ldots + 0 = 0$.
        \item Let $\vec{u}, \vec{v} \in H$. Then, we have:
        \[ u_1 + u_2 + \ldots + u_n = 0 \quad \text{and} \quad v_1 + v_2 + \ldots + v_n = 0 \]
        Adding these two equations, we get:
        \[(u_1 + v_1) + (u_2 + v_2) + \ldots + (u_n + v_n) = 0 + 0 = 0\]
        Thus, $\vec{u} + \vec{v} \in H$.
        \item Let $\vec{u} \in H$ and $c \in \mathbb{R}$. Then, we have:
        \[u_1 + u_2 + \ldots + u_n = 0\]
        Multiplying this equation by $c$, we get:
        \[c u_1 + c u_2 + \ldots + c u_n = c \cdot 0 = 0\]
        Thus, $c \vec{u} \in H$.
    \end{itemize}
    Since $H$ satisfies all three conditions, it is a subspace of $V$
\end{eg}
Let's see some examples of sets that are not subspaces:
\begin{eg}
    Let $V = \mathbb{R}^n$ and $H = \{\vec{x} \in \mathbb{R}^n : x_1 + \ldots + x_n = 1\}$. Let's show that $H$ is not a subspace of $V$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector $\vec{0} = (0, 0, \ldots, 0)$ is not in $H$ since $0 + 0 + \ldots + 0 \neq 1$.
    \end{itemize}
    Since $H$ does not satisfy the first condition, it is not a subspace of $V$.
\end{eg}
\begin{eg}
    Let $V = \mathbb{R}^n$ and $H = \{\vec{x} \in \mathbb{R}^n : x_1 \geq 0, \ldots, x_n \geq 0\}$. Let's show that $H$ is not a subspace of $V$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector $\vec{0} = (0, 0, \ldots, 0)$ is in $H$ since $0 \geq 0, \ldots, 0 \geq 0$.
        \item Let $\vec{u}, \vec{v} \in H$. Then, we have:
        \[ u_1 \geq 0, \ldots, u_n \geq 0 \quad \text{and} \quad v_1 \geq 0, \ldots, v_n \geq 0 \]
        Adding these two equations, we get:
        \[(u_1 + v_1) \geq 0, \ldots, (u_n + v_n) \geq 0\]
        Thus, $\vec{u} + \vec{v} \in H$.
        \item Let $\vec{u} \in H$ and $c \in \mathbb{R}$. If $c \geq 0$, then we have:
        \[u_1 \geq 0, \ldots, u_n \geq 0\]
        Multiplying this equation by $c$, we get:
        \[c u_1 \geq 0, \ldots, c u_n \geq 0\]
        Thus, $c \vec{u} \in H$. However, if $c < 0$, then we have:
        \[u_1 \geq 0, \ldots, u_n \geq 0\]
        Multiplying this equation by $c$, we get:
        \[c u_1 \leq 0, \ldots, c u_n \leq 0\]
        Thus, $c \vec{u} \notin H$.
    \end{itemize}
    Since $H$ does not satisfy the third condition, it is not a subspace of $V$.
\end{eg}
\begin{eg}
    Let $V = \mathbb{R}^{n \times n }$ and $H = \{A \in \mathbb{R}^{n \times n} : A = A^T\}$. Let's show that $H$ is a subspace of $V$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero matrix $0$ is in $H$ since $0 = 0^T$.
        \item Let $A, B \in H$. Then, we have:
        \[ A = A^T \quad \text{and} \quad B = B^T \]
        Adding these two equations, we get:
        \[ (A + B)^T = A^T + B^T = A + B \]
        Thus, $A + B \in H$.
        \item Let $A \in H$ and $c \in \mathbb{R}$. Then, we have:
        \[ A = A^T \]
        Multiplying this equation by $c$, we get:
        \[ (cA)^T = cA^T = cA \]
        Thus, $cA \in H$.
    \end{itemize}
    Since $H$ satisfies all three conditions, it is a subspace of $V$.
\end{eg}

\begin{eg}
    Let $V = \mathbb{P}_5$ and $H = \mathbb{P}_3$. Let's show that $H$ is a subspace of $V$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero polynomial $0$ is in $H$ since it is a polynomial of degree less than or equal to $3$.
        \item Let $p(x), q(x) \in H$. Then, we have:
        \[ \text{deg}(p(x)) \leq 3 \quad \text{and} \quad \text{deg}(q(x)) \leq 3 \]
        Adding these two polynomials, we get:
        \[ \text{deg}(p(x) + q(x)) \leq 3 \]
        Thus, $p(x) + q(x) \in H$.
        \item Let $p(x) \in H$ and $c \in \mathbb{R}$. Then, we have:
        \[ \text{deg}(p(x)) \leq 3 \]
        Multiplying this polynomial by $c$, we get:
        \[ \text{deg}(cp(x)) \leq 3 \]
        Thus, $cp(x) \in H$.
    \end{itemize}
    Since $H$ satisfies all three conditions, it is a subspace of $V$.
\end{eg}
Note that $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$ since it is not a subset of $\mathbb{R}^3$.
\begin{definition}[Null Subspace]
    The null subspace (or trivial subspace) of a vectorial space $V$ is the set $\{\vec{0}\}$ containing only the zero vector of $V$. It is a subspace of $V$ since it satisfies all three conditions of a subspace.
\end{definition}

\begin{eg}
    Let $V$ be a vectorial space. Let $\vec{v_1}, \vec{v_2}, \vec{v_3} \in V$ vectors of $V$ and the set $H = Span\{v_1,v_2,v_3\}$ of all linear combinations of $\vec{v_1}, \vec{v_2}, \vec{v_3}$. This set is a subset of $V$. Let's show that $H$ is a subspace of $V$:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector $\vec{0}$ is in $H$ since we can write it as a linear combination of $\vec{v_1}, \vec{v_2}, \vec{v_3}$ with all coefficients equal to $0$:
        \[ \vec{0} = 0\vec{v_1} + 0\vec{v_2} + 0\vec{v_3} \]
        \item Let $\vec{u}, \vec{w} \in H$. Then, we can write them as linear combinations of $\vec{v_1}, \vec{v_2}, \vec{v_3}$:
        \[ \vec{u} = a_1\vec{v_1} + a_2\vec{v_2} + a_3\vec{v_3} \quad \text{and} \quad \vec{w} = b_1\vec{v_1} + b_2\vec{v_2} + b_3\vec{v_3} \]
        where $a_1, a_2, a_3, b_1, b_2, b_3 \in \mathbb{R}$. Adding these two equations, we get:
        \[ \vec{u} + \vec{w} = (a_1 + b_1)\vec{v_1} + (a_2 + b_2)\vec{v_2} + (a_3 + b_3)\vec{v_3} \]
        Thus, $\vec{u} + \vec{w} \in H$.
        \item Let $\vec{u} \in H$ and $c \in \mathbb{R}$. Then, we can write $\vec{u}$ as a linear combination of $\vec{v_1}, \vec{v_2}, \vec{v_3}$:
        \[ \vec{u} = a_1\vec{v_1} + a_2\vec{v_2} + a_3\vec{v_3} \]
        where $a_1, a_2, a_3 \in \mathbb{R}$. Multiplying this equation by $c$, we get:
        \[ c\vec{u} = (ca_1)\vec{v_1} + (ca_2)\vec{v_2} + (ca_3)\vec{v_3} \]
        Thus, $c\vec{u} \in H$.
    \end{itemize}
    Since $H$ satisfies all three conditions, it is a subspace of $V$.
\end{eg}

\begin{theorem}
    Let $\vec{v_1}, v_2, \ldots, v_p$ be vectors in a vector space $V$. Then, the set $H = Span\{v_1, v_2, \ldots, v_p\}$ is a subspace of $V$.
\end{theorem}

\begin{eg}
    Let $V = \mathbb{R}^3$ and $\vec{v_1}, \vec{v_2}, \vec{v_3} \in V$. We have:
    \begin{itemize}
        \item The origin $\{(0,0,0)\}$ is a subspace of $V$.
        \item Any line through the origin ($Span(\vec{v_1}), \vec{v_1} \neq \vec{0}$) is a subspace of $V$.
        \item Any plane through the origin ($Span(\vec{v_1}, \vec{v_2})$ and $\vec{v_1}, \vec{v_2}$ linearly independent) is a subspace of $V$.
        \item The whole space $V$ is a subspace of $V$ ($Span(\vec{v_1}, \vec{v_2}, \vec{v_3})$ and $\vec{v_1}, \vec{v_2}, \vec{v_3}$ linearly independent).
    \end{itemize}
\end{eg}

\begin{theorem}
    The set of solutions $H$ of a homogeneous system of linear equations $A\vec{x} = \vec{0}$ (with $A \in \mathbb{R}^{m \times n}$) is a subspace of $\mathbb{R}^n$.
\end{theorem}
\begin{proof}
    Let's show that $H$ satisfies the three conditions of a subspace:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector $\vec{0}$ is in $H$ since $A\vec{0} = \vec{0}$.
        \item Let $\vec{u}, \vec{v} \in H$. Then, we have:
        \[ A\vec{u} = \vec{0} \quad \text{and} \quad A\vec{v} = \vec{0} \]
        Adding these two equations, we get:
        \[ A(\vec{u} + \vec{v}) = A\vec{u} + A\vec{v} = \vec{0} + \vec{0} = \vec{0} \]
        Thus, $\vec{u} + \vec{v} \in H$.
        \item Let $\vec{u} \in H$ and $c \in \mathbb{R}$. Then, we have:
        \[ A\vec{u} = \vec{0} \]
        Multiplying this equation by $c$, we get:
        \[ A(c\vec{u}) = cA\vec{u} = c\cdot\vec{0} = \vec{0} \]
        Thus, $c\vec{u} \in H$.
    \end{itemize}
    Since $H$ satisfies all three conditions, it is a subspace of $\mathbb{R}^n$.
\end{proof}

\section{Kernel and Images}
Let $A \in \mathbb{R}^{m \times n}$.
\begin{definition}[Kernel]
    The kernel (or null space) of $A$ is the set of all vectors $\vec{x} \in \mathbb{R}^n$ such that $A\vec{x} = \vec{0}$, i.e.,
    \[ \text{Ker}(A) = \{ \vec{x} \in \mathbb{R}^n \mid A\vec{x} = \vec{0} \} \]
    The kernel is a subspace of $\mathbb{R}^n$.
\end{definition}

\begin{definition}[Image]
    The image of $A$ is the set of all vectors $\vec{y} \in \mathbb{R}^m$ such that there exists a vector $\vec{x} \in \mathbb{R}^n$ with $A\vec{x} = \vec{y}$, i.e.,
    \[ \text{Im}(A) = \{ \vec{y} \in \mathbb{R}^m \mid \exists \vec{x} \in \mathbb{R}^n, A\vec{x} = \vec{y} \} \]
    The image is a subspace of $\mathbb{R}^m$.
\end{definition}
The Image of $T$ is also called the column space of $A$ since it is spanned by the columns of $A$.

\begin{eg}
    Let $A = \begin{bmatrix}
        -3 & 6 & -1 & 1 & -7 \\
        1 & -2 & 2 & 3 & -1 \\
        2 & -4 & 5 & 8 & -4
    \end{bmatrix}$. Let's find a basis for the kernel and the image of the linear transformation $T: \mathbb{R}^5 \to \mathbb{R}^3$ defined by $T(\vec{x}) = A\vec{x}$. We need to solve the homogeneous system $A\vec{x} = \vec{0}$:
    \[ \begin{bmatrix}
        -3 & 6 & -1 & 1 & -7 \\
        1 & -2 & 2 & 3 & -1 \\
        2 & -4 & 5 & 8 & -4
    \end{bmatrix} \sim \begin{bmatrix}
        1 & -2 & 0 & -1 & 3 \\
        0 & 0 & 1 & 2 & -2 \\
        0 & 0 & 0 & 0 & 0
    \end{bmatrix} \]
    From the reduced row echelon form, we can write the system of equations:
    \[
        \begin{cases}
            x_1 - 2x_2 - x_4 + 3x_5 = 0 \\
            x_3 + 2x_4 - 2x_5 = 0
        \end{cases} \iff \begin{cases}
            x_1 = 2x_2 + x_4 - 3x_5 \\
            x_3 = -2x_4 + 2x_5
        \end{cases}
    \]
    We can express the solution in terms of the free variables $x_2, x_4, x_5$:
    \[ \vec{x} = \begin{bmatrix}
        x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
    \end{bmatrix} = x_2 \begin{bmatrix}
        2 \\ 1 \\ 0 \\ 0 \\ 0
    \end{bmatrix} + x_4 \begin{bmatrix}
        1 \\ 0 \\ -2 \\ 1 \\ 0
    \end{bmatrix} + x_5 \begin{bmatrix}
        -3 \\ 0 \\ 2 \\ 0 \\ 1
    \end{bmatrix} \]
    % Thus, a basis for the kernel of $T$ is given by:
    % \[ \left\{ \begin{bmatrix}
    %     2 \\ 1 \\ 0 \\ 0 \\ 0
    % \end{bmatrix}, \begin{bmatrix}
    %     1 \\ 0 \\ -2 \\ 1 \\ 0
    % \end{bmatrix}, \begin{bmatrix}
    %     -3 \\ 0 \\ 2 \\ 0 \\ 1
    % \end{bmatrix} \right\} \]
    % To find a basis for the image of $T$, we can look at the pivot columns of the original matrix $A$. The pivot columns are the first and third columns. Thus, a basis for the image of $T$ is given by:
    % \[ \left\{ \begin{bmatrix}
    %     -3 \\ 1 \\ 2
    % \end{bmatrix}, \begin{bmatrix}
    %     -1 \\ 2 \\ 5
    % \end{bmatrix} \right\} \]
\end{eg}

\subsection{Differences between the Kernel and the Image}
We can denote these differences as follows:
\vskip0.3cm
\begin{center}
    \begin{tabular}{p{0.45\textwidth} | p{0.45\textwidth}}
        \\ {\centering \textbf{Kernel} \par} & {\centering \textbf{Image} \par} \\ \\ \hline \\
        The kernel is a subspace of $\mathbb{R}^n$ & The image is a subspace of $\mathbb{R}^m$ \\ \\
        $\text{Ker} A$ is defined implicitly by the equation $A\vec{x} = \vec{0}$ & $\text{Im} A$ is defined explicitly as the span of the columns of $A$ \\ \\
        Finding the $\text{Ker} A$ takes time by solving a homogeneous system of linear equations & Finding the $\text{Im} A$ is easy. The columns of $A$ form a basis for the image. \\ \\
        No simple relation between $\text{Ker} A$ and the coefficients of $A$ & Simple relation between $\text{Im} A$ and the coefficients of $A$, since each column of $A$ is contained $\text{Im} A$ \\ \\
        A vector $\vec{v}$ in $\text{Ker} A$ is characterized by $A\vec{v} = \vec{0}$ & A vector $\vec{v}$ in $\text{Im} A$ is characterized by the existence of a vector $\vec{u}$ such that $A\vec{u} = \vec{v}$ \\ \\
        Given a vector $\vec{v}$, it is easy to check if $\vec{v} \in \text{Ker} A$ by computing $A\vec{v}$ & Given a vector $\vec{v}$, the system $\begin{bmatrix}
            A & \vec{v}
        \end{bmatrix}$ must be solved to check if $\vec{v} \in \text{Im} A$ \\ \\
        $\text{Ker} A = \{\vec{0}\}$ if and only if the equation $A\vec{x} = \vec{0}$ has only the trivial solution & $\text{Im} A = \mathbb{R}^m$ if and only if the equation $A\vec{x} = \vec{b}$ has a solution for every $\vec{b} \in \mathbb{R}^m$ \\ \\
        $\text{Ker} A = \{\vec{0}\}$ if and only if the transformation $T(\vec{x}) = A\vec{x}$ is one-to-one (injective) & $\text{Im} A = \mathbb{R}^m$ if and only if the transformation $T(\vec{x}) = A\vec{x}$ is onto (surjective) \\ \\
    \end{tabular}
\end{center}

\subsection{Kernel and Images of Linear Transformations}
\begin{definition}[Linear Transformation]
    A linear transformation is a function $T: V \to W$ that satisfies the following properties for all vectors $\vec{u}, \vec{v} \in V$ and any scalar $c \in \mathbb{R}$:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v})$ (Additivity)
        \item $T(c\vec{u}) = cT(\vec{u})$ (Homogeneity)
    \end{itemize}
\end{definition}
Again we can define the kernel and the image of a linear transformation.
\begin{definition}[Kernel of a Linear Transformation]
    The kernel (or null space) of a linear transformation $T: V \to W$ is the set of all vectors $\vec{v} \in V$ such that $T(\vec{v}) = \vec{0}$, i.e.,
    \[ \text{Ker}(T) = \{ \vec{v} \in V \mid T(\vec{v}) = \vec{0} \} \]
    The kernel is a subspace of $V$.
\end{definition}
\begin{definition}[Image of a Linear Transformation]
    The image (or range) of a linear transformation $T: V \to W$ is the set of all vectors $\vec{w} \in W$ such that there exists a vector $\vec{v} \in V$ with $T(\vec{v}) = \vec{w}$, i.e.,
    \[ \text{Im}(T) = \{ \vec{w} \in W \mid \exists \vec{v} \in V, T(\vec{v}) = \vec{w} \} \]
    The image is a subspace of $W$.
\end{definition}

\begin{eg}
    Let $V = \mathbb{P}_3$ and $W = \mathbb{P}_2$. Consider the linear transformation $T: V \to W$ defined by $T(p(x)) = p'(x)$, where $p'(x)$ is the derivative of the polynomial $p(x)$:
    \[ p(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 \quad \text{and} \quad p \in \mathbb{P}_3 \]
    where $a_0, a_1, a_2, a_3 \in \mathbb{R}$. The derivative of $p(x)$ is given by:
    \[ p'(x) = a_1 + 2a_2 x + 3a_3 x^2 \quad \text{and} \quad p' \in \mathbb{P}_2 \]
    Let's check that $T$ is a linear transformation:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Let $p(x), q(x) \in V$. Then, we have:
        \[ T(p(x) + q(x)) = (p(x) + q(x))' = p'(x) + q'(x) = T(p(x)) + T(q(x)) \]
        Thus, $T$ satisfies the additivity property.
        \item Let $p(x) \in V$ and $c \in \mathbb{R}$. Then, we have:
        \[ T(cp(x)) = (cp(x))' = c p'(x) = c T(p(x)) \]
        Thus, $T$ satisfies the homogeneity property.
    \end{itemize}
    Since $T$ satisfies both properties, it is a linear transformation. Now, let's find the kernel of $T$:
    \[ \text{Ker}(T) = \{ p(x) \in V \mid T(p(x)) = p'(x) = 0 \} \]
    The only polynomial in $\mathbb{P}_3$ whose derivative is $0$ is the constant polynomial. Thus, we have:
    \[ \text{Ker}(T) = \{ p(x) = a_0 \mid a_0 \in \mathbb{R} \} \]
    Now, let's find the image of $T$:
    \[ \text{Im}(T) = \{ p'(x) \in W \mid p(x) \in V \} \]
    The derivative of any polynomial in $\mathbb{P}_3$ is a polynomial in $\mathbb{P}_2$. Thus, we have:
    \[ \text{Im}(T) = W = \mathbb{P}_2 \]
    Thus $T$ is onto (surjective) but not one-to-one (injective) since $\text{Ker}(T) \neq \{\vec{0}\}$.
\end{eg}

\section{Independent Sets and Bases}
\subsection{Independent Sets}
\begin{definition}[Independent Set]
    A set of vectors $\{\vec{v_1}, \vec{v_2}, \ldots, \vec{v_p}\}$ in a vector space $V$ is said to be linearly independent if the only solution to the equation:
    \[ c_1 \vec{v_1} + c_2 \vec{v_2} + \ldots + c_p \vec{v_p} = \vec{0} \]
    is the trivial solution $c_1 = c_2 = \ldots = c_p = 0$. If there exists a non-trivial solution (i.e., at least one $c_i \neq 0$), then the set is said to be linearly dependent.
\end{definition}
\begin{theorem}
    A set of vectors $\{\vec{v_1}, \vec{v_2}, \ldots, \vec{v_p}\}$ of at least two vectors (where $\vec{v_1} \neq \vec{0}$) is linearly dependent if and only if at least one of the vectors can be expressed as a linear combination of the others.
\end{theorem}

\begin{eg}
    Let $V = \mathbb{P}_2$. Let's consider the set of polynomials $\{p_1(x), p_2(x), p_3(x)\}$ where:
    \[ p_1(x) = 1, \quad p_2(x) = x, \quad p_3(x) = 4 - x \]
    We want to determine if this set is linearly independent or dependent. We need to solve the equation:
    \[
        c_1 p_1(x) + c_2 p_2(x) + c_3 p_3(x) = 0 \implies c_1 (1) + c_2 (x) + c_3 (4 - x) = 0
    \]
    This simplifies to:
    \[ (c_1 + 4c_3) + (c_2 - c_3)x = 0 \]
    For this equation to hold for all $x$, the coefficients of both the constant term and the coefficient of $x$ must be zero. This gives us the system:
    \[ \begin{cases}
        c_1 + 4c_3 = 0 \\
        c_2 - c_3 = 0
    \end{cases} \]
    Solving this system, we find:
    \[ c_1 = -4c_3, \quad c_2 = c_3 \]
    This means that there are infinitely many solutions depending on the value of $c_3$. For example, if we choose $c_3 = 1$, we get:
    \[ c_1 = -4, \quad c_2 = 1, \quad c_3 = 1 \]
    Thus, we have a non-trivial solution:
    \[ -4p_1(x) + 1p_2(x) + 1p_3(x) = 0 \]
    Therefore, the set $\{p_1(x), p_2(x), p_3(x)\}$ is linearly dependent.
\end{eg}

\subsection{Bases}
\begin{definition}[Basis]
    A basis for a vector space $V$ is a set of vectors $\{\vec{b_1}, \vec{b_2}, \ldots, \vec{b_n}\}$ in $V$ that is both linearly independent and spans the entire space $V$. This means that any vector $\vec{v} \in V$ can be expressed as a linear combination of the basis vectors:
    \[ \vec{v} = c_1 \vec{b_1} + c_2 \vec{b_2} + \ldots + c_n \vec{b_n} \]
    where $c_1, c_2, \ldots, c_n \in \mathbb{R}$.
\end{definition}
Remark that:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item If $\vec{b_1}, \vec{b_2}, \ldots, \vec{b_p} \in V$ are linearly independent then they form a basis for $Span\{\vec{b_1}, \vec{b_2}, \ldots, \vec{b_p}\}$.
    \item If $\vec{b_1}, \vec{b_2}, \ldots, \vec{b_p} \in V$ form a basis for $H \subseteq V$, then $\vec{b_1}, \vec{b_2}, \ldots, \vec{b_p}$ are in $H$ because $H = Span\{\vec{b_1}, \vec{b_2}, \ldots, \vec{b_p}\}$.
\end{itemize}

\begin{eg}
    Let $V = \mathbb{R}^2$ and $\vec{b_1} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \vec{b_2} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$. We claim that $(\vec{b_1}, \vec{b_2})$ is a basis for $V$.
\end{eg}

\begin{eg}
    Let $V = \mathbb{R}^3$ and $\vec{b_1} = \begin{pmatrix}
        1 \\ 1 \\ 0
    \end{pmatrix}, \vec{b_2} = \begin{pmatrix}
        1 \\ -1 \\ 0
    \end{pmatrix}$. We claim that $(\vec{b_1}, \vec{b_2})$ is a basis for the subspace $H = \{\vec{x} \in \mathbb{R}^3 : x_3 = 0\} = Span\{\vec{b_1}, \vec{b_2}\}$ which is a plane in $\mathbb{R}^3$.
\end{eg}

\begin{eg}
    Let $V = \mathbb{R}^2$ and $\vec{b_1} = \begin{pmatrix}
        1 \\ 0
    \end{pmatrix}, \vec{b_2} = \begin{pmatrix}
        0 \\ 1
    \end{pmatrix}, \vec{b_3} = \begin{pmatrix}
        0 \\ -1
    \end{pmatrix}$. We claim that $(\vec{b_1}, \vec{b_2}, \vec{b_3})$ is not a basis for $V$ since the set is linearly dependent (because $\vec{b_3} = -\vec{b_2}$).
\end{eg}

\begin{eg}
    Let $V = \mathbb{P}_2$ and $\vec{b_1}(t) = 1$, $\vec{b_2}(t) = t$, $\vec{b_3}(t) = t^2$. We claim that $(\vec{b_1}, \vec{b_2}, \vec{b_3})$ is a basis for $V$.
\end{eg}

\begin{eg}
    Let $V = \mathbb{P}_3$ and $\vec{b_1}(t) = 1$, $\vec{b_2}(t) = t$, $\vec{b_3}(t) = t^2$. We claim that $(\vec{b_1}, \vec{b_2}, \vec{b_3})$ is not a basis for $V$ since it does not span $V$ (for example, the polynomial $t^3$ cannot be expressed as a linear combination of $\vec{b_1}, \vec{b_2}, \vec{b_3}$).
\end{eg}

\begin{eg}
    Let $A = \begin{bmatrix}
        | & & | \\
        \vec{a_1} & \ldots & \vec{a_n} \\
        | & & |
    \end{bmatrix} \in \mathbb{R}^{n \times n}$ be an invertible matrix. We claim that $(\vec{a_1}, \ldots, \vec{a_n})$ is a basis for $\mathbb{R}^n$ since the columns of $A$ are linearly independent and span $\mathbb{R}^n$.
\end{eg}
Note that if $A$ is not invertible, then the columns of $A$ do not form a basis for $\mathbb{R}^n$ since they are linearly dependent.
\begin{eg}
    The columns of $I_n$ form the standard basis for $\mathbb{R}^n$ which is given by:
    \[ \left\{ \begin{pmatrix}
        1 \\ 0 \\ \vdots \\ 0
    \end{pmatrix}, \begin{pmatrix}
        0 \\ 1 \\ \vdots \\ 0
    \end{pmatrix}, \ldots, \begin{pmatrix}
        0 \\ 0 \\ \vdots \\ 1
    \end{pmatrix} \right\} = \{ \vec{e_1}, \ldots, \vec{e_n} \} \]
\end{eg}

\subsection{Finding a Basis with a Dependent Set}
\begin{eg}
    Let $\vec{v_1} = \begin{pmatrix}
        1 \\ 1
    \end{pmatrix}$ and $\vec{v_2} = \begin{pmatrix}
        -2 \\ -2
    \end{pmatrix}$. We want to find a basis for $H = Span\{\vec{v_1}, \vec{v_2}\}$. Since $\vec{v_2} = -2\vec{v_1}$, the set $\{\vec{v_1}, \vec{v_2}\}$ is linearly dependent. Thus, we can remove $\vec{v_2}$ and keep only $\vec{v_1}$. Therefore, a basis for $H$ is given by $\{\vec{v_1}\}$ (or another basis could be $\{\vec{v_2}\}$).
\end{eg}

\begin{eg}
    Let $\vec{v_1} = \begin{pmatrix}
        0 \\ 2 \\ -1
    \end{pmatrix}, \vec{v_2} = \begin{pmatrix}
        2 \\ 2 \\ 0
    \end{pmatrix}$ and $\vec{v_3} = \begin{pmatrix}
        6 \\ 16 \\ -5
    \end{pmatrix}$ be linearly dependent vectors in $\mathbb{R}^3$ since:
    \[ 5\vec{v_1} + 3\vec{v_2} = \vec{v_3} \]
    For any $\vec{v}$ that is in $H = Span\{\vec{v_1}, \vec{v_2}, \vec{v_3}\}$, we can write:
    \[ \vec{v} = c_1 \vec{v_1} + c_2 \vec{v_2} + c_3 \vec{v_3} \]
    Substituting $\vec{v_3}$, we get:
    \[ \vec{v} = c_1 \vec{v_1} + c_2 \vec{v_2} + c_3 (5\vec{v_1} + 3\vec{v_2}) = (c_1 + 5c_3)\vec{v_1} + (c_2 + 3c_3)\vec{v_2} \]
    Thus, we can express $\vec{v}$ as a linear combination of only $\vec{v_1}$ and $\vec{v_2}$. Therefore, a basis for $H$ is given by $\{\vec{v_1}, \vec{v_2}\}$ and thus $Span\{\vec{v_1}, \vec{v_2}, \vec{v_3}\} = Span\{\vec{v_1}, \vec{v_2}\} = H$.
\end{eg}

\begin{theorem}
    Let $F = (\vec{v_1, \ldots, \vec{v_p}})$ be a set of vectors in a vector space $V$ and $H = Span\{\vec{v_1}, \ldots, \vec{v_p}\}$. We have:
    \begin{itemize}
        \item If any vector $\vec{v_k} \in F$ is a linear combination of the other vectors in $F$, then we can remove $\vec{v_k}$ from $F$ without changing the span of $H$.
        \item If $H \neq \{\vec{0}\}$, then we can remove vectors from $F$ until we obtain a basis for $H$.
    \end{itemize}
\end{theorem}

\subsection{Basis of Kernel and Image}
\begin{eg}
    Let $A = \begin{bmatrix}
        -3 & 6 & -1 & 1 & -7 \\
        1 & -2 & 2 & 3 & -1 \\
        2 & -4 & 5 & 8 & -4
    \end{bmatrix}$ the same matrix as in another example. We had found that the kernel of $A$ is given by:
    \[ \text{Ker}(A) = Span\left\{ \begin{bmatrix}
        2 \\ 1 \\ 0 \\ 0 \\ 0
    \end{bmatrix}, \begin{bmatrix}
        1 \\ 0 \\ -2 \\ 1 \\ 0
    \end{bmatrix}, \begin{bmatrix}
        -3 \\ 0 \\ 2 \\ 0 \\ 1
    \end{bmatrix} \right\} \]
    We know that a basis for $\text{Im} A$ can be extracted from the columns of $A$. To find which columns to take, we look at the vectors that form the kernel of $A$. We can rewrite the kernel as:
    \[
        \begin{cases}
            2\vec{a_1} + \vec{a_2} = \vec{0} \\
            \vec{a_1} - 2\vec{a_3} + \vec{a_4} = \vec{0} \\
            -3\vec{a_1} + 2\vec{a_3} + \vec{a_5} = \vec{0}
        \end{cases}
    \]
    where $\vec{a_1}, \vec{a_2}, \vec{a_3}, \vec{a_4}, \vec{a_5}$ are the columns of $A$. We can rewrite this system in terms of $\vec{a_1}$ and $\vec{a_3}$:
    \[ \begin{cases}
        \vec{a_2} = -2\vec{a_1} \\
        \vec{a_4} = -\vec{a_1} + 2\vec{a_3} \\
        \vec{a_5} = 3\vec{a_1} - 2\vec{a_3}
    \end{cases} \]
    Thus, we can express $\vec{a_2}, \vec{a_4}, \vec{a_5}$ as linear combinations of $\vec{a_1}$ and $\vec{a_3}$. Therefore, a basis for the image of $A$ is given by the columns $\vec{a_1}$ and $\vec{a_3}$:
    \[ \left\{ \begin{bmatrix}
        -3 \\ 1 \\ 2
    \end{bmatrix}, \begin{bmatrix}
        -1 \\ 2 \\ 5
    \end{bmatrix} \right\} \]
\end{eg}

\begin{theorem}
    The pivot columns of a matrix form a basis for the image of the matrix.
\end{theorem}

\subsection{System of Coordinates}
\begin{theorem}
    Let $B = (\vec{b_1}, \vec{b_2}, \ldots, \vec{b_n})$ be a basis for a vector space $V$. Then, for each vector $\vec{v} \in V$, there exists a unique set of scalars $c_1, c_2, \ldots, c_n$ such that:
    \[ \vec{v} = c_1 \vec{b_1} + c_2 \vec{b_2} + \ldots + c_n \vec{b_n} \]
    % The scalars $c_1, c_2, \ldots, c_n$ are called the coordinates of $\vec{v}$ relative to the basis $B$. The vector of coordinates is denoted by:
    % \[ [\vec{v}]_B = \begin{pmatrix}
    %     c_1 \\ c_2 \\ \vdots \\ c_n
    % \end{pmatrix} \]
\end{theorem}
\begin{proof}
    Let's prove that the representation exists and is unique. \\
    \textbf{Existence:} Since $B$ is a basis for $V$, it spans $V$. Therefore, for any vector $\vec{v} \in V$, there exist scalars $c_1, c_2, \ldots, c_n$ such that:
    \[ \vec{v} = c_1 \vec{b_1} + c_2 \vec{b_2} + \ldots + c_n \vec{b_n} \]
    \textbf{Uniqueness:} Suppose there are two different representations of $\vec{v}$:
    \[ \vec{v} = c_1 \vec{b_1} + c_2 \vec{b_2} + \ldots + c_n \vec{b_n} \]
    and
    \[ \vec{v} = d_1 \vec{b_1} + d_2 \vec{b_2} + \ldots + d_n \vec{b_n} \]
    where $c_i \neq d_i$ for at least one $i$. Subtracting these two equations, we get:
    \[ \vec{0} = (c_1 - d_1) \vec{b_1} + (c_2 - d_2) \vec{b_2} + \ldots + (c_n - d_n) \vec{b_n} \]
    Since $B$ is a basis, the vectors $\vec{b_1}, \vec{b_2}, \ldots, \vec{b_n}$ are linearly independent. Therefore, the only solution to the above equation is:
    \[ c_1 - d_1 = 0, c_2 - d_2 = 0, \ldots, c_n - d_n = 0 \]
    which implies that $c_i = d_i$ for all $i$. This contradicts our assumption that there exists at least one $i$ such that $c_i \neq d_i$. Hence, the representation is unique.
\end{proof}

\begin{definition}[Coordinates Relative to a Basis]
    The scalars $c_1, c_2, \ldots, c_n$ are called the coordinates of $\vec{v}$ relative to the basis $B$. The vector of coordinates is denoted by:
    \[ [\vec{v}]_B = \begin{bmatrix}
        c_1 \\ c_2 \\ \vdots \\ c_n
    \end{bmatrix} \]
\end{definition}

\begin{eg}
    Let $\mathcal{E} = (\vec{e_1}, \vec{e_2})$ be the standard basis for $\mathbb{R}^2$ and let $\vec{v} = \begin{bmatrix}
        5 \\ 3
    \end{bmatrix}$. We want to find the coordinates of $\vec{v}$ relative to the basis $\mathcal{E}$. We can express $\vec{v}$ as a linear combination of the basis vectors:
    \[ \vec{v} = 5\vec{e_1} + 3\vec{e_2} \]
    Thus, the coordinates of $\vec{v}$ relative to the basis $\mathcal{E}$ are:
    \[ [\vec{v}]_{\mathcal{E}} = \begin{bmatrix}
        5 \\ 3
    \end{bmatrix} \]
\end{eg}
More generally, if $V = \mathbb{R}^n$ and $\mathcal{E} = (\vec{e_1}, \vec{e_2}, \ldots, \vec{e_n})$ is the standard basis for $\mathbb{R}^n$, then for any vector $[\vec{v}]_{\mathcal{E}} = \vec{v}$

\begin{eg}
    Let $\mathcal{B} = (\vec{b_1}, \vec{b_2})$ with $\vec{b_1} = \begin{bmatrix}1 \\ 0\end{bmatrix}$ and $\vec{b_2} = \begin{bmatrix}1 \\ 2\end{bmatrix}$ be a basis for $\mathbb{R}^2$ and let [$\vec{v}]_{\mathcal{B}} = \begin{bmatrix}
        3 \\ 1
    \end{bmatrix}$. We want to find the coordinates of $\vec{v}$ in the standard basis $\mathcal{E}$. We can express $\vec{v}$ as a linear combination of the basis vectors:
    \[ \vec{v} = 3\vec{b_1} + 1\vec{b_2} = 3\begin{bmatrix}1 \\ 0\end{bmatrix} + 1\begin{bmatrix}1 \\ 2\end{bmatrix} = \begin{bmatrix}
        3 + 1 \\ 0 + 2
    \end{bmatrix} = \begin{bmatrix}
        4 \\ 2
    \end{bmatrix} \]
    Thus, the coordinates of $\vec{v}$ in the standard basis $\mathcal{E}$ are:
    \[ [\vec{v}]_{\mathcal{E}} = \begin{bmatrix}
        4 \\ 2
    \end{bmatrix} \]
    Now let $[\vec{u}]_{\mathcal{E}} = \begin{bmatrix}
        u_1 \\ u_2
    \end{bmatrix}$. We want to find the coordinates of $\vec{u}$ relative to the basis $\mathcal{B}$. We need to solve the equation:
    \[ \vec{u} = c_1 \vec{b_1} + c_2 \vec{b_2} \iff  \begin{bmatrix}
        1 & 1 \\
        0 & 2
    \end{bmatrix} \begin{bmatrix}
        c_1 \\ c_2
    \end{bmatrix} = \begin{bmatrix}
        u_1 \\ u_2
    \end{bmatrix} \]
    We can solve this system using the inverse of the matrix:
    \[ \begin{bmatrix}
        c_1 \\ c_2
    \end{bmatrix} = \begin{bmatrix}
        1 & 1 \\
        0 & 2
    \end{bmatrix}^{-1} \begin{bmatrix}
        u_1 \\ u_2
    \end{bmatrix} = \begin{bmatrix}
        1 & -\frac{1}{2} \\
        0 & \frac{1}{2}
    \end{bmatrix} \begin{bmatrix}
        u_1 \\ u_2
    \end{bmatrix} = \begin{bmatrix}
        u_1 - \frac{u_2}{2} \\ \frac{u_2}{2}
    \end{bmatrix} \]
    Thus, the coordinates of $\vec{u}$ relative to the basis $\mathcal{B}$ are:
    \[ [\vec{u}]_{\mathcal{B}} = \begin{bmatrix}
        u_1 - \frac{u_2}{2} \\ \frac{u_2}{2}
    \end{bmatrix} \]
    Let's take $[\vec{u}]_{\mathcal{E}} = \begin{bmatrix}
        7 \\ 4
    \end{bmatrix}$. Then, we have:
    \[ [\vec{u}]_{\mathcal{B}} = \begin{bmatrix}
        7 - \frac{4}{2} \\ \frac{4}{2}
    \end{bmatrix} = \begin{bmatrix}
        5 \\ 2
    \end{bmatrix} \]
\end{eg}
More generally, if $\mathcal{B} = (\vec{b_1}, \ldots, \vec{b_n})$ is a basis of $\mathbb{R}^n$, we define the matrix:
\[ P_{\mathcal{B}} = \begin{bmatrix}
    | & & | \\
    \vec{b_1} & \ldots & \vec{b_n} \\
    | & & |
\end{bmatrix} \in \mathbb{R}^{n \times n} \]
Then, for any vector $[\vec{v}]_{\mathcal{B}} = \begin{bmatrix}
    c_1 \\ c_2 \\ \vdots \\ c_n
\end{bmatrix}$, we have:
\[ \vec{v} = [\vec{v}]_{\mathcal{E}} = P_{\mathcal{B}} [\vec{v}]_{\mathcal{B}} \]
And for any vector $[\vec{u}]_{\mathcal{E}} = \begin{bmatrix}
    u_1 \\ u_2 \\ \vdots \\ u_n
\end{bmatrix}$, we have:
\[ [\vec{u}]_{\mathcal{B}} = P_{\mathcal{B}}^{-1} [\vec{u}]_{\mathcal{E}} \]
\begin{definition}[Change of Basis Matrix]
    The matrix $P_{\mathcal{B}}$ is called the change of basis matrix from the basis $\mathcal{B}$ to the standard basis $\mathcal{E}$. And the matrix $P_{\mathcal{B}}^{-1}$ represents a transformation that maps coordinates from the standard basis $\mathcal{E}$ to the basis $\mathcal{B}$.
\end{definition}

\begin{theorem}
    Let $B$ be a basis for a vector space $V$. Then, the transformation that maps each vector $\vec{v} \in V$ to its coordinate vector $[\vec{v}]_B$ is a linear transformation from $V$ to $\mathbb{R}^n$ that is one-to-one and onto (thus bijective).
\end{theorem}

\begin{eg}
    Let the canonical basis of $\mathbb{P}_3$ is given by $\mathcal{B} = (1, t, t^2, t^3)$. Consider the polynomial $p(t) = a_0 + a_1 t + a_2 t^2 + a_3 t^3$. The coordinates of $p(t)$ relative to the basis $\mathcal{B}$ are given by:
    \[ [p(t)]_{\mathcal{B}} = \begin{bmatrix}
        a_0 \\ a_1 \\ a_2 \\ a_3
    \end{bmatrix} \]
    Thus $\mathbb{P}_3$ is isomorphic to $\mathbb{R}^4$.
\end{eg}

\begin{eg}
    Let $V = \mathbb{R}^{2 \times 3}$ be the vector space of all $2 \times 3$ matrices. It's canonical basis is given by:
    \[ \mathcal{B} = \left( E_{11}, E_{12}, E_{13}, E_{21}, E_{22}, E_{23} \right) \]
    where $E_{ij}$ is the matrix with a $1$ in the $(i,j)$ position and $0$ elsewhere. Consider the matrix:
    \[ M = \begin{bmatrix}
        m_{11} & m_{12} & m_{13} \\
        m_{21} & m_{22} & m_{23}
    \end{bmatrix} \]
    The coordinates of $M$ relative to the basis $\mathcal{B}$ are given by:
    \[ [M]_{\mathcal{B}} = \begin{bmatrix}
        m_{11} \\ m_{12} \\ m_{13} \\ m_{21} \\ m_{22} \\ m_{23}
    \end{bmatrix} \]
    Thus $V$ is isomorphic to $\mathbb{R}^6$.
\end{eg}

\begin{eg}
    We want to find a basis for all symetrical matrices in $\mathbb{R}^{2 \times 2}$. Let:
    \[ M = \begin{bmatrix}
        a & b \\
        b & c
    \end{bmatrix} \]
    We can express $M$ as a linear combination of the following matrices:
    \[ M = a \begin{bmatrix}
        1 & 0 \\
        0 & 0
    \end{bmatrix} + b \begin{bmatrix}
        0 & 1 \\
        1 & 0
    \end{bmatrix} + c \begin{bmatrix}
        0 & 0 \\
        0 & 1
    \end{bmatrix} \]
    Thus, a basis for the space of all symetrical matrices in $\mathbb{R}^{2 \times 2}$ is given by:
    \[ \left\{ \begin{bmatrix}
        1 & 0 \\
        0 & 0
    \end{bmatrix}, \begin{bmatrix}
        0 & 1 \\
        1 & 0
    \end{bmatrix}, \begin{bmatrix}
        0 & 0 \\
        0 & 1
    \end{bmatrix} \right\} \]
    Thus, the space of all symetrical matrices in $\mathbb{R}^{2 \times 2}$ is isomorphic to $\mathbb{R}^3$.
\end{eg}

\section{Dimension of a Vector Space}
\begin{definition}[Dimension]
    The dimension of a vector space $V$, denoted by $\text{dim}(V)$, is defined as the number of vectors in any basis for $V$. If $V$ has no finite basis, then we say that $V$ is infinite-dimensional.
\end{definition}
By convention, we define the dimension of the zero vector space $\{\vec{0}\}$ to be $0$ since the empty set is a basis for $\{\vec{0}\}$.

\begin{eg}
    Let $V = \mathbb{P}_3$. A basis for $V$ is given by $\{1, t, t^2, t^3\}$. Thus, the dimension of $V$ is:
    \[ \text{dim}(V) = 4 \]
    Similarly, for $V = \mathbb{R}^{n \times m}$ the dimension is given by:
    \[ \text{dim}(V) = nm \]
\end{eg}
All subspace of $\mathbb{R}^n$ have a finite dimension less than or equal to $n$.

\begin{theorem}
    All bases of a finite-dimensional vector space have the same number of vectors.
\end{theorem}
Let $H$ be a subspace of $V$ (dim$(V) = p$). Some remarks:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item All free sets of vectors in $H$ can be extended to form a basis for $H$.
    \item dim$(H) \leq$ dim$(V)$.
\end{itemize}

\begin{theorem}
    Let $V$ be a vector space of dimension $n$. Then:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Any set of more than $n$ vectors in $V$ is linearly dependent.
        \item Any set of fewer than $n$ vectors in $V$ does not span $V$.
        \item Any linearly independent set of exactly $n$ vectors in $V$ is a basis for $V$.
        \item Any set of exactly $n$ vectors that spans $V$ is a basis for $V$.
    \end{itemize}
\end{theorem}

\subsection{Dimension of Kernel and Image}
\begin{theorem}
    The dimension of Ker$(A)$ is equal to the number of free variables in the homogeneous system $A\vec{x} = \vec{0}$ and the dimension of Im$(A)$ is equal to the number of pivot columns in $A$.
\end{theorem}

\begin{eg}
    Let $A = \begin{bmatrix}
        -3 & 6 & -1 & 1 & -7 \\
        1 & -2 & 2 & 3 & -1 \\
        2 & -4 & 5 & 8 & -4
    \end{bmatrix}$ the same matrix as a previous example. In reduce row echelon form, we had:
    \[ \begin{bmatrix}
        1 & -2 & 0 & -1 & 3 \\
        0 & 0 & 1 & 2 & -2 \\
        0 & 0 & 0 & 0 & 0
    \end{bmatrix} \]
    We had found that the kernel of $A$ is given by:
    \[ \text{Ker}(A) = Span\left\{ \begin{bmatrix}
        2 \\ 1 \\ 0 \\ 0 \\ 0
    \end{bmatrix}, \begin{bmatrix}
        1 \\ 0 \\ -2 \\ 1 \\ 0
    \end{bmatrix}, \begin{bmatrix}
        -3 \\ 0 \\ 2 \\ 0 \\ 1
    \end{bmatrix} \right\} \]
    And a basis for the image of $A$ is given by the columns $\vec{a_1}$ and $\vec{a_3}$:
    \[ \text{Im}(A) = Span\left\{ \begin{bmatrix}
        -3 \\ 1 \\ 2
    \end{bmatrix}, \begin{bmatrix}
        -1 \\ 2 \\ 5
    \end{bmatrix} \right\} \]
    Thus, we have:
    \[ \text{dim}(\text{Ker}(A)) = 3 \quad \text{and} \quad \text{dim}(\text{Im}(A)) = 2 \]
    Note that the number of free variables in the homogeneous system $A\vec{x} = \vec{0}$ is $3$ (corresponding to $x_2, x_4, x_5$) and the number of pivot columns in $A$ is $2$ (corresponding to columns $1$ and $3$).
\end{eg}

\begin{definition}[Rank]
    The rank of a matrix $A$, denoted by $\text{rank}(A)$, is defined as the dimension of its image:
    \[ \text{rank}(A) = \text{dim}(\text{Im}(A)) \]
\end{definition}
Note that for any matrix $A \in \mathbb{R}^{m \times n}$, the following holds:
\[ \text{rank}(A) \leq \min(m, n) \]
and:
\[ \text{dim}(\text{Ker}(A)) \leq n \]

\begin{theorem}[Rank-Nullity Theorem]
    For any matrix $A$ with $n$ columns:
    \[ \text{dim}(\text{Ker}(A)) + \text{dim}(\text{Im}(A)) = n \]
\end{theorem}

\subsection{Row Space}
\begin{definition}[Row Space]
    The row space of a matrix $A$, denoted by Row$(A)$, is the subspace of $\mathbb{R}^n$ spanned by the row vectors of $A$.
    \[
        \text{Row}(A) = \text{Im}(A^T) = Span\{\text{row}_1(A), \text{row}_2(A), \ldots, \text{row}_m(A)\}
    \]
\end{definition}

\begin{eg}
    Let $A = \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6
    \end{bmatrix}$. The row space of $A$ is given by:
    \[ \text{Row}(A) = Span\left\{ \begin{bmatrix}
        1 & 2 & 3
    \end{bmatrix}, \begin{bmatrix}
        4 & 5 & 6
    \end{bmatrix} \right\} \]
    A basis for Row$(A)$ is given by the two row vectors since they are linearly independent. Thus, dim(Row$(A)$) = 2.
\end{eg}
Remark that:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item dim(Row$(A)$) $\leq n$, because Row$(A) \subseteq \mathbb{R}^n$.
    \item dim(Row$(A)$) $\leq m$, because Row$(A)$ is spanned by $m$ vectors.
\end{itemize}
Let $A$ be a matrix and $B$ be an invertible matrix of appropriate size, then:
\[ \text{Im}(B A) = \text{Im}(A) \]
\begin{proof}
    Let $\vec{y} \in \text{Im}(B A)$. Then, there exists a vector $\vec{x}$ such that:
    \[ \vec{y} = B A \vec{x} \]
    Since $B$ is invertible (and thus a bijection), we can multiply both sides by $B^{-1}$:
    \[ B^{-1} \vec{y} = A \vec{x} \]
    This shows that $B^{-1} \vec{y} \in \text{Im}(A)$, and thus $\vec{y} \in \text{Im}(A)$. Therefore, we have:
    \[ \text{Im}(B A) \subseteq \text{Im}(A) \]
    Conversely, let $\vec{z} \in \text{Im}(A)$. Then, there exists a vector $\vec{x}$ such that:
    \[ \vec{z} = A \vec{x} \]
    Multiplying both sides by $B$ (since $B$ is invertible), we get:
    \[ B \vec{z} = B A \vec{x} \]
    This shows that $B \vec{z} \in \text{Im}(B A)$, and thus $\vec{z} \in \text{Im}(B A)$. Therefore, we have:
    \[ \text{Im}(A) \subseteq \text{Im}(B A) \]
    Combining both inclusions, we conclude that:
    \[ \text{Im}(B A) = \text{Im}(A) \]
\end{proof}

\begin{theorem}
    For any matrix $A$, the following holds:
    \[
        \text{rank}(A) = \text{rank}(A^T)
    \]
    i.e.:
    \[
        \text{dim(Im}(A)) = \text{dim(Row}(A))
    \]
\end{theorem}
\begin{proof}
    Let $A$ be an $m \times n$ matrix. We can perform a series of elementary row operations on $A$ to obtain its row echelon form $R$. These row operations can be represented by multiplying $A$ by an invertible matrix $B$ from the left:
    \[ R = B A \]
    Since $B$ is invertible, we have:
    \[ \text{Im}(R) = \text{Im}(B A) = \text{Im}(A) \]
    The rank of $R$ is equal to the number of non-zero rows in $R$, which is also equal to the rank of $A$. Now, consider the transpose of $R$:
    \[ R^T = (B A)^T = A^T B^T \]
    Since $B^T$ is also invertible, we have:
    \[ \text{Im}(R^T) = \text{Im}(A^T B^T) = \text{Im}(A^T) \]
    The rank of $R^T$ is equal to the number of non-zero columns in $R$, which is equal to the rank of $A$. Therefore, we conclude that:
    \[ \text{rank}(A) = \text{rank}(R) = \text{rank}(R^T) = \text{rank}(A^T) \]
\end{proof}