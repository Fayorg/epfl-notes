\chapter{Vectorial Spaces and Subspaces}
With this chapter, we add another abstraction layer to our mathematical framework. We will get new mathematical tools that will allow us to work with more complex objects in a more general way.

\section{Vectorial Spaces}
\begin{definition}[Vectorial Space]
    A vectorial space (or vector space) is a set $V$ together with two operations: vector addition and scalar multiplication, satisfying these properties for all $\vec{u}, \vec{v}, \vec{w} \in V$ and $c,d \in \mathbb{R}$:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $\vec{u} + \vec{v} = \vec{v} + \vec{u}$ (Commutativity of vector addition)
        \item $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$ (Associativity of vector addition)
        \item There exists a zero vector $\vec{0} \in \mathbb{R}^n$ such that $\vec{u} + \vec{0} = \vec{u}$ (Existence of additive identity)
        \item For each $\vec{u} \in \mathbb{R}^n$, there exists an additive inverse $-\vec{u} \in \mathbb{R}^n$ such that $\vec{u} + (-\vec{u}) = \vec{0}$ (Existence of additive inverse)
        \item $c(\vec{u} + \vec{v}) = c\vec{u} + c\vec{v}$ (Distributivity of scalar multiplication over vector addition)
        \item $(c + d)\vec{u} = c\vec{u} + d\vec{u}$ (Distributivity of scalar addition over scalar multiplication)
        \item $c(d\vec{u}) = (cd)\vec{u}$ (Associativity of scalar multiplication)
        \item $1\vec{u} = \vec{u}$ (Existence of multiplicative identity)
    \end{itemize}
\end{definition}
Instead of working with specific objects like $\mathbb{R}^n$, we can now work with any set $V$ that satisfies the properties of a vectorial space. This abstraction allows us to apply the same mathematical tools and techniques previously seen to a wider variety of objects.

\begin{definition}[Vector]
    Previously defined vectors in $\mathbb{R}^n$ are now called vectors in a vectorial space $V$. The operations of vector addition and scalar multiplication are defined as per the properties of the vectorial space.
\end{definition}

\begin{eg}
    Let $V = \mathbb{R}^n$ with the usual operations of vector addition and scalar multiplication in $\mathbb{R}^n$. 
\end{eg}

\begin{eg}
    Let $V = \mathbb{R}^{n \times n}$ with the usual operations of matrix addition and scalar multiplication in $\mathbb{R}^{n \times n}$. \\
    Note that in this case, the elements of $V$ are matrices but also vectors in the vectorial space $V$.
\end{eg}

\subsection{Polynomial Spaces}
\begin{definition}[Polynomial Space]
    Let $P_n$ be the set of all polynomials of degree less than or equal to $n$. The operations of polynomial addition and scalar multiplication are defined as follows:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Polynomial Addition: For $p(x), q(x) \in P_n$, their sum is defined as $(p + q)(x) = p(x) + q(x)$.
        \item Scalar Multiplication: For a scalar $c \in \mathbb{R}$ and a polynomial $p(x) \in P_n$, the scalar multiplication is defined as $(cp)(x) = c \cdot p(x)$.
    \end{itemize}
    With these operations, we can easily verify that $P_n$ satisfies all the properties of a vectorial space. Thus, $P_n$ is a vectorial space.
\end{definition}
\begin{proof}
    Let's prove that for any two polynomials $p(x), q(x) \in P_n$ and any scalar $c \in \mathbb{R}$, the operations of polynomial are well-defined. We have:
    \[
        p(x) = a_0 + a_1 x + a_2 x^2 + ... + a_n x^n \quad \text{and} \quad q(x) = b_0 + b_1 x + b_2 x^2 + ... + b_n x^n
    \]
    where $a_0, a_1, ..., a_n, b_0, b_1, ..., b_n \in \mathbb{R}$. \\
    \textbf{Polynomial Addition:}
    \[(p + q)(x) = p(x) + q(x) = (a_0 + b_0) + (a_1 + b_1)x + (a_2 + b_2)x^2 + ... + (a_n + b_n)x^n\]
    The resulting polynomial $(p + q)(x)$ is also of degree less than or equal to $n$, so it belongs to $P_n$. \\
    \textbf{Scalar Multiplication:}
    \[(cp)(x) = c \cdot p(x) = c \cdot (a_0 + a_1 x + a_2 x^2 + ... + a_n x^n) = (c a_0) + (c a_1)x + (c a_2)x^2 + ... + (c a_n)x^n\]
    The resulting polynomial $(cp)(x)$ is also of degree less than or equal to $n$, so it belongs to $P_n$. \\
    Thus both operations are well-defined, and we can easily verify that $P_n$ satisfies all the properties of a vectorial space.
\end{proof}

\begin{eg}
    Let $V$ a vectorial space. Let's show that (1) the zero vector is unique, (2) the additive inverse of each vector is unique and (3) $0 \vec{u} = \vec{0}$ and $c \vec{0} = \vec{0}$ for all $c \in \mathbb{R}$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Let's assume that there are two vectors $\vec{0}$ and $\vec{0'}$ that both satisfy the property of the additive identity. Then, we have:
        \[\vec{u} + \vec{0} = \vec{u} \quad \text{and} \quad \vec{u} + \vec{0'} = \vec{u}\]
        By subtracting $\vec{u}$ from both sides of the second equation, we get:
        \[ \vec{u} + \vec{0} = \vec{u} = \vec{u} + \vec{0'} \implies \vec{0'} = \vec{0}\]
        Thus, the zero vector is unique.
        \item Let's assume that there are two vectors $\vec{v}$ and $\vec{v'}$ that both satisfy the property of the additive inverse for a vector $\vec{u}$. Then, we have:
        \[\vec{u} + \vec{v} = \vec{0} \quad \text{and} \quad \vec{u} + \vec{v'} = \vec{0}\]
        By subtracting $\vec{u}$ from both sides of the second equation, we get:
        \[ \vec{u} + \vec{v} = \vec{0} = \vec{u} + \vec{v'} \implies \vec{v'} = \vec{v}\]
        Thus, the additive inverse of each vector is unique.
        \item To show that $0 \vec{u} = \vec{0}$, we can use the distributive property of scalar multiplication over scalar addition:
        \[0 \vec{u} = (0 + 0) \vec{u} = 0 \vec{u} + 0 \vec{u}\]
        By subtracting $0 \vec{u}$ from both sides, we get:
        \[0 \vec{u} = \vec{0}\]
        To show that $c \vec{0} = \vec{0}$ for any scalar $c \in \mathbb{R}$, we can use the distributive property of scalar multiplication over vector addition:
        \[c \vec{0} = c (\vec{0} + \vec{0}) = c \vec{0} + c \vec{0}\]
        By subtracting $c \vec{0}$ from both sides, we get:
        \[c \vec{0} = \vec{0}\]
        Thus, we have shown that $0 \vec{u} = \vec{0}$ and $c \vec{0} = \vec{0}$ for all $c \in \mathbb{R}$.
    \end{itemize}
\end{eg}

\section{Subspaces}
\begin{definition}[Subspace]
    A subspace $W$ of a vectorial space $V$ is a subset of $V$ that is itself a vectorial space under the same operations of vector addition and scalar multiplication defined on $V$. In other words, $W$ is a subspace of $V$ if:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector of $V$ is in $W$.
        \item $W$ is closed under vector addition: For any $\vec{u}, \vec{v} \in W$, the sum $\vec{u} + \vec{v}$ is also in $W$.
        \item $W$ is closed under scalar multiplication: For any $\vec{u} \in W$ and any scalar $c \in \mathbb{R}$, the product $c\vec{u}$ is also in $W$.
    \end{itemize}
\end{definition}

\begin{eg}
    Let $V = \mathbb{R}^n$ and $H = \{\vec{x} \in \mathbb{R}^n : x_1 + \ldots + x_n = 0\}$. Let's show that $H$ is a subspace of $V$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector $\vec{0} = (0, 0, \ldots, 0)$ is in $H$ since $0 + 0 + \ldots + 0 = 0$.
        \item Let $\vec{u}, \vec{v} \in H$. Then, we have:
        \[ u_1 + u_2 + \ldots + u_n = 0 \quad \text{and} \quad v_1 + v_2 + \ldots + v_n = 0 \]
        Adding these two equations, we get:
        \[(u_1 + v_1) + (u_2 + v_2) + \ldots + (u_n + v_n) = 0 + 0 = 0\]
        Thus, $\vec{u} + \vec{v} \in H$.
        \item Let $\vec{u} \in H$ and $c \in \mathbb{R}$. Then, we have:
        \[u_1 + u_2 + \ldots + u_n = 0\]
        Multiplying this equation by $c$, we get:
        \[c u_1 + c u_2 + \ldots + c u_n = c \cdot 0 = 0\]
        Thus, $c \vec{u} \in H$.
    \end{itemize}
    Since $H$ satisfies all three conditions, it is a subspace of $V$
\end{eg}
Let's see some examples of sets that are not subspaces:
\begin{eg}
    Let $V = \mathbb{R}^n$ and $H = \{\vec{x} \in \mathbb{R}^n : x_1 + \ldots + x_n = 1\}$. Let's show that $H$ is not a subspace of $V$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector $\vec{0} = (0, 0, \ldots, 0)$ is not in $H$ since $0 + 0 + \ldots + 0 \neq 1$.
    \end{itemize}
    Since $H$ does not satisfy the first condition, it is not a subspace of $V$.
\end{eg}
\begin{eg}
    Let $V = \mathbb{R}^n$ and $H = \{\vec{x} \in \mathbb{R}^n : x_1 \geq 0, \ldots, x_n \geq 0\}$. Let's show that $H$ is not a subspace of $V$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector $\vec{0} = (0, 0, \ldots, 0)$ is in $H$ since $0 \geq 0, \ldots, 0 \geq 0$.
        \item Let $\vec{u}, \vec{v} \in H$. Then, we have:
        \[ u_1 \geq 0, \ldots, u_n \geq 0 \quad \text{and} \quad v_1 \geq 0, \ldots, v_n \geq 0 \]
        Adding these two equations, we get:
        \[(u_1 + v_1) \geq 0, \ldots, (u_n + v_n) \geq 0\]
        Thus, $\vec{u} + \vec{v} \in H$.
        \item Let $\vec{u} \in H$ and $c \in \mathbb{R}$. If $c \geq 0$, then we have:
        \[u_1 \geq 0, \ldots, u_n \geq 0\]
        Multiplying this equation by $c$, we get:
        \[c u_1 \geq 0, \ldots, c u_n \geq 0\]
        Thus, $c \vec{u} \in H$. However, if $c < 0$, then we have:
        \[u_1 \geq 0, \ldots, u_n \geq 0\]
        Multiplying this equation by $c$, we get:
        \[c u_1 \leq 0, \ldots, c u_n \leq 0\]
        Thus, $c \vec{u} \notin H$.
    \end{itemize}
    Since $H$ does not satisfy the third condition, it is not a subspace of $V$.
\end{eg}
\begin{eg}
    Let $V = \mathbb{R}^{n \times n }$ and $H = \{A \in \mathbb{R}^{n \times n} : A = A^T\}$. Let's show that $H$ is a subspace of $V$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero matrix $0$ is in $H$ since $0 = 0^T$.
        \item Let $A, B \in H$. Then, we have:
        \[ A = A^T \quad \text{and} \quad B = B^T \]
        Adding these two equations, we get:
        \[ (A + B)^T = A^T + B^T = A + B \]
        Thus, $A + B \in H$.
        \item Let $A \in H$ and $c \in \mathbb{R}$. Then, we have:
        \[ A = A^T \]
        Multiplying this equation by $c$, we get:
        \[ (cA)^T = cA^T = cA \]
        Thus, $cA \in H$.
    \end{itemize}
    Since $H$ satisfies all three conditions, it is a subspace of $V$.
\end{eg}

\begin{eg}
    Let $V = \mathbb{P}_5$ and $H = \mathbb{P}_3$. Let's show that $H$ is a subspace of $V$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero polynomial $0$ is in $H$ since it is a polynomial of degree less than or equal to $3$.
        \item Let $p(x), q(x) \in H$. Then, we have:
        \[ \text{deg}(p(x)) \leq 3 \quad \text{and} \quad \text{deg}(q(x)) \leq 3 \]
        Adding these two polynomials, we get:
        \[ \text{deg}(p(x) + q(x)) \leq 3 \]
        Thus, $p(x) + q(x) \in H$.
        \item Let $p(x) \in H$ and $c \in \mathbb{R}$. Then, we have:
        \[ \text{deg}(p(x)) \leq 3 \]
        Multiplying this polynomial by $c$, we get:
        \[ \text{deg}(cp(x)) \leq 3 \]
        Thus, $cp(x) \in H$.
    \end{itemize}
    Since $H$ satisfies all three conditions, it is a subspace of $V$.
\end{eg}
Note that $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$ since it is not a subset of $\mathbb{R}^3$.
\begin{definition}[Null Subspace]
    The null subspace (or trivial subspace) of a vectorial space $V$ is the set $\{\vec{0}\}$ containing only the zero vector of $V$. It is a subspace of $V$ since it satisfies all three conditions of a subspace.
\end{definition}

\begin{eg}
    Let $V$ be a vectorial space. Let $\vec{v_1}, \vec{v_2}, \vec{v_3} \in V$ vectors of $V$ and the set $H = Span\{v_1,v_2,v_3\}$ of all linear combinations of $\vec{v_1}, \vec{v_2}, \vec{v_3}$. This set is a subset of $V$. Let's show that $H$ is a subspace of $V$:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector $\vec{0}$ is in $H$ since we can write it as a linear combination of $\vec{v_1}, \vec{v_2}, \vec{v_3}$ with all coefficients equal to $0$:
        \[ \vec{0} = 0\vec{v_1} + 0\vec{v_2} + 0\vec{v_3} \]
        \item Let $\vec{u}, \vec{w} \in H$. Then, we can write them as linear combinations of $\vec{v_1}, \vec{v_2}, \vec{v_3}$:
        \[ \vec{u} = a_1\vec{v_1} + a_2\vec{v_2} + a_3\vec{v_3} \quad \text{and} \quad \vec{w} = b_1\vec{v_1} + b_2\vec{v_2} + b_3\vec{v_3} \]
        where $a_1, a_2, a_3, b_1, b_2, b_3 \in \mathbb{R}$. Adding these two equations, we get:
        \[ \vec{u} + \vec{w} = (a_1 + b_1)\vec{v_1} + (a_2 + b_2)\vec{v_2} + (a_3 + b_3)\vec{v_3} \]
        Thus, $\vec{u} + \vec{w} \in H$.
        \item Let $\vec{u} \in H$ and $c \in \mathbb{R}$. Then, we can write $\vec{u}$ as a linear combination of $\vec{v_1}, \vec{v_2}, \vec{v_3}$:
        \[ \vec{u} = a_1\vec{v_1} + a_2\vec{v_2} + a_3\vec{v_3} \]
        where $a_1, a_2, a_3 \in \mathbb{R}$. Multiplying this equation by $c$, we get:
        \[ c\vec{u} = (ca_1)\vec{v_1} + (ca_2)\vec{v_2} + (ca_3)\vec{v_3} \]
        Thus, $c\vec{u} \in H$.
    \end{itemize}
    Since $H$ satisfies all three conditions, it is a subspace of $V$.
\end{eg}

\begin{theorem}
    Let $\vec{v_1}, v_2, \ldots, v_p$ be vectors in a vector space $V$. Then, the set $H = Span\{v_1, v_2, \ldots, v_p\}$ is a subspace of $V$.
\end{theorem}

\begin{eg}
    Let $V = \mathbb{R}^3$ and $\vec{v_1}, \vec{v_2}, \vec{v_3} \in V$. We have:
    \begin{itemize}
        \item The origin $\{(0,0,0)\}$ is a subspace of $V$.
        \item Any line through the origin ($Span(\vec{v_1}), \vec{v_1} \neq \vec{0}$) is a subspace of $V$.
        \item Any plane through the origin ($Span(\vec{v_1}, \vec{v_2})$ and $\vec{v_1}, \vec{v_2}$ linearly independent) is a subspace of $V$.
        \item The whole space $V$ is a subspace of $V$ ($Span(\vec{v_1}, \vec{v_2}, \vec{v_3})$ and $\vec{v_1}, \vec{v_2}, \vec{v_3}$ linearly independent).
    \end{itemize}
\end{eg}

\begin{theorem}
    The set of solutions $H$ of a homogeneous system of linear equations $A\vec{x} = \vec{0}$ (with $A \in \mathbb{R}^{m \times n}$) is a subspace of $\mathbb{R}^n$.
\end{theorem}
\begin{proof}
    Let's show that $H$ satisfies the three conditions of a subspace:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The zero vector $\vec{0}$ is in $H$ since $A\vec{0} = \vec{0}$.
        \item Let $\vec{u}, \vec{v} \in H$. Then, we have:
        \[ A\vec{u} = \vec{0} \quad \text{and} \quad A\vec{v} = \vec{0} \]
        Adding these two equations, we get:
        \[ A(\vec{u} + \vec{v}) = A\vec{u} + A\vec{v} = \vec{0} + \vec{0} = \vec{0} \]
        Thus, $\vec{u} + \vec{v} \in H$.
        \item Let $\vec{u} \in H$ and $c \in \mathbb{R}$. Then, we have:
        \[ A\vec{u} = \vec{0} \]
        Multiplying this equation by $c$, we get:
        \[ A(c\vec{u}) = cA\vec{u} = c\cdot\vec{0} = \vec{0} \]
        Thus, $c\vec{u} \in H$.
    \end{itemize}
    Since $H$ satisfies all three conditions, it is a subspace of $\mathbb{R}^n$.
\end{proof}

\section{Kernel and Images}
Let $A \in \mathbb{R}^{m \times n}$.
\begin{definition}[Kernel]
    The kernel (or null space) of $A$ is the set of all vectors $\vec{x} \in \mathbb{R}^n$ such that $A\vec{x} = \vec{0}$, i.e.,
    \[ \text{Ker}(A) = \{ \vec{x} \in \mathbb{R}^n \mid A\vec{x} = \vec{0} \} \]
    The kernel is a subspace of $\mathbb{R}^n$.
\end{definition}

\begin{definition}[Image]
    The image of $A$ is the set of all vectors $\vec{y} \in \mathbb{R}^m$ such that there exists a vector $\vec{x} \in \mathbb{R}^n$ with $A\vec{x} = \vec{y}$, i.e.,
    \[ \text{Im}(A) = \{ \vec{y} \in \mathbb{R}^m \mid \exists \vec{x} \in \mathbb{R}^n, A\vec{x} = \vec{y} \} \]
    The image is a subspace of $\mathbb{R}^m$.
\end{definition}
The Image of $T$ is also called the column space of $A$ since it is spanned by the columns of $A$.

\begin{eg}
    Let $A = \begin{bmatrix}
        -3 & 6 & -1 & 1 & -7 \\
        1 & -2 & 2 & 3 & -1 \\
        2 & -4 & 5 & 8 & -4
    \end{bmatrix}$. Let's find a basis for the kernel and the image of the linear transformation $T: \mathbb{R}^5 \to \mathbb{R}^3$ defined by $T(\vec{x}) = A\vec{x}$. We need to solve the homogeneous system $A\vec{x} = \vec{0}$:
    \[ \begin{bmatrix}
        -3 & 6 & -1 & 1 & -7 \\
        1 & -2 & 2 & 3 & -1 \\
        2 & -4 & 5 & 8 & -4
    \end{bmatrix} \sim \begin{bmatrix}
        1 & -2 & 0 & -1 & 3 \\
        0 & 0 & 1 & 2 & -2 \\
        0 & 0 & 0 & 0 & 0
    \end{bmatrix} \]
    From the reduced row echelon form, we can write the system of equations:
    \[
        \begin{cases}
            x_1 - 2x_2 - x_4 + 3x_5 = 0 \\
            x_3 + 2x_4 - 2x_5 = 0
        \end{cases} \iff \begin{cases}
            x_1 = 2x_2 + x_4 - 3x_5 \\
            x_3 = -2x_4 + 2x_5
        \end{cases}
    \]
    We can express the solution in terms of the free variables $x_2, x_4, x_5$:
    \[ \vec{x} = \begin{bmatrix}
        x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
    \end{bmatrix} = x_2 \begin{bmatrix}
        2 \\ 1 \\ 0 \\ 0 \\ 0
    \end{bmatrix} + x_4 \begin{bmatrix}
        1 \\ 0 \\ -2 \\ 1 \\ 0
    \end{bmatrix} + x_5 \begin{bmatrix}
        -3 \\ 0 \\ 2 \\ 0 \\ 1
    \end{bmatrix} \]
    Thus, a basis for the kernel of $T$ is given by:
    \[ \left\{ \begin{bmatrix}
        2 \\ 1 \\ 0 \\ 0 \\ 0
    \end{bmatrix}, \begin{bmatrix}
        1 \\ 0 \\ -2 \\ 1 \\ 0
    \end{bmatrix}, \begin{bmatrix}
        -3 \\ 0 \\ 2 \\ 0 \\ 1
    \end{bmatrix} \right\} \]
    To find a basis for the image of $T$, we can look at the pivot columns of the original matrix $A$. The pivot columns are the first and third columns. Thus, a basis for the image of $T$ is given by:
    \[ \left\{ \begin{bmatrix}
        -3 \\ 1 \\ 2
    \end{bmatrix}, \begin{bmatrix}
        -1 \\ 2 \\ 5
    \end{bmatrix} \right\} \]
\end{eg}

\subsection{Differences between the Kernel and the Image}
We can denote these differences as follows:
\vskip0.3cm
\begin{center}
    \begin{tabular}{p{0.45\textwidth} | p{0.45\textwidth}}
        \\ {\centering \textbf{Kernel} \par} & {\centering \textbf{Image} \par} \\ \\ \hline \\
        The kernel is a subspace of $\mathbb{R}^n$ & The image is a subspace of $\mathbb{R}^m$ \\ \\
        $\text{Ker} A$ is defined implicitly by the equation $A\vec{x} = \vec{0}$ & $\text{Im} A$ is defined explicitly as the span of the columns of $A$ \\ \\
        Finding the $\text{Ker} A$ takes time by solving a homogeneous system of linear equations & Finding the $\text{Im} A$ is easy. The columns of $A$ form a basis for the image. \\ \\
        No simple relation between $\text{Ker} A$ and the coefficients of $A$ & Simple relation between $\text{Im} A$ and the coefficients of $A$, since each column of $A$ is contained $\text{Im} A$ \\ \\
        A vector $\vec{v}$ in $\text{Ker} A$ is characterized by $A\vec{v} = \vec{0}$ & A vector $\vec{v}$ in $\text{Im} A$ is characterized by the existence of a vector $\vec{u}$ such that $A\vec{u} = \vec{v}$ \\ \\
        Given a vector $\vec{v}$, it is easy to check if $\vec{v} \in \text{Ker} A$ by computing $A\vec{v}$ & Given a vector $\vec{v}$, the system $\begin{bmatrix}
            A & \vec{v}
        \end{bmatrix}$ must be solved to check if $\vec{v} \in \text{Im} A$ \\ \\
        $\text{Ker} A = \{\vec{0}\}$ if and only if the equation $A\vec{x} = \vec{0}$ has only the trivial solution & $\text{Im} A = \mathbb{R}^m$ if and only if the equation $A\vec{x} = \vec{b}$ has a solution for every $\vec{b} \in \mathbb{R}^m$ \\ \\
        $\text{Ker} A = \{\vec{0}\}$ if and only if the transformation $T(\vec{x}) = A\vec{x}$ is one-to-one (injective) & $\text{Im} A = \mathbb{R}^m$ if and only if the transformation $T(\vec{x}) = A\vec{x}$ is onto (surjective) \\ \\
    \end{tabular}
\end{center}

\subsection{Kernel and Images of Linear Transformations}
\begin{definition}[Linear Transformation]
    A linear transformation is a function $T: V \to W$ that satisfies the following properties for all vectors $\vec{u}, \vec{v} \in V$ and any scalar $c \in \mathbb{R}$:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v})$ (Additivity)
        \item $T(c\vec{u}) = cT(\vec{u})$ (Homogeneity)
    \end{itemize}
\end{definition}
Again we can define the kernel and the image of a linear transformation.
\begin{definition}[Kernel of a Linear Transformation]
    The kernel (or null space) of a linear transformation $T: V \to W$ is the set of all vectors $\vec{v} \in V$ such that $T(\vec{v}) = \vec{0}$, i.e.,
    \[ \text{Ker}(T) = \{ \vec{v} \in V \mid T(\vec{v}) = \vec{0} \} \]
    The kernel is a subspace of $V$.
\end{definition}
\begin{definition}[Image of a Linear Transformation]
    The image (or range) of a linear transformation $T: V \to W$ is the set of all vectors $\vec{w} \in W$ such that there exists a vector $\vec{v} \in V$ with $T(\vec{v}) = \vec{w}$, i.e.,
    \[ \text{Im}(T) = \{ \vec{w} \in W \mid \exists \vec{v} \in V, T(\vec{v}) = \vec{w} \} \]
    The image is a subspace of $W$.
\end{definition}

\begin{eg}
    Let $V = \mathbb{P}_3$ and $W = \mathbb{P}_2$. Consider the linear transformation $T: V \to W$ defined by $T(p(x)) = p'(x)$, where $p'(x)$ is the derivative of the polynomial $p(x)$:
    \[ p(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 \quad \text{and} \quad p \in \mathbb{P}_3 \]
    where $a_0, a_1, a_2, a_3 \in \mathbb{R}$. The derivative of $p(x)$ is given by:
    \[ p'(x) = a_1 + 2a_2 x + 3a_3 x^2 \quad \text{and} \quad p' \in \mathbb{P}_2 \]
    Let's check that $T$ is a linear transformation:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Let $p(x), q(x) \in V$. Then, we have:
        \[ T(p(x) + q(x)) = (p(x) + q(x))' = p'(x) + q'(x) = T(p(x)) + T(q(x)) \]
        Thus, $T$ satisfies the additivity property.
        \item Let $p(x) \in V$ and $c \in \mathbb{R}$. Then, we have:
        \[ T(cp(x)) = (cp(x))' = c p'(x) = c T(p(x)) \]
        Thus, $T$ satisfies the homogeneity property.
    \end{itemize}
    Since $T$ satisfies both properties, it is a linear transformation. Now, let's find the kernel of $T$:
    \[ \text{Ker}(T) = \{ p(x) \in V \mid T(p(x)) = p'(x) = 0 \} \]
    The only polynomial in $\mathbb{P}_3$ whose derivative is $0$ is the constant polynomial. Thus, we have:
    \[ \text{Ker}(T) = \{ p(x) = a_0 \mid a_0 \in \mathbb{R} \} \]
    Now, let's find the image of $T$:
    \[ \text{Im}(T) = \{ p'(x) \in W \mid p(x) \in V \} \]
    The derivative of any polynomial in $\mathbb{P}_3$ is a polynomial in $\mathbb{P}_2$. Thus, we have:
    \[ \text{Im}(T) = W = \mathbb{P}_2 \]
    Thus $T$ is onto (surjective) but not one-to-one (injective) since $\text{Ker}(T) \neq \{\vec{0}\}$.
\end{eg}

\section{Independent Sets and Bases}
\subsection{Independent Sets}
\begin{definition}[Independent Set]
    A set of vectors $\{\vec{v_1}, \vec{v_2}, \ldots, \vec{v_p}\}$ in a vector space $V$ is said to be linearly independent if the only solution to the equation:
    \[ c_1 \vec{v_1} + c_2 \vec{v_2} + \ldots + c_p \vec{v_p} = \vec{0} \]
    is the trivial solution $c_1 = c_2 = \ldots = c_p = 0$. If there exists a non-trivial solution (i.e., at least one $c_i \neq 0$), then the set is said to be linearly dependent.
\end{definition}
\begin{theorem}
    A set of vectors $\{\vec{v_1}, \vec{v_2}, \ldots, \vec{v_p}\}$ of at least two vectors (where $\vec{v_1} \neq \vec{0}$) is linearly dependent if and only if at least one of the vectors can be expressed as a linear combination of the others.
\end{theorem}

\begin{eg}
    Let $V = \mathbb{P}_2$. Let's consider the set of polynomials $\{p_1(x), p_2(x), p_3(x)\}$ where:
    \[ p_1(x) = 1, \quad p_2(x) = x, \quad p_3(x) = 4 - x \]
    We want to determine if this set is linearly independent or dependent. We need to solve the equation:
    \[
        c_1 p_1(x) + c_2 p_2(x) + c_3 p_3(x) = 0 \implies c_1 (1) + c_2 (x) + c_3 (4 - x) = 0
    \]
    This simplifies to:
    \[ (c_1 + 4c_3) + (c_2 - c_3)x = 0 \]
    For this equation to hold for all $x$, the coefficients of both the constant term and the coefficient of $x$ must be zero. This gives us the system:
    \[ \begin{cases}
        c_1 + 4c_3 = 0 \\
        c_2 - c_3 = 0
    \end{cases} \]
    Solving this system, we find:
    \[ c_1 = -4c_3, \quad c_2 = c_3 \]
    This means that there are infinitely many solutions depending on the value of $c_3$. For example, if we choose $c_3 = 1$, we get:
    \[ c_1 = -4, \quad c_2 = 1, \quad c_3 = 1 \]
    Thus, we have a non-trivial solution:
    \[ -4p_1(x) + 1p_2(x) + 1p_3(x) = 0 \]
    Therefore, the set $\{p_1(x), p_2(x), p_3(x)\}$ is linearly dependent.
\end{eg}

\subsection{Bases}
\begin{definition}[Basis]
    A basis for a vector space $V$ is a set of vectors $\{\vec{b_1}, \vec{b_2}, \ldots, \vec{b_n}\}$ in $V$ that is both linearly independent and spans the entire space $V$. This means that any vector $\vec{v} \in V$ can be expressed as a linear combination of the basis vectors:
    \[ \vec{v} = c_1 \vec{b_1} + c_2 \vec{b_2} + \ldots + c_n \vec{b_n} \]
    where $c_1, c_2, \ldots, c_n \in \mathbb{R}$.
\end{definition}
Remark that:
\begin{itemize}
    \item If $\vec{b_1}, \vec{b_2}, \ldots, \vec{b_p} \in V$ are linearly independent then they form a basis for $Span\{\vec{b_1}, \vec{b_2}, \ldots, \vec{b_p}\}$.
    \item If $\vec{b_1}, \vec{b_2}, \ldots, \vec{b_p} \in V$ form a basis for $H \subseteq V$, then $\vec{b_1}, \vec{b_2}, \ldots, \vec{b_p}$ are in $H$ because $H = Span\{\vec{b_1}, \vec{b_2}, \ldots, \vec{b_p}\}$.
\end{itemize}

\begin{eg}
    Let $V = \mathbb{R}^2$ and $\vec{b_1} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \vec{b_2} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$. We claim that $(\vec{b_1}, \vec{b_2})$ is a basis for $V$.
\end{eg}

\begin{eg}
    Let $V = \mathbb{R}^3$ and $\vec{b_1} = \begin{pmatrix}
        1 \\ 1 \\ 0
    \end{pmatrix}, \vec{b_2} = \begin{pmatrix}
        1 \\ -1 \\ 0
    \end{pmatrix}$. We claim that $(\vec{b_1}, \vec{b_2})$ is a basis for the subspace $H = \{\vec{x} \in \mathbb{R}^3 : x_3 = 0\} = Span\{\vec{b_1}, \vec{b_2}\}$ which is a plane in $\mathbb{R}^3$.
\end{eg}

\begin{eg}
    Let $V = \mathbb{R}^2$ and $\vec{b_1} = \begin{pmatrix}
        1 \\ 0
    \end{pmatrix}, \vec{b_2} = \begin{pmatrix}
        0 \\ 1
    \end{pmatrix}, \vec{b_3} = \begin{pmatrix}
        0 \\ -1
    \end{pmatrix}$. We claim that $(\vec{b_1}, \vec{b_2}, \vec{b_3})$ is not a basis for $V$ since the set is linearly dependent (because $\vec{b_3} = -\vec{b_2}$).
\end{eg}

\begin{eg}
    Let $V = \mathbb{P}_2$ and $\vec{b_1}(t) = 1$, $\vec{b_2}(t) = t$, $\vec{b_3}(t) = t^2$. We claim that $(\vec{b_1}, \vec{b_2}, \vec{b_3})$ is a basis for $V$.
\end{eg}

\begin{eg}
    Let $V = \mathbb{P}_3$ and $\vec{b_1}(t) = 1$, $\vec{b_2}(t) = t$, $\vec{b_3}(t) = t^2$. We claim that $(\vec{b_1}, \vec{b_2}, \vec{b_3})$ is not a basis for $V$ since it does not span $V$ (for example, the polynomial $t^3$ cannot be expressed as a linear combination of $\vec{b_1}, \vec{b_2}, \vec{b_3}$).
\end{eg}

\begin{eg}
    Let $A = \begin{bmatrix}
        | & & | \\
        \vec{a_1} & \ldots & \vec{a_n} \\
        | & & |
    \end{bmatrix} \in \mathbb{R}^{n \times n}$ be an invertible matrix. We claim that $(\vec{a_1}, \ldots, \vec{a_n})$ is a basis for $\mathbb{R}^n$ since the columns of $A$ are linearly independent and span $\mathbb{R}^n$.
\end{eg}