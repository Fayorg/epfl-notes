\chapter{Determinants}
For a square matrix \(A = \begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix}\), then $A$ is invertible if and only if \(ad - bc \neq 0\). This quantity is called the determinant of $A$. Let's generalize this to larger square matrices.
\begin{definition}[Determinant]
    Let $A = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \cdots & a_{nn}
    \end{bmatrix}$ be an \(n \times n\) matrix. The determinant of $A$ is defined as:
    \[
        \det(A) = \sum_{j = 1}^{n} a_{ij} (-1)^{i+j} \det(A_{ij})
    \]
    where \(A_{ij}\) is the \((n-1) \times (n-1)\) matrix obtained by removing the \(i\)-th row and \(j\)-th column from $A$. Note that the $i$-th row can be chosen arbitrarily, and the formula still holds. \\ 
    This definition is recursive, with the base case being that the determinant of a \(1 \times 1\) matrix \([a]\) is simply \(a\).
\end{definition}
This is an expansion along the \(i\)-th row. We can also define an expansion along the \(j\)-th column:
\[
    \det(A) = \sum_{i = 1}^{n} a_{ij} (-1)^{i+j} \det(A_{ij})
\]
The determinant can be computed using either row or column expansion, and the result will be the same. However, the choice of row or column can affect the computational efficiency, especially for larger matrices.

% Moved to line 430
% \begin{theorem}
%     For a matrix $A \in \mathbb{R}^{n \times n}$, we have:
%     \[
%         \det(A) = \det(A^T)
%     \]
% \end{theorem}

\begin{definition}[Minor and Cofactor]
    The minor \(M_{ij}\) of an element \(a_{ij}\) in a matrix $A$ is the determinant of the submatrix \(A_{ij}\) obtained by removing the \(i\)-th row and \(j\)-th column from $A$. The cofactor \(C_{ij}\) is defined as:
    \[
        C_{ij} = (-1)^{i+j} M_{ij}
    \]
\end{definition}

\begin{eg}
    Let's compute the determinant of a \(2 \times 2\) matrix using the definition:
    \[
        A = \begin{bmatrix}
            a & b \\
            c & d
        \end{bmatrix}
    \]
    Then, we have:
    \[
        \det(A) = \sum_{j = 1}^{2} a_{ij} (-1)^{i+j} \det(A_{ij})
    \]
    For \(i = 1\), we get:
    \[
        \det(A) = a_{11} (-1)^{1+1} \det(A_{11}) + a_{12} (-1)^{1+2} \det(A_{12})
    \]
    where \(A_{11} = [d]\) and \(A_{12} = [c]\). Thus:
    \[
        \det(A) = a_{11} \cdot d - a_{12} \cdot c = ad - bc
    \]
    Let prove that $i$ can be chosen arbitrarily. For \(i = 2\), we have:
    \[
        \det(A) = a_{21} (-1)^{2+1} \det(A_{21}) + a_{22} (-1)^{2+2} \det(A_{22})
    \]
    where \(A_{21} = [b]\) and \(A_{22} = [a]\). Thus:
    \[
        \det(A) = -a_{21} \cdot b + a_{22} \cdot a = -cb + ad = ad - bc
    \]
    Therefore, the determinant is the same regardless of the row chosen for expansion.
\end{eg}

\begin{eg}
    Let's compute the determinant of a \(3 \times 3\) matrix:
    \[
        B = \begin{bmatrix}
            a_{11} & a_{12} & a_{13} \\
            a_{21} & a_{22} & a_{23} \\
            a_{31} & a_{32} & a_{33}
        \end{bmatrix}
    \]
    We can expand along the first row (\(i = 1\)):
    \[
        \det(B) = a_{11} (-1)^{1+1} \det(B_{11}) + a_{12} (-1)^{1+2} \det(B_{12}) + a_{13} (-1)^{1+3} \det(B_{13})
    \]
    where:
    \[
        B_{11} = \begin{bmatrix}
            a_{22} & a_{23} \\
            a_{32} & a_{33}
        \end{bmatrix}, \quad
        B_{12} = \begin{bmatrix}
            a_{21} & a_{23} \\
            a_{31} & a_{33}
        \end{bmatrix}, \quad
        B_{13} = \begin{bmatrix}
            a_{21} & a_{22} \\
            a_{31} & a_{32}
        \end{bmatrix}
    \]
    Now, we compute the determinants of these \(2 \times 2\) matrices:
    \[
        \det(B_{11}) = a_{22}a_{33} - a_{23}a_{32}, \quad
        \det(B_{12}) = a_{21}a_{33} - a_{23}a_{31}, \quad
        \det(B_{13}) = a_{21}a_{32} - a_{22}a_{31}
    \]
    Substituting these back into the determinant formula, we get:
    \[
        \det(B) = a_{11}(a_{22}a_{33} - a_{23}a_{32}) - a_{12}(a_{21}a_{33} - a_{23}a_{31}) + a_{13}(a_{21}a_{32} - a_{22}a_{31})
    \]
    Simplifying this expression, we have:
    \[
        \det(B) = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32}
    \]
    This is the determinant of the \(3 \times 3\) matrix \(B\).
\end{eg}
\begin{eg}
    Let's compute the determinant of a $2 \times 2$ matrix using the column expansion:
    \[
        C = \begin{bmatrix}
            a & b \\
            c & d
        \end{bmatrix}
    \]
    Then, we have:
    \[
        \det(C) = \sum_{i = 1}^{2} a_{ij} (-1)^{i+j} \det(C_{ij})
    \]
    For \(j = 1\), we get:
    \[
        \det(C) = a_{11} (-1)^{1+1} \det(C_{11}) + a_{21} (-1)^{2+1} \det(C_{21})
    \]
    where \(C_{11} = [d]\) and \(C_{21} = [b]\). Thus:
    \[
        \det(C) = a_{11} \cdot d - a_{21} \cdot b = ad - bc
    \]
    Let prove that $j$ can be chosen arbitrarily. For \(j = 2\), we have:
    \[
        \det(C) = a_{12} (-1)^{1+2} \det(C_{12}) + a_{22} (-1)^{2+2} \det(C_{22})
    \]
    where \(C_{12} = [c]\) and \(C_{22} = [a]\). Thus:
    \[
        \det(C) = -a_{12} \cdot c + a_{22} \cdot a = -bc + ad = ad - bc
    \]
    Therefore, the determinant is the same regardless of the column chosen for expansion (or using row expansion).
\end{eg}

\begin{eg}
    Let's compute the determinant of a $4 \times 4$ matrix:
    \[
        D = \begin{bmatrix}
            0 & 1 & 3 & 0 \\
            2 & 0 & 1 & 2 \\
            0 & 1 & 4 & 2 \\
            0 & 1 & 1 & 1
        \end{bmatrix}
    \]
    If we don't choose wisely, this can get very tedious. Let's expand along the first column (\(j = 1\)):
    \begin{align*}
        \det(D) &= 0 \cdot (-1)^{2} \cdot \det(D_{11}) + 2 \cdot (-1)^{3} \cdot \det(D_{21}) \\
        &\quad + 0 \cdot (-1)^{4} \cdot \det(D_{31}) + 0 \cdot (-1)^{5} \cdot \det(D_{41}) \\
        &= 2 \cdot (-1)^{3} \cdot \det(D_{21})
    \end{align*}
    where:
    \[
        D_{21} = \begin{bmatrix}
            1 & 3 & 0 \\
            1 & 4 & 2 \\
            1 & 1 & 1
        \end{bmatrix}
    \]
    Now, we compute the determinant of this \(3 \times 3\) matrix:
    \[
        \det(D_{21}) = 1(4 \cdot 1 - 2 \cdot 1) - 3(1 \cdot 1 - 2 \cdot 1) + 0(1 \cdot 1 - 4 \cdot 1)
    \]
    Simplifying this expression, we have:
    \[
        \det(D_{21}) = 1(4 - 2) - 3(1 - 2) + 0 = 2 + 3 = 5
    \]
    Substituting this back into the determinant formula, we get:
    \[
        \det(D) = 2 \cdot (-1)^{3} \cdot 5 = -10
    \]
    Therefore, the determinant of the \(4 \times 4\) matrix \(D\) is \(-10\).
\end{eg}
Remark that in the previous example, we chose to expand along the first column because it contained the most zeros, which simplified our calculations. Choosing a row or column with more zeros can significantly reduce the number of terms we need to compute, making the process more efficient.

\subsection{Determinant of Triangular and Diagonal Matrices}
\begin{theorem}
    If $A$ is a triangular matrix (upper or lower) or a diagonal matrix, then the determinant of $A$ is the product of its diagonal entries. In other words, if:
    \[
        A = \begin{bmatrix}
            a_{11} & a_{12} & \cdots & a_{1n} \\
            0 & a_{22} & \cdots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & a_{nn}
        \end{bmatrix}
    \]
    or
    \[
        A = \begin{bmatrix}
            a_{11} & 0 & \cdots & 0 \\
            a_{21} & a_{22} & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{n1} & a_{n2} & \cdots & a_{nn}
        \end{bmatrix}
    \]
    then:
    \[
        \det(A) = a_{11} \cdot a_{22} \cdots a_{nn}
    \]
\end{theorem}
\begin{proof}
    We will prove this for an upper triangular matrix; the proof for a lower triangular matrix is analogous. We proceed by induction on \(n\), the size of the matrix. \\
    \textbf{Base Case:} For \(n = 1\), the matrix is simply \([a_{11}]\), and its determinant is \(a_{11}\), which is the product of its diagonal entries. \\
    \textbf{Inductive Step:} Assume the statement holds for all upper triangular matrices of size \(k \times k\). Now consider an upper triangular matrix \(A\) of size \((k+1) \times (k+1)\):
    \[
        A = \begin{bmatrix}
            a_{11} & a_{12} & \cdots & a_{1,k+1} \\
            0 & a_{22} & \cdots & a_{2,k+1} \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & a_{k+1,k+1}
        \end{bmatrix}
    \]
    We can expand the determinant along the first row:
    \[
        \det(A) = a_{11} (-1)^{1+1} \det(A_{11}) + \sum_{j=2}^{k+1} a_{1j} (-1)^{1+j} \det(A_{1j})
    \]
    where \(A_{11}\) is the \(k \times k\) upper triangular matrix obtained by removing the first row and first column, and each \(A_{1j}\) (for \(j > 1\)) has at least one row of zeros (the first column of these submatrices will be all zeros since they come from the first row of \(A\)). Therefore, their determinants are zero:
    \[
        \det(A) = a_{11} \det(A_{11}) + 0 = a_{11} \det(A_{11})
    \]
    By the inductive hypothesis, we know that:
    \[
        \det(A_{11}) = a_{22} a_{33} \cdots a_{k+1,k+1}
    \]
    Thus:
    \[
        \det(A) = a_{11} \cdot a_{22} a_{33} \cdots a_{k+1,k+1}
    \]
    This completes the inductive step, and hence the proof.
\end{proof}

\subsection{Elementary Operations and Determinants}
\begin{theorem}
    Let $A$ be an \(n \times n\) matrix. The following properties hold for the determinant under elementary row operations:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item If a row of $A$ is multiplied by a scalar \(k\), then the determinant of the new matrix is \(k\) times the determinant of $A$. In other words, if \(B\) is obtained from \(A\) by multiplying a row by \(k\), then:
        \[
            \det(B) = k \cdot \det(A)
        \]
        \item If two rows of $A$ are swapped, then the determinant of the new matrix is the negative of the determinant of $A$. In other words, if \(C\) is obtained from \(A\) by swapping two rows, then:
        \[
            \det(C) = -\det(A)
        \]
        \item If a multiple of one row is added to another row, then the determinant of the new matrix is the same as the determinant of $A$. In other words, if \(D\) is obtained from \(A\) by adding \(k\) times one row to another row, then:
        \[
            \det(D) = \det(A)
        \]
    \end{itemize}
\end{theorem}
\begin{proof}
    Let's prove each property by considering the effect on $2 \times 2$ matrices: \\
    \textbf{Property 1:} Consider the matrix \(A = \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}\). If we multiply the first row by a scalar \(k\), we get the new matrix \(B = \begin{bmatrix}
        ka & kb \\
        c & d
    \end{bmatrix}\). The determinant of \(B\) is:
    \[
        \det(B) = (ka)d - (kb)c = k(ad - bc) = k \cdot \det(A)
    \]
    \textbf{Property 2:} Now, consider swapping the two rows of \(A\) to get the new matrix \(C = \begin{bmatrix}
        c & d \\
        a & b
    \end{bmatrix}\). The determinant of \(C\) is:
    \[
        \det(C) = cb - da = -(ad - bc) = -\det(A)
    \]
    \textbf{Property 3:} Finally, consider adding \(k\) times the first row to the second row of \(A\) to get the new matrix \(D = \begin{bmatrix}
        a & b \\
        c + ka & d + kb
    \end{bmatrix}\). The determinant of \(D\) is:
    \[
        \det(D) = a(d + kb) - b(c + ka) = ad + akb - bc - bak = ad - bc = \det(A)
    \]
    Therefore, all three properties hold for \(2 \times 2\) matrices. These properties can be extended to \(n \times n\) matrices using similar reasoning and the multilinearity of the determinant function.
\end{proof}
This theorem can also be expressed as for a matrix $A$ and an elementary matrix $E$:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item If \(E\) is obtained by multiplying a row by \(k\), then \(\det(EA) = \det(E) \cdot \det(A) = k \cdot \det(A)\).
    \item If \(E\) is obtained by swapping two rows, then \(\det(EA) = \det(E) \cdot \det(A) = -\det(A)\).
    \item If \(E\) is obtained by adding a multiple of one row to another, then \(\det(EA) = \det(E) \cdot \det(A) = \det(A)\).
\end{itemize}

\begin{eg}
    Let's compute the determinant of a \(3 \times 3\) matrix using row operations:
    \[
        A = \begin{bmatrix}
            1 & -4 & 2 \\
            -2 & 8 & -9 \\
            -1 & 7 & 0
        \end{bmatrix}
    \]
    We can perform row operations to convert \(A\) into an upper triangular matrix. First, we can subtract \(4\) times the first row from the second row:
    \[
        R_2 \leftarrow R_2 + 2R_1 \implies \begin{bmatrix}
            1 & -4 & 2 \\
            0 & 0 & -5 \\
            -1 & 7 & 0
        \end{bmatrix}
    \]
    Next, we can add the first row to the third row:
    \[
        R_3 \leftarrow R_3 + R_1 \implies \begin{bmatrix}
            1 & -4 & 2 \\
            0 & 0 & -5 \\
            0 & 3 & 2
        \end{bmatrix}
    \]
    Now, we can swap the second and third rows to get an upper triangular matrix:
    \[
        R_2 \leftrightarrow R_3 \implies \begin{bmatrix}
            1 & -4 & 2 \\
            0 & 3 & 2 \\
            0 & 0 & -5
        \end{bmatrix}
    \]
    The determinant of the resulting upper triangular matrix is the product of its diagonal entries:
    \[
        \det(A) = 1 \cdot 3 \cdot (-5) = -15
    \]
    However, we performed one row swap, which changes the sign of the determinant. Therefore, the determinant of the original matrix \(A\) is:
    \[
        \det(A) = -(-15) = 15
    \]
    Note that the rest of the operations (adding multiples of rows) did not change the determinant. We could express all theses opertaions by matrices:
    \[
        E_1 = \begin{bmatrix}
            1 & 0 & 0 \\
            2 & 1 & 0 \\
            0 & 0 & 1
        \end{bmatrix}, \quad
        E_2 = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            1 & 0 & 1
        \end{bmatrix}, \quad
        E_3 = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 0 & 1 \\
            0 & 1 & 0
        \end{bmatrix}
    \]
    Then:
    \[
        \det(A) = \det(E_3 E_2 E_1 A) = \det(E_3) \det(E_2) \det(E_1) \det(A) = -1 \cdot 1 \cdot 1 \cdot \det(A) = -\det(A)
    \]
    Thus, \(\det(A) = 15\).
\end{eg}

\subsection{Determinant and Invertibility}
\begin{theorem}
    A square matrix \(A\) is invertible if and only if its determinant is non-zero. In other words:
    \[
        A \text{ is invertible } \iff \det(A) \neq 0
    \]
\end{theorem}
\begin{proof}
    Let $M \in \mathbb{R}^{n \times n}$ the reduce row echelon form of $A$. There exists a sequence of elementary matrices \(E_1, E_2, \ldots, E_k\) such that:
    \[
        M = E_k E_{k-1} \cdots E_1 A
    \]
    Taking the determinant on both sides, we have:
    \[
        \det(M) = \det(E_k) \det(E_{k-1}) \cdots \det(E_1) \det(A)
    \]
    Since the elementary matrices have non-zero determinants, we have:
    \[
        \det(A) \neq 0 \iff \det(M) \neq 0
    \]
    If $A$ is invertible, then $M = I_n$, and thus \(\det(M) = 1 \neq 0\). Conversely, if \(A\) is not invertible, then \(M\) has at least one row of zeros, and thus \(\det(M) = 0\). Therefore, \(A\) is invertible if and only if \(\det(A) \neq 0\).
\end{proof}

\subsection{Operations on Determinants}
\begin{theorem}
    For square matrices \(A\) and \(B\) of the same size, the following properties hold:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The determinant of the product of two matrices is the product of their determinants:
        \[
            \det(AB) = \det(A) \cdot \det(B)
        \]
        \item The determinant of the inverse of a matrix is the reciprocal of the determinant of the matrix:
        \[
            \det(A^{-1}) = \frac{1}{\det(A)} \quad \text{if } A \text{ is invertible}
        \]
    \end{itemize}
\end{theorem}
\begin{proof}
    Let's prove the first property. We have two cases, depending on whether \(A\) is invertible or not. \\
    \textbf{Case 1:} If \(A\) is not invertible, then \(\det(A) = 0\). Let's now show that $AB$ is also not invertible. Suppose, for the sake of contradiction, that \(AB\) is invertible. Then there exists a matrix \(C\) such that:
    \[
        (AB)C = I_n \implies A(BC) = I_n
    \]
    This implies that \(A\) is invertible, which contradicts our assumption. Therefore, \(AB\) is not invertible, and thus \(\det(AB) = 0 = \det(A) \cdot \det(B)\). \\
    \textbf{Case 2:} If \(A\) is invertible, then we can express \(A\) as a product of elementary matrices:
    \[
        E_k E_{k-1} \cdots E_1 A = I_n
    \]
    where each \(E_i\) is an elementary matrix. We also have:
    \[
        A^{-1} = E_k E_{k-1} \cdots E_1 \quad \Leftrightarrow \quad (A^{-1})^{-1} = (E_k E_{k-1} \cdots E_1)^{-1} = E_1^{-1} E_2^{-1} \cdots E_k^{-1}
    \]
    Thus:
    \[
        \det(AB) = \det(E_1^{-1} E_2^{-1} \cdots E_k^{-1} B) = \det(E_1^{-1}) \det(E_2^{-1}) \cdots \det(E_k^{-1}) \det(B)
    \]
    By the properties of determinants of elementary matrices, we have:
    \[
        \det(E_1^{-1}) \det(E_2^{-1}) \cdots \det(E_k^{-1}) = \det(E_1)^{-1} \det(E_2)^{-1} \cdots \det(E_{k-1}^{-1} \cdot E_k^{-1}) = \dots = \det(A)
    \]
    Therefore:
    \[
        \det(AB) = \det(A) \cdot \det(B)
    \]
    This completes the proof of the first property. The second can be derived from the first:
    \[
        \det(AA^{-1}) = \det(I_n) = 1 \implies \det(A) \cdot \det(A^{-1}) = 1 \implies \det(A^{-1}) = \frac{1}{\det(A)}
    \]
\end{proof}

\begin{theorem}
    For a matrix $A \in \mathbb{R}^{n \times n}$, we have:
    \[
        \det(A) = \det(A^T)
    \]
\end{theorem}

\section{Interpretation of the Determinant}
The determinant of a matrix can be interpreted geometrically as a scaling factor for volume (or area in 2D) when the matrix is viewed as a linear transformation.

\begin{theorem}
    If $A$ is a \(2 \times 2\) matrix, then the absolute value of the determinant of $A$ represents the area scaling factor of the linear transformation defined by $A$. If $A$ is a \(3 \times 3\) matrix, then the absolute value of the determinant represents the volume scaling factor.
\end{theorem}
\begin{eg}
    Let $A$ be a \(2 \times 2\) matrix:
    \[
        A = \begin{bmatrix}
            a_{11} & a_{12} \\
            a_{21} & a_{22}
        \end{bmatrix} = \begin{bmatrix}
            \vec{u} & \vec{v}
        \end{bmatrix}
    \]
    Then the absolute value of the determinant of $A$ is the area of the parallelogram formed by the vectors \(\vec{u}\) and \(\vec{v}\).
    \begin{center}
        \begin{tikzpicture}[scale=2.5]
            % Axes
            \draw[->] (-0.8,0) -- (1.2,0) node[right] {$x_1$};
            \draw[->] (0,-0.2) -- (0,1.2) node[above] {$x_2$};

            % Original vectors
            \draw[thick,->,primary] (0,0) -- (1,0) node[midway, below] {$\vec{u}$};
            \draw[thick,->,primary] (0,0) -- (-0.6,1) node[midway, left] {$\vec{v}$};

            % Shaded area
            \fill[secondary!40,opacity=0.3] (0,0) -- (1,0) -- (0.4,1) -- (-0.6,1) -- cycle;
            \draw[dashed] (1,0) -- (0.4,1) -- (-0.6,1);
        \end{tikzpicture}
    \end{center}
    $Aire = |\det(A)| = |a_{11}a_{22} - a_{12}a_{21}|$
\end{eg}

\begin{eg}
    Similarly, for a \(3 \times 3\) matrix:
    \[
        B = \begin{bmatrix}
            a_{11} & a_{12} & a_{13} \\
            a_{21} & a_{22} & a_{23} \\
            a_{31} & a_{32} & a_{33}
        \end{bmatrix} = \begin{bmatrix}
            \vec{u} & \vec{v} & \vec{w}
        \end{bmatrix}
    \]
    The absolute value of the determinant of $B$ represents the volume of the parallelepiped formed by the vectors \(\vec{u}\), \(\vec{v}\), and \(\vec{w}\).
    \begin{center}
        \begin{tikzpicture}[scale=2.5]
            % Axes
            \draw[->] (-0.8,0,0) -- (1.2,0,0) node[right] {$x_1$};
            \draw[->] (0,-0.2,0) -- (0,1.2,0) node[above] {$x_2$};
            \draw[->] (0,0,0.2) -- (0,0,-1.2) node[below right] {$x_3$};

            % Original vectors
            \draw[thick,->,primary] (0,0,0) -- (1,0,0) node[midway, below] {$\vec{u}$};
            \draw[thick,->,primary] (0,0,0) -- (-0.6,1,0) node[midway, left] {$\vec{v}$};
            \draw[thick,->,primary] (0,0,0) -- (0,0,-1) node[midway, right] {$\vec{w}$};

            % Shaded area
            \fill[secondary!40,opacity=0.3] (0,0,0) -- (1,0,0) -- (0.4,1,0) -- (-0.6,1,0) -- cycle;
            \fill[secondary!40,opacity=0.3] (0,0,0) -- (1,0,0) -- (1,0,-1) -- (0,0,-1) -- cycle;
            \draw[dashed] (1,0,0) -- (1,0,-1) -- (0,0,-1) -- (-0.6,1,-1) -- (-0.6,1,0) -- (0.4,1,0) -- cycle;
            \draw[dashed] (1,0,-1) -- (0.4,1,-1) -- (-0.6,1,-1);
            \draw[dashed] (0.4,1,0) -- (0.4,1,-1);
        \end{tikzpicture}
    \end{center}
    $Volume = |\det(B)| = |a_{11}(a_{22}a_{33} - a_{23}a_{32}) - a_{12}(a_{21}a_{33} - a_{23}a_{31}) + a_{13}(a_{21}a_{32} - a_{22}a_{31})|$
\end{eg}
The determinant assiciated with a transformation $T: \mathbb{R}^2 \to \mathbb{R}^2$ can also be interpreted as the scaling factor of areas under the transformation. If \(T\) is represented by a matrix \(A\), then for any region \(R\) in \(\mathbb{R}^2\), the area of the transformed region \(T(R)\) is given by:
\[
    \text{Area}(T(R)) = |\det(A)| \cdot \text{Area}(R)
\]