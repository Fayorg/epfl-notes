\chapter{Matrix Algebra}

\section{Matrix Operations}
\begin{definition}[Coefficient]
    The coefficient of a matrix is an individual element within the matrix, typically denoted by its row and column indices. \\
    For a matrix \( A \) with elements \( a_{ij} \), the coefficient \( a_{ij} \) represents the element located in the \( i \)-th row and \( j \)-th column.
\end{definition}
The columns of a matrix $A$ are often denoted by $\vec{a_1}, ..., \vec{a_n} \in \mathbb{R}^m$, such as:
\[
    A = 
    \begin{bmatrix}
        | & | &        & | \\
        \vec{a_1} & \vec{a_2} & ... & \vec{a_n} \\
        | & | &        & |
    \end{bmatrix}
\]
\begin{definition}[Diagonal Coefficient]
    A diagonal coefficient of a matrix is an element located on the main diagonal of a matrix, which starts from the top left corner and extends to the either the bottom or the right, depending on the matrix's dimensions. For a square matrix \( A \) of size \( n \times n \), the diagonal coefficients are those elements \( a_{ii} \) where the row index \( i \) is equal to the column index \( j \) (i.e., \( i = j \)).
\end{definition}

\begin{definition}[Diagonal Matrix]
    A diagonal matrix is a square matrix in which all the elements outside the main diagonal are zero. The main diagonal itself can contain either zero or non-zero elements. \\
    Formally, a matrix \( D \) of size \( n \times n \) is called a diagonal matrix if \( D_{ij} = 0 \) for all \( i \neq j \). The elements on the main diagonal can be represented as \( D_{ii} \) for \( i = 1, 2, ..., n \).
\end{definition}
\begin{eg}
    An example of a diagonal matrix is:
    \[
        D = 
        \begin{bmatrix}
            5 & 0 & 0 \\
            0 & -3 & 0 \\
            0 & 0 & 2
        \end{bmatrix}
    \]
    In this matrix, all the off-diagonal elements are zero, while the diagonal elements are \( 5, -3, \) and \( 2 \).
\end{eg}

\subsection{Matrix Addition and Scalar Multiplication}
\begin{definition}[Matrix Addition]
    Matrix addition is the operation of adding two matrices of the same dimensions by adding their corresponding elements. \\
    If \( A \) and \( B \) are two matrices of size \( m \times n \), then their sum \( C = A + B \) is also a matrix of size \( m \times n \), where each element \( c_{ij} \) is given by:
    \[
        c_{ij} = a_{ij} + b_{ij}
    \]
    for all \( i = 1, 2, ..., m \) and \( j = 1, 2, ..., n \).
\end{definition}
\begin{eg}
    Consider the following two matrices:
    \[
        A = 
        \begin{bmatrix}
            1 & 2 & 3 \\
            4 & 5 & 6
        \end{bmatrix}
        , \quad
        B = 
        \begin{bmatrix}
            7 & 8 & 9 \\
            10 & 11 & 12
        \end{bmatrix}
    \]
    The sum of these matrices is:
    \[
        C = A + B = 
        \begin{bmatrix}
            1+7 & 2+8 & 3+9 \\
            4+10 & 5+11 & 6+12
        \end{bmatrix}
        = 
        \begin{bmatrix}
            8 & 10 & 12 \\
            14 & 16 & 18
        \end{bmatrix}
    \]
\end{eg}

\begin{definition}[Scalar Multiplication]
    Scalar multiplication is the operation of multiplying each element of a matrix by a scalar (a real number). \\
    If \( A \) is a matrix of size \( m \times n \) and \( k \) is a scalar, then the product \( B = kA \) is also a matrix of size \( m \times n \), where each element \( b_{ij} \) is given by:
    \[
        b_{ij} = k \cdot a_{ij}
    \]
    for all \( i = 1, 2, ..., m \) and \( j = 2, 3, ..., n \).
\end{definition}
\begin{eg}
    Consider the following matrix and scalar:
    \[
        A = 
        \begin{bmatrix}
            1 & 2 & 3 \\
            4 & 5 & 6
        \end{bmatrix}
        , \quad k = 3
    \]
    The product of the scalar and the matrix is:
    \[
        B = kA = 3 \cdot
        \begin{bmatrix}
            1 & 2 & 3 \\
            4 & 5 & 6
        \end{bmatrix}
        =
        \begin{bmatrix}
            3 \cdot 1 & 3 \cdot 2 & 3 \cdot 3 \\
            3 \cdot 4 & 3 \cdot 5 & 3 \cdot 6
        \end{bmatrix}
        =
        \begin{bmatrix}
            3 & 6 & 9 \\
            12 & 15 & 18
        \end{bmatrix}
    \]
\end{eg}

\begin{theorem}
    Let \( A, B, \) and \( C \) be matrices of the same dimensions, and let \( k \) and \( l \) be scalars. The following properties hold for matrix addition and scalar multiplication:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item \( A + B = B + A \) (Commutative Property of Addition)
        \item \( (A + B) + C = A + (B + C) \) (Associative Property of Addition)
        \item There exists a zero matrix \( 0 \) such that \( A + 0 = A \) (Identity Element of Addition)
        \item For every matrix \( A \), there exists a matrix \( -A \) such that \( A + (-A) = 0 \) (Inverse Element of Addition)
        \item \( k(A + B) = kA + kB \) (Distributive Property of Scalar Multiplication over Matrix Addition)
        \item \( (k + l)A = kA + lA \) (Distributive Property of Scalar Multiplication over Scalar Addition)
        \item \( k(lA) = (kl)A \) (Associative Property of Scalar Multiplication)
        \item \( 1A = A \) (Identity Element of Scalar Multiplication)
    \end{itemize}
\end{theorem}

\subsection{Transpose of a Matrix}
\begin{definition}[Transpose of a Matrix]
    The transpose of a matrix \( A \), denoted as \( A^T \), is obtained by swapping the rows and columns of \( A \). If \( A \) is an \( m \times n \) matrix, then its transpose \( A^T \) will be an \( n \times m \) matrix. The element at the \( i \)-th row and \( j \)-th column of \( A \) becomes the element at the \( j \)-th row and \( i \)-th column of \( A^T \). Formally, if \( A = [a_{ij}] \), then:
    \[
        A^T = [a_{ji}]
    \]
    for all valid indices \( i \) and \( j \).
\end{definition}
\begin{eg}
    Let
    \[
        A = \begin{bmatrix}
            1 & 2 & 3 \\
            4 & 5 & 6
        \end{bmatrix}
    \]
    Then the transpose of \( A \) is:
    \[
        A^T = \begin{bmatrix}
            1 & 4 \\
            2 & 5 \\
            3 & 6
        \end{bmatrix}
    \]
\end{eg}

\begin{theorem}
    Let \( A \) and \( B \) be matrices of appropriate dimensions for addition and multiplication, and let \( k \) be a scalar. The following properties hold for the transpose of matrices:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item \( (A^T)^T = A \) (Involution Property)
        \item \( (A + B)^T = A^T + B^T \) (Transpose of a Sum)
        \item \( (kA)^T = kA^T \) (Transpose of a Scalar Multiple)
        \item \( (AB)^T = B^T A^T \) (Transpose of a Product)
    \end{itemize}
\end{theorem}
\begin{proof}
    Let's prove the property \( (AB)^T = B^T A^T \). Let \( A \) be an \( m \times n \) matrix and \( B \) be an \( n \times p \) matrix. The product \( AB \) is an \( m \times p \) matrix, and its transpose \( (AB)^T \) is a \( p \times m \) matrix. The element at the \( i \)-th row and \( j \)-th column of \( AB \) is given by:
    \[
        (AB)_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}
    \]
    Therefore, the element at the \( j \)-th row and \( i \)-th column of \( (AB)^T \) is:
    \[
        (AB)^T_{ji} = (AB)_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}
    \]
    Now, consider the product \( B^T A^T \). The transpose \( B^T \) is a \( p \times n \) matrix, and \( A^T \) is an \( n \times m \) matrix. The product \( B^T A^T \) is a \( p \times m \) matrix, and its element at the \( j \)-th row and \( i \)-th column is given by:
    \[
        (B^T A^T)_{ji} = \sum_{k=1}^{n} b_{jk} a_{ki}
    \]
    Since matrix multiplication is defined such that the order of multiplication matters, we can see that:
    \[
        (B^T A^T)_{ji} = (AB)^T_{ji}
    \]
    for all valid indices \( i \) and \( j \). Thus, we conclude that:
    \[
        (AB)^T = B^T A^T
    \]
\end{proof}
More generally, for any number of matrices \( A_1, A_2, \ldots, A_k \):
\[
    (A_1 A_2 \cdots A_k)^T = A_k^T A_{k-1}^T \cdots A_1^T
\]

\subsection{Matrix Multiplication}
Let's first remark that for a matrix $A$ of size $m \times n$, we can create column/row vectors to extract columns/rows of $A$:
\[
    A = 
    \begin{bmatrix}
        | & | &        & | \\
        \vec{a_1} & \vec{a_2} & ... & \vec{a_n} \\
        | & | &        & |
    \end{bmatrix}
\]
If we want to extract the $j$-th column of $A$, we can use the following multiplication:
\[
    A \cdot \vec{e_j} = \vec{a_j}
\]
where $\vec{e_j}$ is the $j$-th standard basis vector in $\mathbb{R}^n$:
\[
    \vec{e_j} = 
    \begin{bmatrix}
        0 \\
        0 \\
        ... \\
        1 \\
        ... \\
        0
    \end{bmatrix}
\]
where the $1$ is in the $j$-th position. \\
Similarly, we can extract the $i$-th row of $A$ by using the following multiplication:
\[
    \vec{e_i}^T \cdot A = \vec{r_i}
\]
where $\vec{e_i}^T$ is the transpose of the $i$-th standard basis vector in $\mathbb{R}^m$:
\[
    \vec{e_i}^T = 
    \begin{bmatrix}
        0 & 0 & ... & 1 & ... & 0
    \end{bmatrix}
\]
where the $1$ is in the $i$-th position, and $\vec{r_i}$ is the $i$-th row of $A$.
\begin{definition}[Matrix Multiplication]
    Matrix multiplication is the operation of multiplying two matrices to produce a new matrix. For two matrices \( A \) and \( B \), where \( A \) is of size \( m \times n \) and \( B \) is of size \( n \times p \), the product \( C = AB \) is a matrix of size \( m \times p \). The element \( c_{ij} \) of the resulting matrix \( C \) is calculated as:
    \[
        c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}
    \]
    for all \( i = 1, 2, ..., m \) and \( j = 1, 2, ..., p \).
\end{definition}
\begin{proof}
    Let's prove the definition of matrix multiplication using the concept of row and column vectors. Let \( A \) be an \( m \times n \) matrix and \( B \) be an \( n \times p \) matrix. The product \( C = AB \) will be an \( m \times p \) matrix. To find the element \( c_{ij} \) in the resulting matrix \( C \), we can express it as the dot product of the \( i \)-th row of \( A \) and the \( j \)-th column of \( B \). The \( i \)-th row of \( A \) can be represented as \( \vec{r_i} = \vec{e_i}^T A \) and the \( j \)-th column of \( B \) can be represented as \( \vec{b_j} = B \vec{e_j} \). Therefore, we have:
    \[
        c_{ij} = \vec{r_i} \cdot \vec{b_j} = (\vec{e_i}^T A) \cdot (B \vec{e_j})
    \]
    Expanding this dot product, we have:
    \[
        c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}
    \]
    This shows that each element \( c_{ij} \) of the resulting matrix \( C \) is obtained by summing the products of the corresponding elements from the \( i \)-th row of \( A \) and the \( j \)-th column of \( B \). Thus, we have proven the definition of matrix multiplication.
\end{proof}

\begin{eg}
    Consider the following matrices:
    \[
        A = 
        \begin{bmatrix}
            1 & 2 & 3 \\
            4 & 5 & 6
        \end{bmatrix}
        , \quad
        B = 
        \begin{bmatrix}
            7 & 8 \\
            9 & 10 \\
            11 & 12
        \end{bmatrix}
    \]
    The product of these matrices is:
    \[
        C = AB = 
        \begin{bmatrix}
            (1 \cdot 7 + 2 \cdot 9 + 3 \cdot 11) & (1 \cdot 8 + 2 \cdot 10 + 3 \cdot 12) \\
            (4 \cdot 7 + 5 \cdot 9 + 6 \cdot 11) & (4 \cdot 8 + 5 \cdot 10 + 6 \cdot 12)
        \end{bmatrix}
        =
        \begin{bmatrix}
            58 & 64 \\
            139 & 154
        \end{bmatrix}
    \]
\end{eg}
\begin{theorem}
    Let \( A, B, \) and \( C \) be matrices of appropriate dimensions for multiplication, and let \( k \) be a scalar. The following properties hold for matrix multiplication:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item \( A(BC) = (AB)C \) (Associative Property of Multiplication)
        \item \( A(B + C) = AB + AC \) (Distributive Property of Multiplication over Addition)
        \item \( (A + B)C = AC + BC \) (Distributive Property of Multiplication over Addition)
        \item \( k(AB) = (kA)B = A(kB) \) (Scalar Multiplication Property)
        \item There exists an identity matrix \( I \) such that \( AI = IA = A \) (Identity Element of Multiplication)
    \end{itemize}
\end{theorem}
Notes:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item Matrix multiplication is generally not commutative, meaning that \( AB \neq BA \) in most cases.
    \item Generally, matrix multiplication is not cancellative, meaning that from \( AB = AC \), we cannot generally conclude that \( B = C \).
    \item If \( AB = 0 \) (the zero matrix), it does not necessarily imply that either \( A = 0 \) or \( B = 0 \).
\end{itemize}
\begin{eg}
    Consider the following matrices:
    \[
        A = 
        \begin{bmatrix}
            1 & 2 \\
            3 & 4
        \end{bmatrix}
        , \quad
        B = 
        \begin{bmatrix}
            5 & 6 \\
            7 & 8
        \end{bmatrix}
    \]
    The products of these matrices are:
    \[
        AB = 
        \begin{bmatrix}
            (1 \cdot 5 + 2 \cdot 7) & (1 \cdot 6 + 2 \cdot 8) \\
            (3 \cdot 5 + 4 \cdot 7) & (3 \cdot 6 + 4 \cdot 8)
        \end{bmatrix}
        =
        \begin{bmatrix}
            19 & 22 \\
            43 & 50
        \end{bmatrix}
    \]
    and
    \[
        BA = 
        \begin{bmatrix}
            (5 \cdot 1 + 6 \cdot 3) & (5 \cdot 2 + 6 \cdot 4) \\
            (7 \cdot 1 + 8 \cdot 3) & (7 \cdot 2 + 8 \cdot 4)
        \end{bmatrix}
        =
        \begin{bmatrix}
            23 & 34 \\
            31 & 46
        \end{bmatrix}
    \]
    As we can see, \( AB \neq BA \), demonstrating that matrix multiplication is not commutative.
\end{eg}
\begin{eg}
    Consider the following matrices:
    \[
        A = 
        \begin{bmatrix}
            1 & -1 \\
            1 & -1
        \end{bmatrix}
        , \quad
        B = 
        \begin{bmatrix}
            1 & 1 \\
            1 & 1
        \end{bmatrix}
    \]
    The product of these matrices is:
    \[
        AB = 
        \begin{bmatrix}
            (1 \cdot 1 + -1 \cdot 1) & (1 \cdot 1 + -1 \cdot 1) \\
            (1 \cdot 1 + -1 \cdot 1) & (1 \cdot 1 + -1 \cdot 1)
        \end{bmatrix}
        =
        \begin{bmatrix}
            0 & 0 \\
            0 & 0
        \end{bmatrix}
    \]
    Here, \( AB = 0 \) (the zero matrix), but neither \( A = 0 \) nor \( B = 0 \).
\end{eg}

\begin{definition}[Power of a Matrix]
    Let $A$ be a square matrix of size $n \times n$. The power of $A$ is defined as:
    \[
        A^k = \underbrace{A \cdot A \cdot A \cdots A}_{k \text{ times}}
    \]
    for any integer $k \geq 1$, and $A^0 = I_n$, where $I_n$ is the identity matrix of size $n$.
\end{definition}

\section{Inverse of a Matrix}
In real number, the inverse of a non-zero number $a$ is the number $b$ such that $a \cdot b = 1$. Similarly, we can define the inverse of a matrix.
\begin{definition}[Inverse of a Matrix]
    Let $A$ be a square matrix of size $n \times n$. The inverse of $A$, denoted as $A^{-1}$, is a matrix of the same size such that:
    \[
        A \cdot A^{-1} = A^{-1} \cdot A = I_n
    \]
    where $I_n$ is the identity matrix of size $n$. If such a matrix $A^{-1}$ exists, then $A$ is said to be invertible or non-singular. If no such matrix exists, then $A$ is said to be non-invertible or singular.
\end{definition}

\begin{theorem}
    If a matrix $A$ is invertible, then its inverse $A^{-1}$ is unique.
\end{theorem}
\begin{proof}
    Suppose there are two inverses of $A$, denoted as $B$ and $C$. By the definition of the inverse, we have:
    \[
        A \cdot B = I_n \quad \text{and} \quad A \cdot C = I_n
    \]
    To show that $B = C$, we can do the following:
    \[
        B = B \cdot I_n = B \cdot (A \cdot C) = (B \cdot A) \cdot C = I_n \cdot C = C
    \]
    Therefore, the inverse of $A$ is unique.
\end{proof}
\begin{eg}
    Let $A = \begin{bmatrix}
        3 & 5 \\
        2 & 3
    \end{bmatrix}$ and $B = \begin{bmatrix}
        -3 & 5 \\
        2 & -3
    \end{bmatrix}$ be two matrices. We can verify that:
    \[
        A \cdot B = \begin{bmatrix}
            3 & 5 \\
            2 & 3
        \end{bmatrix} \cdot \begin{bmatrix}
            -3 & 5 \\
            2 & -3
        \end{bmatrix} = \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix} = I_2
    \]
    and
    \[
        B \cdot A = \begin{bmatrix}
            -3 & 5 \\
            2 & -3
        \end{bmatrix} \cdot \begin{bmatrix}
            3 & 5 \\
            2 & 3
        \end{bmatrix} = \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix} = I_2
    \]
    Thus, $B$ is the inverse of $A$, denoted as $A^{-1} = B$.
\end{eg}
Note that if $A$ is invertible, then $A^{-1}$ is also invertible and $(A^{-1})^{-1} = A$.

\begin{theorem}
    If $A$ is an invertible matrix, then for any vector $\vec{b} \in \mathbb{R}^n$, the equation $A \cdot \vec{x} = \vec{b}$ has a unique solution given by:
    \[
        \vec{x} = A^{-1} \cdot \vec{b}
    \]
\end{theorem}
\begin{proof}
    To prove this, we start with the equation:
    \[
        A \cdot \vec{x} = \vec{b}
    \]
    Since $A$ is invertible, we can multiply both sides of the equation by $A^{-1}$:
    \[
        A^{-1} \cdot (A \cdot \vec{x}) = A^{-1} \cdot \vec{b}
    \]
    Using the associative property of matrix multiplication, we have:
    \[
        (A^{-1} \cdot A) \cdot \vec{x} = A^{-1} \cdot \vec{b}
    \]
    Since $A^{-1} \cdot A = I_n$, where $I_n$ is the identity matrix, we can simplify the left side:
    \[
        I_n \cdot \vec{x} = A^{-1} \cdot \vec{b}
    \]
    The identity matrix multiplied by any vector $\vec{x}$ is just $\vec{x}$ itself, so we have:
    \[
        \vec{x} = A^{-1} \cdot \vec{b}
    \]
    This shows that the equation $A \cdot \vec{x} = \vec{b}$ has a unique solution given by $\vec{x} = A^{-1} \cdot \vec{b}$.
\end{proof}
If $A$ is invertible, then the column of $A$ are linearly independent and span $\mathbb{R}^n$.

\subsection{Finding the Inverse of a 2x2 Matrix}
\begin{definition}[Inverse of a 2x2 Matrix]
Let $A = \begin{bmatrix}
    a & b \\
    c & d
\end{bmatrix} \in \mathbb{R}^2$, then we can verify that $A$ is invertible if and only if $ad - bc \neq 0$. In this case, the inverse of $A$ is given by:
\[
    A^{-1} = \frac{1}{ad - bc} \begin{bmatrix}
        d & -b \\
        -c & a
    \end{bmatrix}
\]
\end{definition}

\begin{definition}[Determinant of a 2x2 Matrix]
    The determinant of a 2x2 matrix \( A = \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix} \) is a scalar value that can be computed using the formula:
    \[
        \text{det}(A) = ad - bc
    \]
    The determinant provides important information about the matrix, such as whether it is invertible. Specifically, the matrix \( A \) is invertible if and only if \( \text{det}(A) \neq 0 \).
\end{definition}

\begin{eg}
    Let $A = \begin{bmatrix}
        2 & 3 \\
        2 & 4
    \end{bmatrix}$. Then, we can compute the determinant of \( A \):
    \[
        \text{det}(A) = (2)(4) - (3)(2) = 8 - 6 = 2
    \]
    Since \( \text{det}(A) \neq 0 \), the matrix \( A \) is invertible. The inverse of \( A \) can be calculated as:
    \[
        A^{-1} = \frac{1}{2} \begin{bmatrix}
            4 & -3 \\
            -2 & 2
        \end{bmatrix}
    \]
    To verify, we can check that:
    \[
        A \cdot A^{-1} = \begin{bmatrix}
            2 & 3 \\
            2 & 4
        \end{bmatrix} \cdot \frac{1}{2} \begin{bmatrix}
            4 & -3 \\
            -2 & 2
        \end{bmatrix} = \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix} = I_2
    \]
    This means that for a system of equations represented by \( A \cdot \vec{x} = \vec{b} \) (for any $\vec{b} \in \mathbb{R}^2$), the unique solution can be found using:
    \[
        \vec{x} = A^{-1} \cdot \vec{b}
    \]
\end{eg}
\begin{theorem}
    If $A$ and $B$ are invertible matrices of the same size, the the following properties hold:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The inverse of $A^{-1}$ is $A$, i.e.:
        \[
            (A^{-1})^{-1} = A
        \]
        \item The product $AB$ is also invertible, and its inverse is given by:
        \[
            (AB)^{-1} = B^{-1} A^{-1}
        \]
        \item The transpose of $A$ is also invertible, and its inverse is given by:
        \[
            (A^T)^{-1} = (A^{-1})^T
        \]
    \end{itemize}
\end{theorem}
\begin{proof}
    Let's prove the second property: \( (AB)^{-1} = B^{-1} A^{-1} \). We need to show that multiplying \( AB \) by \( B^{-1} A^{-1} \) results in the identity matrix \( I_n \). We have:
    \[
        (AB)(B^{-1} A^{-1}) = A(B B^{-1}) A^{-1} = A I_n A^{-1} = A A^{-1} = I_n
    \]
    Similarly, we can show that:
    \[
        (B^{-1} A^{-1})(AB) = B^{-1}(A^{-1} A)B = B^{-1} I_n B = B^{-1} B = I_n
    \]
    Since both products yield the identity matrix, we conclude that \( (AB)^{-1} = B^{-1} A^{-1} \).
\end{proof}

\subsection{Finding the Inverse of a Matrix}
\begin{theorem}
    Let $A$ be a square matrix of size $n \times n$. To find the inverse of $A$, we can augment $A$ with the identity matrix $I_n$ and perform row operations to transform $A$ into $I_n$. The augmented matrix will then transform into $[I_n | A^{-1}]$. If $A$ cannot be transformed into $I_n$, then $A$ is not invertible.
\end{theorem}
\begin{proof}
    Let's prove column by column. Let $\vec{e_1}, \vec{e_2}, ..., \vec{e_n}$ be the columns of the identity matrix $I_n$. The matrix $A^{-1}$ is of size $n \times n$, and we can denote its columns as $\vec{y_1}, \vec{y_2}, ..., \vec{y_n}$, we then have:
    \[
        A^{-1} = \begin{bmatrix}
            | & | &        & | \\
            \vec{y_1} & \vec{y_2} & ... & \vec{y_n} \\
            | & | &        & |
        \end{bmatrix}
    \]
    where the $k$-th column is $\vec{y_k}$ that we can also obtain by:
    \[
        \vec{y_k} = A^{-1} \cdot \vec{e_k}
    \]
    If we then multiply the left side by $A$, we have:
    \[
        A \cdot \vec{y_k} = A \cdot (A^{-1} \cdot \vec{e_k}) = (A \cdot A^{-1}) \cdot \vec{e_k} = I_n \cdot \vec{e_k} = \vec{e_k}
    \]
    This means that to find the $k$-th column of $A^{-1}$, we need to solve the equation:
    \[
        A \cdot \vec{y_k} = \vec{e_k}
    \]
    for each $k = 1, 2, ..., n$. By augmenting $A$ with $I_n$ and performing row operations, we are effectively solving these equations simultaneously. If we can transform $A$ into $I_n$, then the augmented part will transform into $A^{-1}$. If we cannot transform $A$ into $I_n$, then there exists no solution for at least one of these equations, indicating that $A$ is not invertible.
\end{proof}

\begin{eg}
    Let $A = \begin{bmatrix}
        2 & 3 \\
        2 & 4
    \end{bmatrix}$ be a matrix. To find its inverse, we augment it with the identity matrix $I_2$:
    \[
        [A | I_2] = \left[\begin{array}{cc|cc}
            2 & 3 & 1 & 0 \\
            2 & 4 & 0 & 1
        \end{array}\right]
    \]
    We then perform row operations to transform the left side into the identity matrix:
    \[
        \xrightarrow{R_2 \leftarrow R_2 - R_1}
        \left[\begin{array}{cc|cc}
            2 & 3 & 1 & 0 \\
            0 & 1 & -1 & 1
        \end{array}\right]
        \xrightarrow{R_1 \leftarrow R_1 - 3R_2}
        \left[\begin{array}{cc|cc}
            2 & 0 & 4 & -3 \\
            0 & 1 & -1 & 1
        \end{array}\right]
    \]
    \[
        \xrightarrow{R_1 \leftarrow \frac{1}{2} R_1}
        \left[\begin{array}{cc|cc}
            1 & 0 & 2 & -\frac{3}{2} \\
            0 & 1 & -1 & 1
        \end{array}\right]
    \]
    The left side is now the identity matrix, and the right side is the inverse of $A$:
    \[
        A^{-1} = \begin{bmatrix}
            2 & -\frac{3}{2} \\
            -1 & 1
        \end{bmatrix}
    \]
\end{eg}

\section{Elementary Matrices}
Elementary operations are the row operations that we can perform on a matrix. There are three types of elementary row operations:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item Interchanging two rows.
    \item Multiplying a row by a non-zero scalar.
    \item Adding a multiple of one row to another row.
\end{itemize}
These operations can be represented by elementary matrices that can be multiplied with the original matrix to perform the corresponding row operation.
\begin{definition}[Elementary Matrix]
    An elementary matrix is a matrix that is obtained by performing a single elementary row operation on an identity matrix. There are three types of elementary matrices corresponding to the three types of elementary row operations:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Type 1: Interchanging two rows of the identity matrix.
        \item Type 2: Multiplying a row of the identity matrix by a non-zero scalar.
        \item Type 3: Adding a multiple of one row of the identity matrix to another row.
    \end{itemize}
    When an elementary matrix \( E \) is multiplied by a matrix \( A \) from the left (i.e., \( EA \)), it performs the corresponding row operation on \( A \).
\end{definition}
\begin{eg}
    Let's apply each type of elementary row operation to a matrix \( A \) using elementary matrices. Consider the matrix:
    \[
        A = \begin{bmatrix}
            1 & 2 & 3 \\
            4 & 5 & 6 \\
            7 & 8 & 9
        \end{bmatrix}
    \]
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Type 1: Interchanging the first and second rows. The elementary matrix for this operation is:
        \[
            E_1 = \begin{bmatrix}
                0 & 1 & 0 \\
                1 & 0 & 0 \\
                0 & 0 & 1
            \end{bmatrix}
        \]
        Multiplying \( E_1 \) by \( A \):
        \[
            E_1 A = \begin{bmatrix}
                0 & 1 & 0 \\
                1 & 0 & 0 \\
                0 & 0 & 1
            \end{bmatrix} \begin{bmatrix}
                1 & 2 & 3 \\
                4 & 5 & 6 \\
                7 & 8 & 9
            \end{bmatrix} = \begin{bmatrix}
                4 & 5 & 6 \\
                1 & 2 & 3 \\
                7 & 8 & 9
            \end{bmatrix}
        \]
        \item Type 2: Multiplying the second row by 3. The elementary matrix for this operation is:
        \[
            E_2 = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 3 & 0 \\
                0 & 0 & 1
            \end{bmatrix}
        \]
        Multiplying \( E_2 \) by \( A \):
        \[
            E_2 A = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 3 & 0 \\
                0 & 0 & 1
            \end{bmatrix} \begin{bmatrix}
                1 & 2 & 3 \\
                4 & 5 & 6 \\
                7 & 8 & 9
            \end{bmatrix} = \begin{bmatrix}
                1 & 2 & 3 \\
                12 & 15 & 18 \\
                7 & 8 & 9
            \end{bmatrix}
        \]
        \item Type 3: Adding 2 times the first row to the third row. The elementary matrix for this operation is:
        \[
            E_3 = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                2 & 0 & 1
            \end{bmatrix}
        \]
        Multiplying \( E_3 \) by \( A \):
        \[
            E_3 A = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                2 & 0 & 1
            \end{bmatrix} \begin{bmatrix}
                1 & 2 & 3 \\
                4 & 5 & 6 \\
                7 & 8 & 9
            \end{bmatrix} = \begin{bmatrix}
                1 & 2 & 3 \\
                4 & 5 & 6 \\
                9 & 12 & 15
            \end{bmatrix}
        \]
    \end{itemize}
\end{eg}
Since these elementary row operations are reversible, each elementary matrix is invertible. The inverse of an elementary matrix corresponds to the reverse of the original row operation:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item The inverse of a Type 1 elementary matrix (row interchange) is the same matrix, as interchanging the same two rows again will revert to the original configuration.
    \item The inverse of a Type 2 elementary matrix (row scaling) is obtained by multiplying the same row by the reciprocal of the original scalar.
    \item The inverse of a Type 3 elementary matrix (row addition) is obtained by subtracting the same multiple of the original row from the target row.
\end{itemize}
\begin{eg}
    Let's find the inverses of the elementary matrices from the previous example:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item For the Type 1 elementary matrix \( E_1 \):
        \[
            E_1 = \begin{bmatrix}
                0 & 1 & 0 \\
                1 & 0 & 0 \\
                0 & 0 & 1
            \end{bmatrix}
        \]
        The inverse is:
        \[
            E_1^{-1} = E_1 = \begin{bmatrix}
                0 & 1 & 0 \\
                1 & 0 & 0 \\
                0 & 0 & 1
            \end{bmatrix}
        \]
        \item For the Type 2 elementary matrix \( E_2 \):
        \[
            E_2 = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 3 & 0 \\
                0 & 0 & 1
            \end{bmatrix}
        \]
        The inverse is:
        \[
            E_2^{-1} = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & \frac{1}{3} & 0 \\
                0 & 0 & 1
            \end{bmatrix}
        \]
        \item For the Type 3 elementary matrix \( E_3 \):
        \[
            E_3 = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                2 & 0 & 1
            \end{bmatrix}
        \]
        The inverse is:
        \[
            E_3^{-1} = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                -2 & 0 & 1
            \end{bmatrix}
        \]
    \end{itemize}
\end{eg}
If there exists a sequence of elementary row operations that transforms a matrix \( A \) into the identity matrix \( I_n \), we have:
\[
    E_k E_{k-1} \cdots E_2 E_1 A = I_n
\]
where \( E_1, E_2, \ldots, E_k \) are the elementary matrices corresponding to the row operations. By multiplying on the right both sides by the inverse of $A$, we get:
\[
    E_k E_{k-1} \cdots E_2 E_1 A A^{-1} = I_n A^{-1}
\]
Since \( A A^{-1} = I_n \), we have:
\[
    E_k E_{k-1} \cdots E_2 E_1 = A^{-1}
\]
Thus, we can express the inverse of \( A \) as a product of elementary matrices.

\section{Invertible Matrix Theorem}
\begin{theorem}
    Let $A$ be a square matrix of size $n \times n$. The following statements are equivalent (meaning that they are either all true or all false):
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $A$ is invertible.
        \item $A$ is row equivalent to the identity matrix $I_n$
        \item $A$ has $n$ pivot positions.
        \item The equation $A \cdot \vec{x} = 0$ has only the trivial solution $\vec{x} = 0$.
        \item The columns of $A$ are linearly independent.
        \item The linear transformation $T(\vec{x}) = A \cdot \vec{x}$ is injective (one-to-one).
        \item The equation $A \cdot \vec{x} = \vec{b}$ has a unique solution for every $\vec{b} \in \mathbb{R}^n$.
        \item The columns of $A$ span $\mathbb{R}^n$.
        \item The linear transformation $T(\vec{x}) = A \cdot \vec{x}$ is surjective (onto).
        \item There exists a matrix $C$ such that $C \cdot A = I_n$.
        \item There exists a matrix $D$ such that $A \cdot D = I_n$.
        \item The transpose of $A$, denoted as $A^T$, is invertible.
    \end{itemize}
\end{theorem}

\subsection{Inverse of Linear Transformations}
\begin{definition}[Inverse of a Linear Transformation]
    Let \( T: \mathbb{R}^n \to \mathbb{R}^n \) be a linear transformation. The inverse of \( T \) (or the reciprocal), denoted as \( T^{-1} \), is a function \( T^{-1}: \mathbb{R}^n \to \mathbb{R}^n \) such that:
    \[
        T(T^{-1}(\vec{y})) = \vec{y} \quad \text{for all } \vec{y} \in \mathbb{R}^n
    \]
    and
    \[
        T^{-1}(T(\vec{x})) = \vec{x} \quad \text{for all } \vec{x} \in \mathbb{R}^n
    \]
    If such a function \( T^{-1} \) exists, then \( T \) is said to be invertible and the associated matrix of \( T^{-1} \) is the inverse of the matrix of \( T \).
\end{definition}