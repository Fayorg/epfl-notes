\chapter{Orthogonality and the least squares method}
Let's consider the following system of linear equations:
\[
    A \vec{x} = \vec{b}
\]
with $A \in \mathbb{R}^{m \times n}$, $\vec{b} \in \mathbb{R}^n$ and $\vec{x} \in \mathbb{R}^n$. If this system has no solution, there might be $\vec{x}$ such that:
\[
    A \vec{x} \approx \vec{b}
\]
Thus new concepts, such as size and distance, are needed to find an approximate solution.

\section{Dot Product}
\begin{definition}[Dot Product]
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$. The dot product of $\vec{u}$ and $\vec{v}$ is defined as:
    \[
        \vec{u} \cdot \vec{v} = \vec{u}^T \vec{v} = \begin{bmatrix}
            u_1 & u_2 & \ldots & u_n
        \end{bmatrix} \begin{bmatrix}
            v_1 \\ v_2 \\ \vdots \\ v_n
        \end{bmatrix} = u_1 v_1 + u_2 v_2 + \ldots + u_n v_n
    \]
\end{definition}
Remark that $\vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u}$, i.e., the dot product is commutative.

\begin{eg}
    Let $\vec{u} = \begin{bmatrix}
        1 \\ 2 \\ 3
    \end{bmatrix}$ and $\vec{v} = \begin{bmatrix}
        4 \\ 5 \\ 6
    \end{bmatrix}$. Then:
    \[
        \vec{u} \cdot \vec{v} = 1 \cdot 4 + 2 \cdot 5 + 3 \cdot 6 = 32
    \]
\end{eg}

\begin{theorem}
    Let $\vec{u}, \vec{v}$ and $\vec{w}$ be vectors in $\mathbb{R}^n$ and $c \in \mathbb{R}$. The following properties hold:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $\vec{u} \cdot \vec{u} \geq 0$ and $\vec{u} \cdot \vec{u} = 0$ if and only if $\vec{u} = \vec{0}$ (positivity).
        \item $\vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u}$ (commutativity).
        \item $(\vec{u} + \vec{v}) \cdot \vec{w} = \vec{u} \cdot \vec{w} + \vec{v} \cdot \vec{w}$ (distributivity).
        \item $(c \vec{u}) \cdot \vec{v} = c (\vec{u} \cdot \vec{v})$ (homogeneity).
    \end{itemize}
\end{theorem}

\begin{eg}
    Let $\vec{u_1}, \vec{u_2}, \vec{u_3}$ and $\vec{w}$ be vectors in $\mathbb{R}^n$. Then:
    \[
        (7\vec{u_1} - 2 \vec{u_2} + 4\vec{u_3}) \cdot \vec{w} = 7(\vec{u_1} \cdot \vec{w}) - 2(\vec{u_2} \cdot \vec{w}) + 4(\vec{u_3} \cdot \vec{w})
    \]
\end{eg}

\subsection{Norm}
\begin{definition}[Norm]
    Let $\vec{u} \in \mathbb{R}^n$. The norm (or length) of $\vec{u}$ is defined as:
    \[
        \|\vec{u}\| = \sqrt{\vec{u} \cdot \vec{u}} = \sqrt{u_1^2 + u_2^2 + \ldots + u_n^2}
    \]
\end{definition}
Note that since $\vec{u} \cdot \vec{u} \geq 0$, the norm is always defined.

\begin{eg}
    Let $\vec{u} = \begin{bmatrix}
        3 \\ -4
    \end{bmatrix}$. Then:
    \[
        \|\vec{u}\| = \sqrt{3^2 + (-4)^2} = \sqrt{9 + 16} = \sqrt{25} = 5
    \]
    We then have:
    \[
        \|5 \vec{u}\| = 5 \|\vec{u}\| = 5 \times 5 = 25
    \]
    and \[
        \|-\vec{u}\| = \| \vec{u} \| = 5
    \]
\end{eg}

\begin{definition}[Unit Vector]
    A unit vector is a vector $\vec{u}$ such that $\|\vec{u}\| = 1$.
\end{definition}
Remark that any non-zero vector $\vec{u}$ can be converted into a unit vector by dividing it by its norm:
\[
    \frac{\vec{u}}{\|\vec{u}\|}
\]
This process is called normalizing the vector $\vec{u}$.

\begin{eg}
    Let $\vec{u} = \begin{bmatrix}
        3 \\ 4
    \end{bmatrix}$. Then the unit vector in the direction of $\vec{u}$ is:
    \[
        \frac{\vec{u}}{\|\vec{u}\|} = \frac{1}{5} \begin{bmatrix}
            3 \\ 4
        \end{bmatrix} = \begin{bmatrix}
            \frac{3}{5} \\ \frac{4}{5}
        \end{bmatrix}
    \]
\end{eg}

\subsection{Distance}
\begin{definition}[Distance]
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$. The distance between $\vec{u}$ and $\vec{v}$ is defined as:
    \[
        d(\vec{u}, \vec{v}) = \|\vec{u} - \vec{v}\|
    \]
\end{definition}
Remark that the distance is symmetric, i.e., $d(\vec{u}, \vec{v}) = d(\vec{v}, \vec{u})$. Graphically:
\begin{center}
    \begin{tikzpicture}
        \draw[->, primary] (0,0) -- (4,2) node[midway, above] {$\vec{u}$};
        \draw[->, primary] (0,0) -- (1,3) node[midway, left] {$\vec{v}$};
        \draw[->, dashed, secondary, thick] (4,2) -- (1,3) node[midway, above right] {$\vec{u} - \vec{v}$};
        % \draw[fill] (4,2) circle [radius=0.05];
        % \draw[fill] (1,3) circle [radius=0.05];
    \end{tikzpicture}
\end{center}

\begin{eg}
    Let $\vec{u} = \begin{bmatrix}
        2 \\ 3
    \end{bmatrix}$ and $\vec{v} = \begin{bmatrix}
        5 \\ 7
    \end{bmatrix}$. Then the distance between $\vec{u}$ and $\vec{v}$ is:
    \[
        d(\vec{u}, \vec{v}) = \|\vec{u} - \vec{v}\| = \left\| \begin{bmatrix}
            2 - 5 \\ 3 - 7
        \end{bmatrix} \right\| = \left\| \begin{bmatrix}
            -3 \\ -4
        \end{bmatrix} \right\| = \sqrt{(-3)^2 + (-4)^2} = \sqrt{9 + 16} = \sqrt{25} = 5
    \]
\end{eg}

\section{Orthogonality}
\begin{definition}[Orthogonal Vectors]
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$. The vectors $\vec{u}$ and $\vec{v}$ are said to be orthogonal if:
    \[
        \vec{u} \cdot \vec{v} = 0
    \]
\end{definition}

\begin{eg}
    Let $\vec{u} = \begin{bmatrix}
        1 \\ 2 \\ -2
    \end{bmatrix}$ and $\vec{v} = \begin{bmatrix}
        2 \\ -1 \\ 1
    \end{bmatrix}$. Then:
    \[
        \vec{u} \cdot \vec{v} = 1 \cdot 2 + 2 \cdot (-1) + (-2) \cdot 1 = 2 - 2 - 2 = -2
    \]
    Thus, $\vec{u}$ and $\vec{v}$ are not orthogonal.
\end{eg}

\begin{theorem}
    Let $\vec{u}$ and $\vec{v}$ be orthogonal vectors in $\mathbb{R}^n$ if and only if:
    \[
        \|\vec{u} + \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2
    \]
\end{theorem}
\begin{proof}
    We have:
    \begin{align*}
        \|\vec{u} + \vec{v}\|^2 &= (\vec{u} + \vec{v}) \cdot (\vec{u} + \vec{v}) \\
        &= \vec{u} \cdot \vec{u} + 2 \vec{u} \cdot \vec{v} + \vec{v} \cdot \vec{v} \\
        &= \|\vec{u}\|^2 + 2 \vec{u} \cdot \vec{v} + \|\vec{v}\|^2
    \end{align*}
    Thus, if $\vec{u}$ and $\vec{v}$ are orthogonal, then $\vec{u} \cdot \vec{v} = 0$ and:
    \[
        \|\vec{u} + \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2
    \]
    Conversely, if $\|\vec{u} + \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2$, then:
    \[
        0 = 2 \vec{u} \cdot \vec{v}
    \]
    which implies that $\vec{u} \cdot \vec{v} = 0$, i.e., $\vec{u}$ and $\vec{v}$ are orthogonal.
\end{proof}
Remark that when multiplying a vector and a matrix, the result is multiple dot product between the rows of the matrix and the vector. Thus, if all rows of a matrix are orthogonal to a vector, then the result of the multiplication is the zero vector.

\begin{eg}
    Let $\vec{v_1} = \begin{bmatrix}
        7 \\ 3 \\ 1
    \end{bmatrix}, \vec{v_2} = \begin{bmatrix}
        2 \\ 4 \\ 5
    \end{bmatrix}$ and $\vec{x} = \begin{bmatrix}
        9 \\ 10 \\ 8
    \end{bmatrix}$. Then:
    \[
        \begin{bmatrix}
            7 & 3 & 1 \\
            2 & 4 & 5
        \end{bmatrix} \begin{bmatrix}
            9 \\ 10 \\ 8
        \end{bmatrix} = \begin{bmatrix}
            \vec{v_1} \cdot \vec{x} \\
            \vec{v_2} \cdot \vec{x}
        \end{bmatrix} = \begin{bmatrix}
            7 \cdot 9 + 3 \cdot 10 + 1 \cdot 8 \\
            2 \cdot 9 + 4 \cdot 10 + 5 \cdot 8
        \end{bmatrix} = \begin{bmatrix}
            63 + 30 + 8 \\
            18 + 40 + 40
        \end{bmatrix} = \begin{bmatrix}
            101 \\ 98
        \end{bmatrix}
    \]
    Therefore, $\vec{x}$ is not orthogonal to the rows of the matrix.
\end{eg}

\subsection{Angle between Vectors}
\begin{theorem}
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$ be non-zero vectors. The angle $\theta$ between $\vec{u}$ and $\vec{v}$ is given by:
    \[
        \cos(\theta) = \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\| \|\vec{v}\|}
    \]
\end{theorem}
\begin{proof}
    From the law of cosines, we have:
    \[
        a^2 + b^2 - 2ab \cos(\theta) = c^2
    \]
    where $a = \|\vec{u}\|$, $b = \|\vec{v}\|$ and $c = \|\vec{u} - \vec{v}\|$. Thus:
    \[
        \|\vec{u}\|^2 + \|\vec{v}\|^2 - 2 \|\vec{u}\| \|\vec{v}\| \cos(\theta) = \|\vec{u} - \vec{v}\|^2
    \]
    But:
    \begin{align*}
        \|\vec{u} - \vec{v}\|^2 &= (\vec{u} - \vec{v}) \cdot (\vec{u} - \vec{v}) \\
        &= \vec{u} \cdot \vec{u} - 2 \vec{u} \cdot \vec{v} + \vec{v} \cdot \vec{v} \\
        &= \|\vec{u}\|^2 - 2 \vec{u} \cdot \vec{v} + \|\vec{v}\|^2
    \end{align*}
    Thus:
    \[
        \|\vec{u}\|^2 + \|\vec{v}\|^2 - 2 \|\vec{u}\| \|\vec{v}\| \cos(\theta) = \|\vec{u}\|^2 - 2 \vec{u} \cdot \vec{v} + \|\vec{v}\|^2
    \]
    which implies that:
    \[
        \vec{u} \cdot \vec{v} = \|\vec{u}\| \|\vec{v}\| \cos(\theta)
    \]
    Therefore:
    \[
        \cos(\theta) = \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\| \|\vec{v}\|}
    \]
\end{proof}

\subsection{Orthogonality to a Subspace}
Remark that if $\vec{u}$ is orthogonal to $\vec{v}$, then $\vec{u}$ is also orthogonal to any vector in $\text{span}\{\vec{v}\}$ i.e., for any vector $\vec{w} = c \vec{v}$ for some scalar $c \in \mathbb{R}$:
\[
    \vec{u} \cdot \vec{w} = \vec{u} \cdot (c \vec{v}) = c (\vec{u} \cdot \vec{v}) = c \cdot 0 = 0
\]

\begin{definition}[Orthogonal Complement to a Subspace]
    Let $W$ be a subspace of $\mathbb{R}^n$. The orthogonal complement to $W$, denoted by $W^{\perp}$, is defined as:
    \[
        W^{\perp} = \{ \vec{u} \in \mathbb{R}^n \mid \vec{u} \cdot \vec{w} = 0, \forall \vec{w} \in W \}
    \]
\end{definition}
Graphically, if $W$ is a line in $\mathbb{R}^3$, then $W^{\perp}$ is the plane orthogonal to that line passing through the origin:
\begin{center}
    \begin{tikzpicture}[scale=2.5]
        % Axes
        \draw[->] (-0.8,0,0) -- (1.2,0,0) node[right] {$x_1$};
        \draw[->] (0,-1.0,0) -- (0,1.2,0) node[above] {$x_2$};
        \draw[->] (0,0,0.8) -- (0,0,-1.2) node[below right] {$x_3$};

        % Plane WâŠ¥ (contains x_3 and makes -45 deg with x_1)
        \filldraw[fill=secondary!40, draw=secondary, opacity=0.3] 
            (0.7,-0.7,-0.8) -- (0.7,-0.7,0.8) -- (-0.7,0.7,0.8) -- (-0.7,0.7,-0.8) -- cycle;
        \node[secondary, thick, left] at (-0.7,0.7,0.8) {$W^{\perp}$};
        
        % Line W (orthogonal to the plane, i.e. along x_2 = x_1)
        \draw[->, thick, primary] (0,0,0) -- (0.8,0.8,0) node[right] {$W$};
        \draw[dashed, thick, primary] (0,0,0) -- (-0.3,-0.3,0);
        \draw[thick, primary] (-0.3,-0.3,0) -- (-0.6,-0.6,0);
    \end{tikzpicture}
\end{center}

\begin{theorem}
    $W^{\perp}$ is a subspace of $\mathbb{R}^n$.
\end{theorem}
\begin{proof}
    Let $\vec{u_1}, \vec{u_2} \in W^{\perp}$ and $c \in \mathbb{R}$. We need to show that $\vec{u_1} + \vec{u_2} \in W^{\perp}$ and $c \vec{u_1} \in W^{\perp}$.
    For any $\vec{w} \in W$, we have:
    \[
        (\vec{u_1} + \vec{u_2}) \cdot \vec{w} = \vec{u_1} \cdot \vec{w} + \vec{u_2} \cdot \vec{w} = 0 + 0 = 0
    \]
    Thus, $\vec{u_1} + \vec{u_2} \in W^{\perp}$.
    Similarly, for any $\vec{w} \in W$, we have:
    \[
        (c \vec{u_1}) \cdot \vec{w} = c (\vec{u_1} \cdot \vec{w}) = c \cdot 0 = 0
    \]
    Thus, $c \vec{u_1} \in W^{\perp}$.
    Therefore, $W^{\perp}$ is a subspace of $\mathbb{R}^n$.
\end{proof}

\begin{theorem}
    $(W^{\perp})^{\perp} = W$
\end{theorem}
\begin{proof}
    We need to show that $W \subseteq (W^{\perp})^{\perp}$ and $(W^{\perp})^{\perp} \subseteq W$.
    Let $\vec{w} \in W$. For any $\vec{u} \in W^{\perp}$, we have:
    \[
        \vec{w} \cdot \vec{u} = 0
    \]
    Thus, $\vec{w} \in (W^{\perp})^{\perp}$, which implies that $W \subseteq (W^{\perp})^{\perp}$.
    Conversely, let $\vec{v} \in (W^{\perp})^{\perp}$. For any $\vec{w} \in W$, we have:
    \[
        \vec{v} \cdot \vec{w} = 0
    \]
    Thus, $\vec{v} \in W$, which implies that $(W^{\perp})^{\perp} \subseteq W$.
    Therefore, $(W^{\perp})^{\perp} = W$.
\end{proof}

\begin{theorem}
    If $W = \text{span}(\vec{w_1}, \vec{w_2}, \ldots, \vec{w_k})$, then $\vec{u} \in W^{\perp}$ if and only if $\vec{u} \cdot \vec{w_i} = 0$ for all $i = 1, 2, \ldots, k$.
\end{theorem}
\begin{proof}
    \textbf{($\Rightarrow$)} Let $\vec{u} \in W^{\perp}$. Then, for any $\vec{w} \in W$, we have $\vec{u} \cdot \vec{w} = 0$. In particular, for each basis vector $\vec{w_i}$, we have $\vec{u} \cdot \vec{w_i} = 0$. \\
    \textbf{($\Leftarrow$)} Let $\vec{u}$ be such that $\vec{u} \cdot \vec{w_i} = 0$ for all $i = 1, 2, \ldots, k$. Any vector $\vec{w} \in W$ can be expressed as a linear combination of the basis vectors:
    \[
        \vec{w} = c_1 \vec{w_1} + c_2 \vec{w_2} + \ldots + c_k \vec{w_k}
    \]
    for some scalars $c_1, c_2, \ldots, c_k$. Then:
    \[
        \vec{u} \cdot \vec{w} = \vec{u} \cdot (c_1 \vec{w_1} + c_2 \vec{w_2} + \ldots + c_k \vec{w_k}) = c_1 (\vec{u} \cdot \vec{w_1}) + c_2 (\vec{u} \cdot \vec{w_2}) + \ldots + c_k (\vec{u} \cdot \vec{w_k}) = 0
    \]
    Thus, $\vec{u} \in W^{\perp}$.
\end{proof}

\begin{theorem}
    $(\text{Im} A^T)^{\perp} = \text{Ker} A$
\end{theorem}
\begin{proof}
    We need to show that $(\text{Im} A^T)^{\perp} \subseteq \text{Ker} A$ and $\text{Ker} A \subseteq (\text{Im} A^T)^{\perp}$.
    Let $\vec{x} \in \text{Ker A}$ and let $\vec{w} \in \text{Im} A^T$. Then, there exists $\vec{y}$ such that $\vec{w} = A^T \vec{y}$. Thus:
    \[
        \vec{x} \cdot \vec{w} = \vec{x} \cdot (A^T \vec{y}) = (A \vec{x}) \cdot \vec{y} = \vec{0} \cdot \vec{y} = 0
    \]
    Therefore, $\vec{x} \in (\text{Im} A^T)^{\perp}$, which implies that $\text{Ker} A \subseteq (\text{Im} A^T)^{\perp}$.
    Conversely, let $\vec{x} \in (\text{Im} A^T)^{\perp}$. For any $\vec{y}$, we have:
    \[
        \vec{x} \cdot (A^T \vec{y}) = 0
    \]
    which implies that:
    \[
        (A \vec{x}) \cdot \vec{y} = 0
    \]
    for all $\vec{y}$. Thus, $A \vec{x} = \vec{0}$, which means that $\vec{x} \in \text{Ker} A$. Therefore, $(\text{Im} A^T)^{\perp} \subseteq \text{Ker} A$.
\end{proof}
Remark that from this theorem, the following corollary holds:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $\text{Im} A = (\text{Ker} A^T)^{\perp}$
    \item $\text{Ker} A^T = (\text{Im} A)^{\perp}$
    \item $\text{Im} A^T = (\text{Ker} A)^{\perp}$
    \item $\text{Ker} A = (\text{Im} A^T)^{\perp}$
\end{itemize}

\begin{eg}
    Let $W$ be a subspace of $\mathbb{R}^n$ and let $(\vec{w_1}, \ldots, \vec{w_p})$ a basis of $W$. To find a basis of $W^{\perp}$, we can use the theorem above to get:
    \[
        W = \text{Im} A \quad \iff \quad W^{\perp} = \left(\text{Im} A\right)^{\perp} = \text{Ker} A^T
    \]
    where:
    \[
        A^T = \begin{bmatrix}
            - & (\vec{w_1})^T & - \\
            - & (\vec{w_2})^T & - \\
            & \vdots & \\
            - & (\vec{w_p})^T & -   
        \end{bmatrix}
    \]
    Therefore, a basis of $W^{\perp}$ can be found by finding the kernel of $A^T$.
\end{eg}

\subsection{Orthogonal Family}
\begin{definition}[Orthogonal Family]
    A family of vectors $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ in $\mathbb{R}^n$ is said to be orthogonal if:
    \[
        \vec{u_i} \cdot \vec{u_j} = 0 \quad \text{for all } i \neq j
    \]
\end{definition}
Note that since the null vector is orthogonal to any vector, an orthogonal family can contain the null vector.

\begin{eg}
    Let $\vec{v_1} = \begin{bmatrix}
        2 \\ 1
    \end{bmatrix}$ and $\vec{v_2} = \begin{bmatrix}
        2 \\ -4
    \end{bmatrix}$ in $\mathbb{R}^2$. Then:
    \[
        \vec{v_1} \cdot \vec{v_2} = 2 \cdot 2 + 1 \cdot (-4) = 4 - 4 = 0
    \]
    Thus, $(\vec{v_1}, \vec{v_2})$ is an orthogonal family.
\end{eg}

\begin{eg}
    Let $\vec{v_1} = \begin{bmatrix}
        1 \\ 0 \\ 0
    \end{bmatrix}, \vec{v_2} = \begin{bmatrix}
        0 \\ 1 \\ 0
    \end{bmatrix}$ and $\vec{v_3} = \begin{bmatrix}
        0 \\ 0 \\ 1
    \end{bmatrix}$ in $\mathbb{R}^3$. Then:
    \[
        \vec{v_1} \cdot \vec{v_2} = 1 \cdot 0 + 0 \cdot 1 + 0 \cdot 0 = 0
    \]
    \[
        \vec{v_1} \cdot \vec{v_3} = 1 \cdot 0 + 0 \cdot 0 + 0 \cdot 1 = 0
    \]
    \[
        \vec{v_2} \cdot \vec{v_3} = 0 \cdot 0 + 1 \cdot 0 + 0 \cdot 1 = 0
    \]
    Thus, $(\vec{v_1}, \vec{v_2}, \vec{v_3})$ is an orthogonal family.
\end{eg}

\begin{theorem}
    An orthogonal family of non-zero vectors is linearly independent.
\end{theorem}
\begin{proof}
    Let $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ be an orthogonal family of non-zero vectors in $\mathbb{R}^n$. We need to show that if:
    \[
        c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k} = \vec{0}
    \]
    for some scalars $c_1, c_2, \ldots, c_k$, then $c_1 = c_2 = \ldots = c_k = 0$.
    Taking the dot product of both sides with $\vec{u_j}$ for some $j \in \{1, 2, \ldots, k\}$, we get:
    \[
        (c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}) \cdot \vec{u_j} = \vec{0} \cdot \vec{u_j} = 0
    \]
    Using the distributive property of the dot product, we have:
    \[
        c_1 (\vec{u_1} \cdot \vec{u_j}) + c_2 (\vec{u_2} \cdot \vec{u_j}) + \ldots + c_k (\vec{u_k} \cdot \vec{u_j}) = 0
    \]
    Since the family is orthogonal, $\vec{u_i} \cdot \vec{u_j} = 0$ for all $i \neq j$, and $\vec{u_j} \cdot \vec{u_j} = \|\vec{u_j}\|^2 > 0$ (since $\vec{u_j}$ is non-zero). Thus, the above equation simplifies to:
    \[
        c_j \|\vec{u_j}\|^2 = 0
    \]
    Since $\|\vec{u_j}\|^2 > 0$, it follows that $c_j = 0$.
    Since this holds for any $j$, we conclude that $c_1 = c_2 = \ldots = c_k = 0$.
    Therefore, the family $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ is linearly independent.
\end{proof}

\begin{definition}[Orthogonal Basis]
    An orthogonal basis of a subspace $W$ of $\mathbb{R}^n$ is a basis of $W$ that is also an orthogonal family.
\end{definition}

\begin{theorem}
    Let $W$ be a subspace of $\mathbb{R}^n$ and let $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ be an orthogonal basis of $W$. Then, for any vector $\vec{v} \in W$, the coordinates of $\vec{v}$ in this basis are given by:
    \[
        c_i = \frac{\vec{v} \cdot \vec{u_i}}{\vec{u_i} \cdot \vec{u_i}} \quad \text{for } i = 1, 2, \ldots, k
    \]
    such that:
    \[
        \vec{v} = c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}
    \]
\end{theorem}
\begin{proof}
    Since $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ is a basis of $W$, any vector $\vec{v} \in W$ can be expressed as:
    \[
        \vec{v} = c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}
    \]
    for some scalars $c_1, c_2, \ldots, c_k$.
    Taking the dot product of both sides with $\vec{u_j}$ for some $j \in \{1, 2, \ldots, k\}$, we get:
    \[
        \vec{v} \cdot \vec{u_j} = (c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}) \cdot \vec{u_j}
    \]
    Using the distributive property of the dot product, we have:
    \[
        \vec{v} \cdot \vec{u_j} = c_1 (\vec{u_1} \cdot \vec{u_j}) + c_2 (\vec{u_2} \cdot \vec{u_j}) + \ldots + c_k (\vec{u_k} \cdot \vec{u_j})
    \]
    Since the family is orthogonal, $\vec{u_i} \cdot \vec{u_j} = 0$ for all $i \neq j$, and $\vec{u_j} \cdot \vec{u_j} = \|\vec{u_j}\|^2$. Thus, the above equation simplifies to:
    \[
        \vec{v} \cdot \vec{u_j} = c_j (\vec{u_j} \cdot \vec{u_j})
    \]
    Therefore:
    \[
        c_j = \frac{\vec{v} \cdot \vec{u_j}}{\vec{u_j} \cdot \vec{u_j}}
    \]
    for each $j = 1, 2, \ldots, k$.
\end{proof}

\begin{eg}
    Let $W$ be the subspace of $\mathbb{R}^3$ spanned by the orthogonal basis:
    \[
        \vec{u_1} = \begin{bmatrix}
            1 \\ 0 \\ 0
        \end{bmatrix}, \quad \vec{u_2} = \begin{bmatrix}
            0 \\ 1 \\ 1
        \end{bmatrix}
    \]
    and let $\vec{v} = \begin{bmatrix}
        3 \\ 4 \\ 4
    \end{bmatrix} \in W$. To find the coordinates of $\vec{v}$ in this basis, we compute:
    \[
        c_1 = \frac{\vec{v} \cdot \vec{u_1}}{\vec{u_1} \cdot \vec{u_1}} = \frac{3 \cdot 1 + 4 \cdot 0 + 4 \cdot 0}{1^2 + 0^2 + 0^2} = \frac{3}{1} = 3
    \]
    \[
        c_2 = \frac{\vec{v} \cdot \vec{u_2}}{\vec{u_2} \cdot \vec{u_2}} = \frac{3 \cdot 0 + 4 \cdot 1 + 4 \cdot 1}{0^2 + 1^2 + 1^2} = \frac{8}{2} = 4
    \]
    Thus, the coordinates of $\vec{v}$ in the orthogonal basis $(\vec{u_1}, \vec{u_2})$ are $(3, 4)$, and we have:
    \[
        \vec{v} = 3\vec{u_1} + 4\vec{u_2} = 3\begin{bmatrix}
            1 \\ 0 \\ 0
        \end{bmatrix} + 4\begin{bmatrix}
            0 \\ 1 \\ 1
        \end{bmatrix} = \begin{bmatrix}
            3 \\ 4 \\ 4
        \end{bmatrix}
    \]
\end{eg}

\subsection{Orthogonal Projection}
\begin{definition}[Orthogonal Projection]
    Let $W$ be a subspace of $\mathbb{R}^n$ and let $\vec{v} \in \mathbb{R}^n$. The orthogonal projection of $\vec{v}$ onto $W$, denoted by $\text{proj}_W(\vec{v})$, is the vector in $W$ such that the vector $\vec{v} - \text{proj}_W(\vec{v})$ is orthogonal to $W$.
\end{definition}
Geometrically, the orthogonal projection of $\vec{y} \in \mathbb{R}^2$ onto the line $W$ spanned by the vector $\vec{w}$ is represented as follows:
\begin{center}
    \begin{tikzpicture}
        \draw[] (-3,-1.5) -- (3,1.5) node[very near start, below right] {$W = \text{span}(\vec{w})$};
        \draw[->, primary] (0,0) -- (2,1) node[midway, above] {$\vec{w}$};
        \draw[->, primary] (0,0) -- (-2,1) node[midway, above] {$\vec{y}$};

        \draw[dashed, secondary] (-6/5,-3/5) -- (-2,1);
        \draw[->, secondary, thick] (0,0) -- (-6/5,-3/5) node[midway, below right] {$\vec{\hat{y}} = \text{proj}_W(\vec{y})$};
    \end{tikzpicture}
\end{center}
Remark that:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item the vector $\vec{y} - \vec{\hat{y}}$ is orthogonal to $W$
    \item the vector $\vec{\hat{y}}$ is the orthogonal projection of $\vec{y}$ onto $W$, denoted by $\text{proj}_W(\vec{y})$
    \item the coordinates of $\vec{\hat{y}}$ are given by:
    \[
        \vec{\hat{y}} = \frac{\vec{y} \cdot \vec{w}}{\vec{w} \cdot \vec{w}} \vec{w}
    \]
\end{itemize}

\begin{eg}
    Let's take the same situation as above but in $\mathbb{R}^3$. We have the subspace $W$, a plane, spanned by the vectors $\vec{u_1}$ and $\vec{u_2}$. To find the coordinates of $\text{proj}_W(\vec{y})$ we can start with:
    \[
        \text{proj}_W(\vec{y}) = c_1 \vec{u_1} + c_2 \vec{u_2}
    \]
    and we also have:
    \[
        \begin{cases*}
            (\vec{y} - \text{proj}_W(\vec{y})) \cdot \vec{u_1} = 0 \\ 
            (\vec{y} - \text{proj}_W(\vec{y})) \cdot \vec{u_2} = 0
        \end{cases*} \quad \iff \quad \begin{cases*}
            \text{proj}_W(\vec{y}) \cdot \vec{u_1} = \vec{y} \cdot \vec{u_1} \\
            \text{proj}_W(\vec{y}) \cdot \vec{u_2} = \vec{y} \cdot \vec{u_2}
        \end{cases*}
    \]
    By substituting $\text{proj}_W(\vec{y})$ in the equations above, we get:
    \[
        \begin{cases*}
            (c_1 \vec{u_1} + c_2 \vec{u_2}) \cdot \vec{u_1} = \vec{y} \cdot \vec{u_1} \\
            (c_1 \vec{u_1} + c_2 \vec{u_2}) \cdot \vec{u_2} = \vec{y} \cdot \vec{u_2}
        \end{cases*}
    \]
    which simplifies to:
    \[
        \begin{cases*}
            c_1 (\vec{u_1} \cdot \vec{u_1}) + c_2 (\vec{u_2} \cdot \vec{u_1}) = \vec{y} \cdot \vec{u_1} \\
            c_1 (\vec{u_1} \cdot \vec{u_2}) + c_2 (\vec{u_2} \cdot \vec{u_2}) = \vec{y} \cdot \vec{u_2}
        \end{cases*}
    \]
    In matrix form, we have:
    \[
        \begin{bmatrix}
            \vec{u_1} \cdot \vec{u_1} & \vec{u_2} \cdot \vec{u_1} \\
            \vec{u_1} \cdot \vec{u_2} & \vec{u_2} \cdot \vec{u_2}
        \end{bmatrix} \begin{bmatrix}
            c_1 \\ c_2
        \end{bmatrix} = \begin{bmatrix}
            \vec{y} \cdot \vec{u_1} \\ \vec{y} \cdot \vec{u_2}
        \end{bmatrix}
    \]
    Therefore, by solving this system of equations, we can find the coefficients $c_1$ and $c_2$, and thus the orthogonal projection $\text{proj}_W(\vec{y})$.
\end{eg}

\begin{theorem}
    Let $W$ be a subspace of $\mathbb{R}^n$ and let $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ be an orthogonal basis of $W$. Then, for any vector $\vec{v} \in \mathbb{R}^n$, the orthogonal projection of $\vec{v}$ onto $W$ is given by:
    \[
        \text{proj}_W(\vec{v}) = \sum_{i=1}^{k} \frac{\vec{v} \cdot \vec{u_i}}{\vec{u_i} \cdot \vec{u_i}} \vec{u_i}
    \]
\end{theorem}
\begin{proof}
    Since $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ is a basis of $W$, any vector in $W$ can be expressed as:
    \[
        \text{proj}_W(\vec{v}) = c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}
    \]
    for some scalars $c_1, c_2, \ldots, c_k$.
    To find the coefficients $c_i$, we use the property that $\vec{v} - \text{proj}_W(\vec{v})$ is orthogonal to $W$. Thus, for each $j = 1, 2, \ldots, k$, we have:
    \[
        (\vec{v} - \text{proj}_W(\vec{v})) \cdot \vec{u_j} = 0
    \]
    which implies:
    \[
        \text{proj}_W(\vec{v}) \cdot \vec{u_j} = \vec{v} \cdot \vec{u_j}
    \]
    Substituting the expression for $\text{proj}_W(\vec{v})$, we get:
    \[
        (c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}) \cdot \vec{u_j} = \vec{v} \cdot \vec{u_j}
    \]
    Using the distributive property of the dot product, we have:
    \[
        c_1 (\vec{u_1} \cdot \vec{u_j}) + c_2 (\vec{u_2} \cdot \vec{u_j}) + \ldots + c_k (\vec{u_k} \cdot \vec{u_j}) = \vec{v} \cdot \vec{u_j}
    \]
    Since the family is orthogonal, $\vec{u_i} \cdot \vec{u_j} = 0$ for all $i \neq j$, and $\vec{u_j} \cdot \vec{u_j} = \|\vec{u_j}\|^2$. Thus, the above equation simplifies to:
    \[
        c_j (\vec{u_j} \cdot \vec{u_j}) = \vec{v} \cdot \vec{u_j}
    \]
    Therefore:
    \[
        c_j = \frac{\vec{v} \cdot \vec{u_j}}{\vec{u_j} \cdot \vec{u_j}}
    \]
    for each $j = 1, 2, \ldots, k$.
    Substituting these coefficients back into the expression for $\text{proj}_W(\vec{v})$, we get:
    \[
        \text{proj}_W(\vec{v}) = \sum_{i=1}^{k} \frac{\vec{v} \cdot \vec{u_i}}{\vec{u_i} \cdot \vec{u_i}} \vec{u_i}
    \]
\end{proof}
Remark the following:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item If $\vec{y}$ is in $W$, then $\text{proj}_W(\vec{y}) = \vec{y}$.
    \item If $\vec{y}$ is in $W^{\perp}$, then $\text{proj}_W(\vec{y}) = \vec{0}$.
    \item For $\vec{y} \in \mathbb{R}^n$ an arbitrary vector and $W$ a subspace with an orthogonal basis $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$, then $\text{proj}_{W^{\perp}}(\vec{y}) = \vec{y} - \text{proj}_W(\vec{y})$.
\end{itemize}

\begin{theorem}
    Let $W$ be a subspace of $\mathbb{R}^n$, $\vec{y}$ an arbitrary vector in $\mathbb{R}^n$ and $\vec{\hat{y}}$ the orthogonal projection of $\vec{y}$ onto $W$. Then, $\vec{\hat{y}}$ is the closest vector in $W$ to $\vec{y}$, i.e.:
    \[
        \|\vec{y} - \vec{\hat{y}}\| < \|\vec{y} - \vec{w}\| \quad \text{for all } \vec{w} \in W
    \]
\end{theorem}
\begin{proof}
    Let $\vec{w} \in W$. We can express $\vec{w}$ as:
    \[
        \vec{w} = \vec{\hat{y}} + \vec{z}
    \]
    for some vector $\vec{z} \in W$. Then:
    \[
        \|\vec{y} - \vec{w}\|^2 = \|\vec{y} - (\vec{\hat{y}} + \vec{z})\|^2 = \|(\vec{y} - \vec{\hat{y}}) - \vec{z}\|^2
    \]
    Expanding the right-hand side, we have:
    \[
        \|(\vec{y} - \vec{\hat{y}}) - \vec{z}\|^2 = \|\vec{y} - \vec{\hat{y}}\|^2 - 2(\vec{y} - \vec{\hat{y}}) \cdot \vec{z} + \|\vec{z}\|^2
    \]
    Since $\vec{y} - \vec{\hat{y}}$ is orthogonal to $W$ and $\vec{z} \in W$, we have:
    \[
        (\vec{y} - \vec{\hat{y}}) \cdot \vec{z} = 0
    \]
    Therefore:
    \[
        \|\vec{y} - \vec{w}\|^2 = \|\vec{y} - \vec{\hat{y}}\|^2 + \|\vec{z}\|^2
    \]
    Since $\|\vec{z}\|^2 > 0$ for all $\vec{z} \neq \vec{0}$, it follows that:
    \[
        \|\vec{y} - \vec{w}\|^2 > \|\vec{y} - \vec{\hat{y}}\|^2
    \]
    Taking the square root of both sides, we get:
    \[
        \|\vec{y} - \vec{w}\| > \|\vec{y} - \vec{\hat{y}}\|
    \]
    Thus, $\vec{\hat{y}}$ is the closest vector in $W$ to $\vec{y}$.
\end{proof}

\subsection{Orthonormal Basis}
\begin{definition}[Orthonormal Basis]
    An orthonormal basis of a subspace $W$ of $\mathbb{R}^n$ is a basis of $W$ that is an orthogonal family of unit vectors i.e.:
    \[
        \vec{u_i} \cdot \vec{u_j} = \begin{cases*}
            1 & if $i = j$ \\
            0 & if $i \neq j$
        \end{cases*}
    \]
\end{definition}
Remark that then, for an orthonormal basis, $\text{proj}_W(\vec{v})$ simplifies to:
\[
    \text{proj}_W(\vec{v}) = \sum_{i=1}^{k} (\vec{v} \cdot \vec{u_i}) \vec{u_i}
\]
Also note that the standard basis of $\mathbb{R}^n$ is an orthonormal basis.

\begin{theorem}
    The columns of a matrix $U \in \mathbb{R}^{m \times n}$ form an orthonormal basis of $\text{Im} U$ if and only if $U^T U = I_n$.
\end{theorem}
\begin{proof}
    Let $\mathcal{U} = (\vec{u_1}, \vec{u_2}, \ldots, \vec{u_n})$ be a basis of a subspace $W$ of $\mathbb{R}^m$. Then, the matrix $U \in \mathbb{R}^{m \times n}$ can be expressed as:
    \[
        U = \begin{bmatrix}
            | & | & & | \\
            \vec{u_1} & \vec{u_2} & \ldots & \vec{u_n} \\
            | & | & & |
        \end{bmatrix}
    \]
    and thus $U^T \in \mathbb{R}^{n \times m}$ is given by:
    \[
        U^T = \begin{bmatrix}
            - & (\vec{u_1})^T & - \\
            - & (\vec{u_2})^T & - \\
            & \vdots & \\
            - & (\vec{u_n})^T & -
        \end{bmatrix}
    \]
    Therefore, the product $U^T U$ is:
    \[
        U^T U = \begin{bmatrix}
            \vec{u_1} \cdot \vec{u_1} & \vec{u_1} \cdot \vec{u_2} & \ldots & \vec{u_1} \cdot \vec{u_n} \\
            \vec{u_2} \cdot \vec{u_1} & \vec{u_2} \cdot \vec{u_2} & \ldots & \vec{u_2} \cdot \vec{u_n} \\
            \vdots & \vdots & \ddots & \vdots \\
            \vec{u_n} \cdot \vec{u_1} & \vec{u_n} \cdot \vec{u_2} & \ldots & \vec{u_n} \cdot \vec{u_n}
        \end{bmatrix}
    \]
    Thus, if $\mathcal{U}$ is an orthonormal basis of $W$, then:
    \[
        U^T U = I_n
    \]
    Conversely, if $U^T U = I_n$, then:
    \[
        \vec{u_i} \cdot \vec{u_j} = \begin{cases*}
            1 & if $i = j$ \\
            0 & if $i \neq j$
        \end{cases*}
    \]
    which means that $\mathcal{U}$ is an orthonormal basis of $W$.
\end{proof}
Remark that if $\mathcal{U}$ is an orthogonal basis but not orthonormal, then $U^T U$ is a diagonal matrix where the diagonal entries are the squared norms of the basis vectors:
\[
    U^T U = \begin{bmatrix}
        \vec{u_1} \cdot \vec{u_1} & 0 & \ldots & 0 \\
        0 & \vec{u_2} \cdot \vec{u_2} & \ldots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \ldots & \vec{u_n} \cdot \vec{u_n}
    \end{bmatrix}
\]

\begin{eg}
    Let $\vec{u_1} = \begin{bmatrix}
        1 \\ 1 \\ 1 \\ 0
    \end{bmatrix}$, $\vec{u_2} = \begin{bmatrix}
        1 \\ 0 \\ -1 \\ 2
    \end{bmatrix}$ and $\vec{u_3} = \begin{bmatrix}
        1 \\ -1 \\ 0 \\ - \frac{1}{2}
    \end{bmatrix}$ be vectors in $\mathbb{R}^4$. Let's check if $\mathcal{U} = (\vec{u_1}, \vec{u_2}, \vec{u_3})$ forms an orthonormal or orthogonal basis. We start by constructing the matrix $U$:
    \[
        U = \begin{bmatrix}
            1 & 1 & 1 \\
            1 & 0 & -1 \\
            1 & -1 & 0 \\
            0 & 2 & -\frac{1}{2}
        \end{bmatrix}
    \]
    Then, we compute $U^T U$:
    \[
        U^T U = \begin{bmatrix}
            3 & 0 & 0 \\
            0 & 6 & 0 \\
            0 & 0 & \frac{9}{4}
        \end{bmatrix}
    \]
    Since $U^T U$ is not the identity matrix, the vectors $\vec{u_1}, \vec{u_2}, \vec{u_3}$ do not form an orthonormal basis. However, they do form an orthogonal basis since the off-diagonal entries are zero. \\
    For any vector $\vec{y} \in \text{span}\{u_1, u_2, u_3\}$, we can find its coordinates in this basis using:
    \[
        \vec{y} = c_1 \vec{u_1} + c_2 \vec{u_2} + c_3 \vec{u_3} = \begin{bmatrix}
            1 & 1 & 1 \\
            1 & 0 & -1 \\
            1 & -1 & 0 \\
            0 & 2 & -\frac{1}{2}
        \end{bmatrix} \begin{bmatrix}
            c_1 \\ c_2 \\ c_3
        \end{bmatrix} = U [\vec{y}]_{\mathcal{U}}
    \]
    We have:
    \[
        U^T \cdot U [\vec{y}]_{\mathcal{U}} = U^T \vec{y}
    \]
    Which implies:
    \[
        \begin{bmatrix}
            3 & 0 & 0 \\
            0 & 6 & 0 \\
            0 & 0 & \frac{9}{4}
        \end{bmatrix}
        \begin{bmatrix}
            c_1 \\ c_2 \\ c_3
        \end{bmatrix} = \begin{bmatrix}
            1 & 1 & 1 & 0 \\
            1 & 0 & -1 & 2 \\
            1 & -1 & 0 & -\frac{1}{2}
        \end{bmatrix} \vec{y}
    \]
    Thus:
    \[
        \begin{bmatrix}
            c_1 \\ c_2 \\ c_3
        \end{bmatrix} = \begin{bmatrix}
            \frac{1}{3} & 0 & 0 \\
            0 & \frac{1}{6} & 0 \\
            0 & 0 & \frac{4}{9}
        \end{bmatrix} \begin{bmatrix}
            \vec{u_1} \cdot \vec{y} \\
            \vec{u_2} \cdot \vec{y} \\
            \vec{u_3} \cdot \vec{y}
        \end{bmatrix}
    \]
\end{eg}

\begin{theorem}
    Let $U$ be a matrix whose columns form an orthonormal basis of a subspace $W$ of $\mathbb{R}^n$. Then, for any vectors $\vec{v}, \vec{w} \in \mathbb{R}^n$:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $||U\vec{v}|| = ||\vec{v}||$
        \item $(U\vec{v}) \cdot (U\vec{w}) = \vec{v} \cdot \vec{w}$
    \end{itemize}
\end{theorem}
\begin{proof}
    Let $\vec{v}, \vec{w} \in \mathbb{R}^n$. Since the columns of $U$ form an orthonormal basis of $W$, we have $U^T U = I_n$.
    For the first property:
    \[
        ||U\vec{v}||^2 = (U\vec{v}) \cdot (U\vec{v}) = (U\vec{v})^T (U\vec{v}) = \vec{v}^T (U^T U) \vec{v} = \vec{v}^T I_n \vec{v} = \vec{v}^T \vec{v} = ||\vec{v}||^2
    \]
    Taking the square root of both sides, we get:
    \[
        ||U\vec{v}|| = ||\vec{v}||
    \]
    For the second property:
    \[
        (U\vec{v}) \cdot (U\vec{w}) = (U\vec{v})^T (U\vec{w}) = \vec{v}^T (U^T U) \vec{w} = \vec{v}^T I_n \vec{w} = \vec{v}^T \vec{w} = \vec{v} \cdot \vec{w}
    \]
\end{proof}

\begin{theorem}
    If $(\vec{u_1}, \ldots, \vec{u_p})$ is a orthonormal basis of a subspace $W$ of $\mathbb{R}^n$, then:
    \[
        \text{proj}_W(\vec{v}) = (\vec{v} \cdot \vec{u_1}) \vec{u_1} + (\vec{v} \cdot \vec{u_2}) \vec{u_2} + \ldots + (\vec{v} \cdot \vec{u_p}) \vec{u_p}
    \]
    If $U$ is the matrix whose columns are the vectors of the orthonormal basis, then:
    \[
        \text{proj}_W(\vec{v}) = U U^T \vec{v}
    \]
\end{theorem}
\begin{proof}
    Let $(\vec{u_1}, \ldots, \vec{u_p})$ be an orthonormal basis of a subspace $W$ of $\mathbb{R}^n$. By the definition of orthonormal basis, we have:
    \[
        \text{proj}_W(\vec{v}) = \sum_{i=1}^{p} (\vec{v} \cdot \vec{u_i}) \vec{u_i}
    \]
    Now, let $U$ be the matrix whose columns are the vectors of the orthonormal basis:
    \[
        U = \begin{bmatrix}
            | & | & & | \\
            \vec{u_1} & \vec{u_2} & \ldots & \vec{u_p} \\
            | & | & & |
        \end{bmatrix}
    \]
    Then, the transpose of $U$ is:
    \[
        U^T = \begin{bmatrix}
            - & (\vec{u_1})^T & - \\
            - & (\vec{u_2})^T & - \\
            & \vdots & \\
            - & (\vec{u_p})^T & -
        \end{bmatrix}
    \]
    The product $U U^T$ is given by:
    \[
        U U^T = \sum_{i=1}^{p} \vec{u_i} (\vec{u_i})^T
    \]
    Therefore, for any vector $\vec{v} \in \mathbb{R}^n$, we have:
    \[
        U U^T \vec{v} = \sum_{i=1}^{p} \vec{u_i} (\vec{u_i})^T \vec{v} = \sum_{i=1}^{p} (\vec{v} \cdot \vec{u_i}) \vec{u_i} = \text{proj}_W(\vec{v})
    \]
\end{proof}
Remark that $U U^T = I_n$ holds but in general $U U^T \neq I_m$.

\begin{definition}[Orthogonal Matrix]
    A matrix $Q \in \mathbb{R}^{n \times n}$ is called an orthogonal matrix if its columns form an orthonormal basis of $\mathbb{R}^n$, i.e. if $Q^T Q = I_n$.
\end{definition}
Remark that $Q^{-1} = Q^T$ since $Q^T Q = I_n$.

\begin{theorem}
    If $Q \in \mathbb{R}^{n \times n}$ is an orthogonal matrix, then:
    \[
        Q^T Q = Q Q^T = I_n
    \]
\end{theorem}
\begin{proof}
    Since $Q$ is an orthogonal matrix, by definition, we have:
    \[
        Q^T Q = I_n
    \]
    To show that $Q Q^T = I_n$, let $V = Q^T$. Then:
    \[
        V^{-1} = (Q^T)^{-1} = (Q^{-1})^T = (Q^T)^T = V^T \quad \iff \quad V^T V = V^{-1} V = I_n
    \]
    Thus $V$ is also an orthogonal matrix, and thus it's columns form an orthonormal basis of $\mathbb{R}^n$. Therefore:
    \[
        Q Q^T = V^T V = I_n
    \]
\end{proof}

\section{Gram-Schmidt Process}
\begin{eg}
    Let $W = \text{span}\{\vec{x_1}, \vec{x_2}\}$ with $\vec{x_1} = \begin{bmatrix}
        3 \\ 6 \\ 0
    \end{bmatrix}$ and $\vec{x_2} = \begin{bmatrix}
        1 \\ 2 \\ 2
    \end{bmatrix}$. We clearly see that $\vec{x_1}$ and $\vec{x_2}$ are not linearly dependent and thus they form a basis of $W$. We want to transform this basis into an orthonormal basis. To do so, let's start by normalizing $\vec{x_1}$:
    \[
        \vec{u_1} = \frac{\vec{x_1}}{||\vec{x_1}||} = \frac{1}{\sqrt{3^2 + 6^2 + 0^2}} \begin{bmatrix}
            3 \\ 6 \\ 0
        \end{bmatrix} = \frac{1}{\sqrt{45}} \begin{bmatrix}
            3 \\ 6 \\ 0
        \end{bmatrix} = \begin{bmatrix}
            \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} \\ 0
        \end{bmatrix}
    \]
    Now, this vector generate a line through the origin. By a previous theorem, if we want to find a vector orthogonal to $\vec{x_1}$, we can project $\vec{x_2}$ onto the line spanned by $\vec{u_1}$ and subtract this projection from $\vec{x_2}$:
    \[
        \text{proj}_{\vec{u_1}}(\vec{x_2}) = (\vec{x_2} \cdot \vec{u_1}) \vec{u_1} = \left( \begin{bmatrix}
            1 \\ 2 \\ 2
        \end{bmatrix} \cdot \begin{bmatrix}
            \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} \\ 0
        \end{bmatrix} \right) \begin{bmatrix}
            \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} \\ 0
        \end{bmatrix} = \frac{5}{\sqrt{5}} \begin{bmatrix}
            \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} \\ 0
        \end{bmatrix} = \begin{bmatrix}
            1 \\ 2 \\ 0
        \end{bmatrix}
    \]
    Thus, we can find a vector orthogonal to $\vec{u_1}$ by computing:
    \[
        \vec{y} = \vec{x_2} - \text{proj}_{\vec{u_1}}(\vec{x_2}) = \begin{bmatrix}
            1 \\ 2 \\ 2
        \end{bmatrix} - \begin{bmatrix}
            1 \\ 2 \\ 0
        \end{bmatrix} = \begin{bmatrix}
            0 \\ 0 \\ 2
        \end{bmatrix}
    \]
    Finally, we normalize $\vec{y}$ to get $\vec{u_2}$:
    \[
        \vec{u_2} = \frac{\vec{y}}{||\vec{y}||} = \frac{1}{\sqrt{0^2 + 0^2 + 2^2}} \begin{bmatrix}
            0 \\ 0 \\ 2
        \end{bmatrix} = \frac{1}{2} \begin{bmatrix}
            0 \\ 0 \\ 2
        \end{bmatrix} = \begin{bmatrix}
            0 \\ 0 \\ 1
        \end{bmatrix}
    \]
    Therefore, the orthonormal basis of $W$ is given by:
    \[
        \mathcal{U} = \left( \begin{bmatrix}
            \frac{1}{\sqrt{5}} \\ \frac{2}{\sqrt{5}} \\ 0
        \end{bmatrix}, \begin{bmatrix}
            0 \\ 0 \\ 1
        \end{bmatrix} \right)
    \]
    Remark that we can view what we did in terms of matrix factorization. We have:
    \[
        \vec{x_1} = ||\vec{x_1}|| \vec{u_1}
    \]
    and:
    \[
        \vec{x_2} = \text{proj}_{\vec{u}_1}(\vec{x_2}) + \vec{y} = (\vec{x_2} \cdot \vec{u_1}) \vec{u_1} + \vec{y} = (\vec{x_2} \cdot \vec{u_1}) \vec{u_1} + ||\vec{y}|| \vec{u_2}
    \]
    Thus, we can write:
    \[
        \underbrace{\begin{bmatrix}
            | & | \\
            \vec{x_1} & \vec{x_2} \\
            | & |
        \end{bmatrix}}_{A} = \underbrace{\begin{bmatrix}
            | & | \\
            \vec{u_1} & \vec{u_2} \\
            | & |
        \end{bmatrix}}_{Q} \underbrace{\begin{bmatrix}
            ||\vec{x_1}|| & \vec{x_2} \cdot \vec{u_1} \\
            0 & ||\vec{y}||
        \end{bmatrix}}_{R}
    \]
\end{eg}
Note that if this example was in $\mathbb{R}^4$ with $\vec{x_1}, \vec{x_2}, \vec{x_3}$ and the orthonormal basis $\vec{u_1}, \vec{u_2}, \vec{u_3}$, then the following holds:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $\{\vec{u_1}\}$ is a basis of $\text{span}\{\vec{x_1}\}$
    \item $\{\vec{u_1}, \vec{u_2}\}$ is a basis of $\text{span}\{\vec{x_1}, \vec{x_2}\}$
    \item $\{\vec{u_1}, \vec{u_2}, \vec{u_3}\}$ is a basis of $\text{span}\{\vec{x_1}, \vec{x_2}, \vec{x_3}\}$
\end{itemize}
Remark that since the columns of $Q$ form an orthonormal basis of $\text{Im} A$, it follows that:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $Q^T Q = I_n$
    \item $\text{proj}_{\text{Im} A} (\vec{y}) = Q Q^T \vec{y}$
\end{itemize}

\begin{theorem}[Gram-Schmidt Process]
    Let $A = \begin{bmatrix}
        | & | & & | \\
        \vec{x_1} & \vec{x_2} & \ldots & \vec{x_n} \\
        | & | & & |
    \end{bmatrix}$ be a matrix in $\mathbb{R}^{m \times n}$ with linearly independent columns i.e. $(\vec{x_1}, \vec{x_2}, \ldots, \vec{x_n})$ is a basis of $W = \text{Im} A$. Then, there exists an orthonormal basis $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_n})$ of $W$ that can be constructed as follows:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Set $\vec{u_1} = \frac{\vec{x_1}}{||\vec{x_1}||}$
        \item For $k = 2, 3, \ldots, n$:
        \[
            \vec{y_k} = \vec{x_k} - \sum_{j=1}^{k-1} \text{proj}_{\vec{u_j}}(\vec{x_k}) = \vec{x_k} - \sum_{j=1}^{k-1} (\vec{x_k} \cdot \vec{u_j}) \vec{u_j}
        \]
        \[
            \vec{u_k} = \frac{\vec{y_k}}{||\vec{y_k}||}
        \]
    \end{itemize}
\end{theorem}
Remark that:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item If the vectors $\vec{x_1}, \vec{x_2}, \ldots, \vec{x_n}$ are orthogonal, then the Gram-Schmidt process simply normalizes each vector to obtain the orthonormal basis.
    \item If the vectors $\vec{x_1}, \vec{x_2}, \ldots, \vec{x_n}$ are linearly dependent, the Gram-Schmidt process can still be applied by ignoring the vectors that lead to $\vec{y_k} = \vec{0}$ or if their norme is zero.
\end{itemize}
Also remark that to solve $A \vec{x} = \vec{b}$ using the QR factorization obtained from the Gram-Schmidt process:
\[
    A \vec{x} = \vec{b} \quad \iff \quad Q R \vec{x} = \vec{b} \quad \iff \quad R \vec{x} = Q^T \vec{b}
\]
and since $R$ is triangular solving this system is easy using back substitution.

\subsection{QR and Determinant}
The $QR$ factorization can also be used to compute/understand the determinant of a square matrix. Indeed, if $A$ is a square matrix and $A = QR$ is its $QR$ factorization, then:
\[
    1 = \det(I_n) = \det(Q^T Q) = \det(Q^T) \det(Q) = (\det(Q))^2 \quad \implies \quad \det(Q) = \pm 1
\]
and thus:
\[
    \det(A) = \det(QR) = \det(Q) \det(R) = \pm \det(R)
\]
Since $R$ is an upper triangular matrix, its determinant is the product of its diagonal entries:
\[
    \det(R) = r_{11} r_{22} \ldots r_{nn} = ||\vec{x_1}|| \cdot ||\vec{y_2}|| \cdot \ldots \cdot ||\vec{y_n}||
\]
Therefore:
\[
    |\det(A)| = ||\vec{x_1}|| \cdot ||\vec{y_2}|| \cdot \ldots \cdot ||\vec{y_n}||
\]
Remark that the absolute value of the determinant of $A$ gives the volume of the "hyper" parallelepiped spanned by the columns of $A$. Thus, the product of the norms of the orthogonal vectors $\vec{x_1}, \vec{y_2}, \ldots, \vec{y_n}$ also gives this volume.

\section{Least Squares}
For a given matrix $A \in \mathbb{R}^{m \times n}$ and a vector $\vec{b} \in \mathbb{R}^m$, the equation $A \vec{x} = \vec{b}$ may not have a solution if $\vec{b} \notin \text{Im} A$. In such cases, one could look for an approximate solution that minimizes the error between $A \vec{x}$ and $\vec{b}$. Note that this relates to the orthogonal projection since the goal is to find the vector in $\text{Im} A$ that is closest to $\vec{b}$ i.e. the projection of $\vec{b}$ onto $\text{Im} A$.

\begin{theorem}
    Let $A \in \mathbb{R}^{m \times n}$ and $\vec{b} \in \mathbb{R}^m$. The least squares solution $\vec{\hat{x}}$ to the equation $A \vec{x} = \vec{b}$ is given by the solution of the normal equations:
    \[
        A^T A \vec{\hat{x}} = A^T \vec{b}
    \]
    Moreover, each solution $\vec{\hat{x}}$ of the normal equations has the property:
    \[
        ||A \vec{\hat{x}} - \vec{b}|| \leq ||A \vec{x} - \vec{b}|| \quad \text{for all } \vec{x} \in \mathbb{R}^n
    \]
\end{theorem}
\begin{proof}
    Let $A$ be a matrix of size $m \times n$ and $\vec{b}$ a vector in $\mathbb{R}^m$. Let's find $\vec{\hat{x}}$ such that $||A \vec{\hat{x}} - \vec{b}||$ is minimized. This is equivalent to finding the orthogonal projection of $\vec{b}$ onto $\text{Im} A$. Let $\vec{\hat{b}} = A \vec{\hat{x}}$ be this projection. Then, the error vector $\vec{e} = \vec{\hat{b}} - \vec{b}$ is orthogonal to $\text{Im} A$ (i.e. $e \in \ker(A^T)$), which means:
    \[
        A^T \vec{e} = 0 \quad \iff \quad A^T (\vec{\hat{b}} - \vec{b}) = 0 \quad \iff \quad A^T (A \vec{\hat{x}} - \vec{b}) = 0
    \]
    Rearranging this equation gives the normal equations:
    \[
        A^T A \vec{\hat{x}} = A^T \vec{b}
    \]
\end{proof}
Remark that the system of normal equations $A^T A \vec{\hat{x}} = A^T \vec{b}$ always has at least one solution.

\begin{theorem}
    The following statements are equivalent for a matrix $A \in \mathbb{R}^{m \times n}$:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The columns of $A$ are linearly independent.
        \item The matrix $A^T A$ is invertible.
        \item The equation $A \vec{x} = \vec{b}$ has a unique least squares solution for every $\vec{b} \in \mathbb{R}^m$.
    \end{itemize}
    In this case, the least squares solution is given by:
    \[
        \vec{\hat{x}} = (A^T A)^{-1} A^T \vec{b}
    \]
\end{theorem}
\begin{proof}
    We will prove the equivalence of the three statements in a cyclic manner. \\
    \textbf{(1) $\implies$ (2)}: If the columns of $A$ are linearly independent, then $\ker(A) = \{\vec{0}\}$. This implies that $A^T A$ is positive definite and thus invertible. \\
    \textbf{(2) $\implies$ (3)}: If $A^T A$ is invertible, then for any $\vec{b} \in \mathbb{R}^m$, the normal equations $A^T A \vec{\hat{x}} = A^T \vec{b}$ have a unique solution given by:
    \[
        \vec{\hat{x}} = (A^T A)^{-1} A^T \vec{b}
    \]
    Thus, the equation $A \vec{x} = \vec{b}$ has a unique least squares solution. \\
    \textbf{(3) $\implies$ (1)}: If the equation $A \vec{x} = \vec{b}$ has a unique least squares solution for every $\vec{b} \in \mathbb{R}^m$, then $\ker(A)$ must be trivial. If there were a non-zero vector $\vec{v} \in \ker(A)$, then for any least squares solution $\vec{\hat{x}}$, the vector $\vec{\hat{x}} + \vec{v}$ would also be a least squares solution, contradicting the uniqueness. Therefore, the columns of $A$ are linearly independent. \\
    Thus, all three statements are equivalent.
\end{proof}

\begin{eg}
    Let $\vec{b} = \begin{bmatrix}
        11 \\ 17 \\ 6
    \end{bmatrix}$ and $A = \begin{bmatrix}
        3 & 1 \\ 6 & 2 \\ 0 & 2
    \end{bmatrix}$. We want to find the least squares solution $\vec{\hat{x}}$ to the equation $A \vec{x} = \vec{b}$. First, we compute $A^T A$ and $A^T \vec{b}$:
    \[
        A^T A = \begin{bmatrix}
            3 & 6 & 0 \\
            1 & 2 & 2
        \end{bmatrix} \begin{bmatrix}
            3 & 1 \\ 6 & 2 \\ 0 & 2
        \end{bmatrix} = \begin{bmatrix}
            45 & 15 \\ 15 & 9
        \end{bmatrix}
    \]
    Since the columns of $A$ are linearly independent, $A^T A$ is invertible. Now, we compute $A^T \vec{b}$:
    \[
        A^T \vec{b} = \begin{bmatrix}
            3 & 6 & 0 \\
            1 & 2 & 2
        \end{bmatrix} \begin{bmatrix}
            11 \\ 17 \\ 6
        \end{bmatrix} = \begin{bmatrix}
            135 \\ 57
        \end{bmatrix}
    \]
    Now, we can solve the normal equations $A^T A \vec{\hat{x}} = A^T \vec{b}$:
    \[
        \begin{bmatrix}
            45 & 15 \\ 15 & 9
        \end{bmatrix} \vec{\hat{x}} = \begin{bmatrix}
            135 \\ 57
        \end{bmatrix}
    \]
    Thus, the unique least squares solution is:
    \[
        \vec{\hat{x}} = \begin{bmatrix}
            2 \\ 3
        \end{bmatrix}
    \]
    We can compute the error vector:
    \[
        \vec{e} = A \vec{\hat{x}} - \vec{b} = \begin{bmatrix}
            3 & 1 \\ 6 & 2 \\ 0 & 2
        \end{bmatrix} \begin{bmatrix}
            2 \\ 3
        \end{bmatrix} - \begin{bmatrix}
            11 \\ 17 \\ 6
        \end{bmatrix} = \begin{bmatrix}
            9 \\ 18 \\ 6
        \end{bmatrix} - \begin{bmatrix}
            11 \\ 17 \\ 6
        \end{bmatrix} = \begin{bmatrix}
            -2 \\ 1 \\ 0
        \end{bmatrix}
    \]
\end{eg}

\subsection{Least Squares and QR Factorization}

\begin{theorem}
    If the columns of $A \in \mathbb{R}^{m \times n}$ are linearly independent then for any $\vec{b} \in \mathbb{R}^m$, the least squares solution $\vec{\hat{x}}$ to the equation $A \vec{x} = \vec{b}$ is given by the solution of the triangular system:
    \[
        R \vec{\hat{x}} = Q^T \vec{b}
    \]
    where $A = QR$ is the QR factorization of $A$.
\end{theorem}
\begin{proof}
    Let $A = QR$ be the QR factorization of $A$, where $Q \in \mathbb{R}^{m \times n}$ has orthonormal columns and $R \in \mathbb{R}^{n \times n}$ is an upper triangular matrix. The normal equations for the least squares solution are given by:
    \[
        A^T A \vec{\hat{x}} = A^T \vec{b}
    \]
    Substituting $A = QR$ into the normal equations, we have:
    \[
        (QR)^T (QR) \vec{\hat{x}} = (QR)^T \vec{b}
    \]
    Simplifying this expression:
    \[
        R^T Q^T Q R \vec{\hat{x}} = R^T Q^T \vec{b}
    \]
    Since the columns of $Q$ are orthonormal, we have $Q^T Q = I_n$. Thus, the equation reduces to:
    \[
        R^T R \vec{\hat{x}} = R^T Q^T \vec{b}
    \]
    Multiplying both sides by $(R^T)^{-1}$ (which exists since the columns of $A$ are linearly independent and thus $R$ is invertible), we obtain:
    \[
        R \vec{\hat{x}} = Q^T \vec{b}
    \]
    Therefore, the least squares solution $\vec{\hat{x}}$ can be found by solving the triangular system:
    \[
        R \vec{\hat{x}} = Q^T \vec{b}
    \]
\end{proof}

\begin{eg}
    Let's take the same example as before with $\vec{b} = \begin{bmatrix}
        11 \\ 17 \\ 6
    \end{bmatrix}$ and $A = \begin{bmatrix}
        3 & 1 \\ 6 & 2 \\ 0 & 2
    \end{bmatrix}$. The QR factorization of $A$ is given by:
    \[
        Q = \begin{bmatrix}
            \frac{1}{\sqrt{5}} & 0 \\
            \frac{2}{\sqrt{5}} & 0 \\
            0 & 1
        \end{bmatrix}, \quad R = \begin{bmatrix}
            3 \sqrt{5} & \sqrt{5} \\
            0 & 2
        \end{bmatrix}
    \]
    Now, we compute $Q^T \vec{b}$:
    \[
        Q^T \vec{b} = \begin{bmatrix}
            \frac{1}{\sqrt{5}} & \frac{2}{\sqrt{5}} & 0 \\
            0 & 0 & 1
        \end{bmatrix} \begin{bmatrix}
            11 \\ 17 \\ 6
        \end{bmatrix} = \begin{bmatrix}
            9 \sqrt{5} \\ 6
        \end{bmatrix}
    \]
    Now, we solve the triangular system $R \vec{\hat{x}} = Q^T \vec{b}$:
    \[
        \begin{bmatrix}
            3 \sqrt{5} & \sqrt{5} \\
            0 & 2
        \end{bmatrix} \vec{\hat{x}} = \begin{bmatrix}
            \frac{45}{\sqrt{5}} \\ 6
        \end{bmatrix}
    \]
    Solving this system, we find the unique least squares solution:
    \[
        \vec{\hat{x}} = \begin{bmatrix}
            2 \\ 3
        \end{bmatrix}
    \]
\end{eg}

\subsection{Linear Regression}
Mathematically, linear regression can be formulated as finding the best-fitting line (or hyperplane in higher dimensions) to a set of data points. Given a set of data points $(x_1, y_1), (x_2, y_2), \ldots, \\ (x_m, y_m)$, the goal is to find a linear function of the form:
\[    y = \beta_1 x + \beta_0
\]
that minimizes the sum of the squared differences between the observed values $y_i$ and the values predicted by the linear function i.e. for each data point $(x_i, y_i)$:
\[
    \beta_1 x_i + \beta_0 \approx y_i
\]
This can be expressed in matrix form as:
\[
    A \vec{\beta} \approx \vec{y}
\]
where:
\[
    A = \begin{bmatrix}
        1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_m
    \end{bmatrix}, \quad \vec{\beta} = \begin{bmatrix}
        \beta_0 \\ \beta_1
    \end{bmatrix}, \quad \vec{y} = \begin{bmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_m
    \end{bmatrix}
\]
To find the best-fitting line, we can use the least squares method to solve for $\vec{\beta}$ by minimizing the error $||A \vec{\beta} - \vec{y}||$. The least squares solution is given by the normal equations:
\[
    A^T A \vec{\beta} = A^T \vec{y}
\]
Remark that a solution always exists (property of the normal equations) and is unique if the columns of $A$ are linearly independent (i.e. if there are at least two distinct $x_i$ values).