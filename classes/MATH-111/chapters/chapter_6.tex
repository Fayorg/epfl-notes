\chapter{Orthogonality and the least squares method}
Let's consider the following system of linear equations:
\[
    A \vec{x} = \vec{b}
\]
with $A \in \mathbb{R}^{m \times n}$, $\vec{b} \in \mathbb{R}^n$ and $\vec{x} \in \mathbb{R}^n$. If this system has no solution, there might be $\vec{x}$ such that:
\[
    A \vec{x} \approx \vec{b}
\]
Thus new concepts, such as size and distance, are needed to find an approximate solution.

\section{Dot Product}
\begin{definition}[Dot Product]
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$. The dot product of $\vec{u}$ and $\vec{v}$ is defined as:
    \[
        \vec{u} \cdot \vec{v} = \vec{u}^T \vec{v} = \begin{bmatrix}
            u_1 & u_2 & \ldots & u_n
        \end{bmatrix} \begin{bmatrix}
            v_1 \\ v_2 \\ \vdots \\ v_n
        \end{bmatrix} = u_1 v_1 + u_2 v_2 + \ldots + u_n v_n
    \]
\end{definition}
Remark that $\vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u}$, i.e., the dot product is commutative.

\begin{eg}
    Let $\vec{u} = \begin{bmatrix}
        1 \\ 2 \\ 3
    \end{bmatrix}$ and $\vec{v} = \begin{bmatrix}
        4 \\ 5 \\ 6
    \end{bmatrix}$. Then:
    \[
        \vec{u} \cdot \vec{v} = 1 \cdot 4 + 2 \cdot 5 + 3 \cdot 6 = 32
    \]
\end{eg}

\begin{theorem}
    Let $\vec{u}, \vec{v}$ and $\vec{w}$ be vectors in $\mathbb{R}^n$ and $c \in \mathbb{R}$. The following properties hold:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $\vec{u} \cdot \vec{u} \geq 0$ and $\vec{u} \cdot \vec{u} = 0$ if and only if $\vec{u} = \vec{0}$ (positivity).
        \item $\vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u}$ (commutativity).
        \item $(\vec{u} + \vec{v}) \cdot \vec{w} = \vec{u} \cdot \vec{w} + \vec{v} \cdot \vec{w}$ (distributivity).
        \item $(c \vec{u}) \cdot \vec{v} = c (\vec{u} \cdot \vec{v})$ (homogeneity).
    \end{itemize}
\end{theorem}

\begin{eg}
    Let $\vec{u_1}, \vec{u_2}, \vec{u_3}$ and $\vec{w}$ be vectors in $\mathbb{R}^n$. Then:
    \[
        (7\vec{u_1} - 2 \vec{u_2} + 4\vec{u_3}) \cdot \vec{w} = 7(\vec{u_1} \cdot \vec{w}) - 2(\vec{u_2} \cdot \vec{w}) + 4(\vec{u_3} \cdot \vec{w})
    \]
\end{eg}

\subsection{Norm}
\begin{definition}[Norm]
    Let $\vec{u} \in \mathbb{R}^n$. The norm (or length) of $\vec{u}$ is defined as:
    \[
        \|\vec{u}\| = \sqrt{\vec{u} \cdot \vec{u}} = \sqrt{u_1^2 + u_2^2 + \ldots + u_n^2}
    \]
\end{definition}
Note that since $\vec{u} \cdot \vec{u} \geq 0$, the norm is always defined.

\begin{eg}
    Let $\vec{u} = \begin{bmatrix}
        3 \\ -4
    \end{bmatrix}$. Then:
    \[
        \|\vec{u}\| = \sqrt{3^2 + (-4)^2} = \sqrt{9 + 16} = \sqrt{25} = 5
    \]
    We then have:
    \[
        \|5 \vec{u}\| = 5 \|\vec{u}\| = 5 \times 5 = 25
    \]
    and \[
        \|-\vec{u}\| = \| \vec{u} \| = 5
    \]
\end{eg}

\begin{definition}[Unit Vector]
    A unit vector is a vector $\vec{u}$ such that $\|\vec{u}\| = 1$.
\end{definition}
Remark that any non-zero vector $\vec{u}$ can be converted into a unit vector by dividing it by its norm:
\[
    \frac{\vec{u}}{\|\vec{u}\|}
\]
This process is called normalizing the vector $\vec{u}$.

\begin{eg}
    Let $\vec{u} = \begin{bmatrix}
        3 \\ 4
    \end{bmatrix}$. Then the unit vector in the direction of $\vec{u}$ is:
    \[
        \frac{\vec{u}}{\|\vec{u}\|} = \frac{1}{5} \begin{bmatrix}
            3 \\ 4
        \end{bmatrix} = \begin{bmatrix}
            \frac{3}{5} \\ \frac{4}{5}
        \end{bmatrix}
    \]
\end{eg}

\subsection{Distance}
\begin{definition}[Distance]
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$. The distance between $\vec{u}$ and $\vec{v}$ is defined as:
    \[
        d(\vec{u}, \vec{v}) = \|\vec{u} - \vec{v}\|
    \]
\end{definition}
Remark that the distance is symmetric, i.e., $d(\vec{u}, \vec{v}) = d(\vec{v}, \vec{u})$. Graphically:
\begin{center}
    \begin{tikzpicture}
        \draw[->, primary] (0,0) -- (4,2) node[midway, above] {$\vec{u}$};
        \draw[->, primary] (0,0) -- (1,3) node[midway, left] {$\vec{v}$};
        \draw[->, dashed, secondary, thick] (4,2) -- (1,3) node[midway, above right] {$\vec{u} - \vec{v}$};
        % \draw[fill] (4,2) circle [radius=0.05];
        % \draw[fill] (1,3) circle [radius=0.05];
    \end{tikzpicture}
\end{center}

\begin{eg}
    Let $\vec{u} = \begin{bmatrix}
        2 \\ 3
    \end{bmatrix}$ and $\vec{v} = \begin{bmatrix}
        5 \\ 7
    \end{bmatrix}$. Then the distance between $\vec{u}$ and $\vec{v}$ is:
    \[
        d(\vec{u}, \vec{v}) = \|\vec{u} - \vec{v}\| = \left\| \begin{bmatrix}
            2 - 5 \\ 3 - 7
        \end{bmatrix} \right\| = \left\| \begin{bmatrix}
            -3 \\ -4
        \end{bmatrix} \right\| = \sqrt{(-3)^2 + (-4)^2} = \sqrt{9 + 16} = \sqrt{25} = 5
    \]
\end{eg}

\section{Orthogonality}
\begin{definition}[Orthogonal Vectors]
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$. The vectors $\vec{u}$ and $\vec{v}$ are said to be orthogonal if:
    \[
        \vec{u} \cdot \vec{v} = 0
    \]
\end{definition}

\begin{eg}
    Let $\vec{u} = \begin{bmatrix}
        1 \\ 2 \\ -2
    \end{bmatrix}$ and $\vec{v} = \begin{bmatrix}
        2 \\ -1 \\ 1
    \end{bmatrix}$. Then:
    \[
        \vec{u} \cdot \vec{v} = 1 \cdot 2 + 2 \cdot (-1) + (-2) \cdot 1 = 2 - 2 - 2 = -2
    \]
    Thus, $\vec{u}$ and $\vec{v}$ are not orthogonal.
\end{eg}

\begin{theorem}
    Let $\vec{u}$ and $\vec{v}$ be orthogonal vectors in $\mathbb{R}^n$ if and only if:
    \[
        \|\vec{u} + \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2
    \]
\end{theorem}
\begin{proof}
    We have:
    \begin{align*}
        \|\vec{u} + \vec{v}\|^2 &= (\vec{u} + \vec{v}) \cdot (\vec{u} + \vec{v}) \\
        &= \vec{u} \cdot \vec{u} + 2 \vec{u} \cdot \vec{v} + \vec{v} \cdot \vec{v} \\
        &= \|\vec{u}\|^2 + 2 \vec{u} \cdot \vec{v} + \|\vec{v}\|^2
    \end{align*}
    Thus, if $\vec{u}$ and $\vec{v}$ are orthogonal, then $\vec{u} \cdot \vec{v} = 0$ and:
    \[
        \|\vec{u} + \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2
    \]
    Conversely, if $\|\vec{u} + \vec{v}\|^2 = \|\vec{u}\|^2 + \|\vec{v}\|^2$, then:
    \[
        0 = 2 \vec{u} \cdot \vec{v}
    \]
    which implies that $\vec{u} \cdot \vec{v} = 0$, i.e., $\vec{u}$ and $\vec{v}$ are orthogonal.
\end{proof}
Remark that when multiplying a vector and a matrix, the result is multiple dot product between the rows of the matrix and the vector. Thus, if all rows of a matrix are orthogonal to a vector, then the result of the multiplication is the zero vector.

\begin{eg}
    Let $\vec{v_1} = \begin{bmatrix}
        7 \\ 3 \\ 1
    \end{bmatrix}, \vec{v_2} = \begin{bmatrix}
        2 \\ 4 \\ 5
    \end{bmatrix}$ and $\vec{x} = \begin{bmatrix}
        9 \\ 10 \\ 8
    \end{bmatrix}$. Then:
    \[
        \begin{bmatrix}
            7 & 3 & 1 \\
            2 & 4 & 5
        \end{bmatrix} \begin{bmatrix}
            9 \\ 10 \\ 8
        \end{bmatrix} = \begin{bmatrix}
            \vec{v_1} \cdot \vec{x} \\
            \vec{v_2} \cdot \vec{x}
        \end{bmatrix} = \begin{bmatrix}
            7 \cdot 9 + 3 \cdot 10 + 1 \cdot 8 \\
            2 \cdot 9 + 4 \cdot 10 + 5 \cdot 8
        \end{bmatrix} = \begin{bmatrix}
            63 + 30 + 8 \\
            18 + 40 + 40
        \end{bmatrix} = \begin{bmatrix}
            101 \\ 98
        \end{bmatrix}
    \]
    Therefore, $\vec{x}$ is not orthogonal to the rows of the matrix.
\end{eg}

\subsection{Angle between Vectors}
\begin{theorem}
    Let $\vec{u}, \vec{v} \in \mathbb{R}^n$ be non-zero vectors. The angle $\theta$ between $\vec{u}$ and $\vec{v}$ is given by:
    \[
        \cos(\theta) = \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\| \|\vec{v}\|}
    \]
\end{theorem}
\begin{proof}
    From the law of cosines, we have:
    \[
        a^2 + b^2 - 2ab \cos(\theta) = c^2
    \]
    where $a = \|\vec{u}\|$, $b = \|\vec{v}\|$ and $c = \|\vec{u} - \vec{v}\|$. Thus:
    \[
        \|\vec{u}\|^2 + \|\vec{v}\|^2 - 2 \|\vec{u}\| \|\vec{v}\| \cos(\theta) = \|\vec{u} - \vec{v}\|^2
    \]
    But:
    \begin{align*}
        \|\vec{u} - \vec{v}\|^2 &= (\vec{u} - \vec{v}) \cdot (\vec{u} - \vec{v}) \\
        &= \vec{u} \cdot \vec{u} - 2 \vec{u} \cdot \vec{v} + \vec{v} \cdot \vec{v} \\
        &= \|\vec{u}\|^2 - 2 \vec{u} \cdot \vec{v} + \|\vec{v}\|^2
    \end{align*}
    Thus:
    \[
        \|\vec{u}\|^2 + \|\vec{v}\|^2 - 2 \|\vec{u}\| \|\vec{v}\| \cos(\theta) = \|\vec{u}\|^2 - 2 \vec{u} \cdot \vec{v} + \|\vec{v}\|^2
    \]
    which implies that:
    \[
        \vec{u} \cdot \vec{v} = \|\vec{u}\| \|\vec{v}\| \cos(\theta)
    \]
    Therefore:
    \[
        \cos(\theta) = \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\| \|\vec{v}\|}
    \]
\end{proof}

\subsection{Orthogonality to a Subspace}
Remark that if $\vec{u}$ is orthogonal to $\vec{v}$, then $\vec{u}$ is also orthogonal to any vector in $\text{span}\{\vec{v}\}$ i.e., for any vector $\vec{w} = c \vec{v}$ for some scalar $c \in \mathbb{R}$:
\[
    \vec{u} \cdot \vec{w} = \vec{u} \cdot (c \vec{v}) = c (\vec{u} \cdot \vec{v}) = c \cdot 0 = 0
\]

\begin{definition}[Orthogonal Complement to a Subspace]
    Let $W$ be a subspace of $\mathbb{R}^n$. The orthogonal complement to $W$, denoted by $W^{\perp}$, is defined as:
    \[
        W^{\perp} = \{ \vec{u} \in \mathbb{R}^n \mid \vec{u} \cdot \vec{w} = 0, \forall \vec{w} \in W \}
    \]
\end{definition}
Graphically, if $W$ is a line in $\mathbb{R}^3$, then $W^{\perp}$ is the plane orthogonal to that line passing through the origin:
\begin{center}
    \begin{tikzpicture}[scale=2.5]
        % Axes
        \draw[->] (-0.8,0,0) -- (1.2,0,0) node[right] {$x_1$};
        \draw[->] (0,-1.0,0) -- (0,1.2,0) node[above] {$x_2$};
        \draw[->] (0,0,0.8) -- (0,0,-1.2) node[below right] {$x_3$};

        % Plane WâŠ¥ (contains x_3 and makes -45 deg with x_1)
        \filldraw[fill=secondary!40, draw=secondary, opacity=0.3] 
            (0.7,-0.7,-0.8) -- (0.7,-0.7,0.8) -- (-0.7,0.7,0.8) -- (-0.7,0.7,-0.8) -- cycle;
        \node[secondary, thick, left] at (-0.7,0.7,0.8) {$W^{\perp}$};
        
        % Line W (orthogonal to the plane, i.e. along x_2 = x_1)
        \draw[->, thick, primary] (0,0,0) -- (0.8,0.8,0) node[right] {$W$};
        \draw[dashed, thick, primary] (0,0,0) -- (-0.3,-0.3,0);
        \draw[thick, primary] (-0.3,-0.3,0) -- (-0.6,-0.6,0);
    \end{tikzpicture}
\end{center}

\begin{theorem}
    $W^{\perp}$ is a subspace of $\mathbb{R}^n$.
\end{theorem}
\begin{proof}
    Let $\vec{u_1}, \vec{u_2} \in W^{\perp}$ and $c \in \mathbb{R}$. We need to show that $\vec{u_1} + \vec{u_2} \in W^{\perp}$ and $c \vec{u_1} \in W^{\perp}$.
    For any $\vec{w} \in W$, we have:
    \[
        (\vec{u_1} + \vec{u_2}) \cdot \vec{w} = \vec{u_1} \cdot \vec{w} + \vec{u_2} \cdot \vec{w} = 0 + 0 = 0
    \]
    Thus, $\vec{u_1} + \vec{u_2} \in W^{\perp}$.
    Similarly, for any $\vec{w} \in W$, we have:
    \[
        (c \vec{u_1}) \cdot \vec{w} = c (\vec{u_1} \cdot \vec{w}) = c \cdot 0 = 0
    \]
    Thus, $c \vec{u_1} \in W^{\perp}$.
    Therefore, $W^{\perp}$ is a subspace of $\mathbb{R}^n$.
\end{proof}

\begin{theorem}
    $(W^{\perp})^{\perp} = W$
\end{theorem}
\begin{proof}
    We need to show that $W \subseteq (W^{\perp})^{\perp}$ and $(W^{\perp})^{\perp} \subseteq W$.
    Let $\vec{w} \in W$. For any $\vec{u} \in W^{\perp}$, we have:
    \[
        \vec{w} \cdot \vec{u} = 0
    \]
    Thus, $\vec{w} \in (W^{\perp})^{\perp}$, which implies that $W \subseteq (W^{\perp})^{\perp}$.
    Conversely, let $\vec{v} \in (W^{\perp})^{\perp}$. For any $\vec{w} \in W$, we have:
    \[
        \vec{v} \cdot \vec{w} = 0
    \]
    Thus, $\vec{v} \in W$, which implies that $(W^{\perp})^{\perp} \subseteq W$.
    Therefore, $(W^{\perp})^{\perp} = W$.
\end{proof}

\begin{theorem}
    If $W = \text{span}(\vec{w_1}, \vec{w_2}, \ldots, \vec{w_k})$, then $\vec{u} \in W^{\perp}$ if and only if $\vec{u} \cdot \vec{w_i} = 0$ for all $i = 1, 2, \ldots, k$.
\end{theorem}
\begin{proof}
    \textbf{($\Rightarrow$)} Let $\vec{u} \in W^{\perp}$. Then, for any $\vec{w} \in W$, we have $\vec{u} \cdot \vec{w} = 0$. In particular, for each basis vector $\vec{w_i}$, we have $\vec{u} \cdot \vec{w_i} = 0$. \\
    \textbf{($\Leftarrow$)} Let $\vec{u}$ be such that $\vec{u} \cdot \vec{w_i} = 0$ for all $i = 1, 2, \ldots, k$. Any vector $\vec{w} \in W$ can be expressed as a linear combination of the basis vectors:
    \[
        \vec{w} = c_1 \vec{w_1} + c_2 \vec{w_2} + \ldots + c_k \vec{w_k}
    \]
    for some scalars $c_1, c_2, \ldots, c_k$. Then:
    \[
        \vec{u} \cdot \vec{w} = \vec{u} \cdot (c_1 \vec{w_1} + c_2 \vec{w_2} + \ldots + c_k \vec{w_k}) = c_1 (\vec{u} \cdot \vec{w_1}) + c_2 (\vec{u} \cdot \vec{w_2}) + \ldots + c_k (\vec{u} \cdot \vec{w_k}) = 0
    \]
    Thus, $\vec{u} \in W^{\perp}$.
\end{proof}

\begin{theorem}
    $(\text{Im} A^T)^{\perp} = \text{Ker} A$
\end{theorem}
\begin{proof}
    We need to show that $(\text{Im} A^T)^{\perp} \subseteq \text{Ker} A$ and $\text{Ker} A \subseteq (\text{Im} A^T)^{\perp}$.
    Let $\vec{x} \in \text{Ker A}$ and let $\vec{w} \in \text{Im} A^T$. Then, there exists $\vec{y}$ such that $\vec{w} = A^T \vec{y}$. Thus:
    \[
        \vec{x} \cdot \vec{w} = \vec{x} \cdot (A^T \vec{y}) = (A \vec{x}) \cdot \vec{y} = \vec{0} \cdot \vec{y} = 0
    \]
    Therefore, $\vec{x} \in (\text{Im} A^T)^{\perp}$, which implies that $\text{Ker} A \subseteq (\text{Im} A^T)^{\perp}$.
    Conversely, let $\vec{x} \in (\text{Im} A^T)^{\perp}$. For any $\vec{y}$, we have:
    \[
        \vec{x} \cdot (A^T \vec{y}) = 0
    \]
    which implies that:
    \[
        (A \vec{x}) \cdot \vec{y} = 0
    \]
    for all $\vec{y}$. Thus, $A \vec{x} = \vec{0}$, which means that $\vec{x} \in \text{Ker} A$. Therefore, $(\text{Im} A^T)^{\perp} \subseteq \text{Ker} A$.
\end{proof}
Remark that from this theorem, the following corollary holds:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $\text{Im} A = (\text{Ker} A^T)^{\perp}$
    \item $\text{Ker} A^T = (\text{Im} A)^{\perp}$
    \item $\text{Im} A^T = (\text{Ker} A)^{\perp}$
    \item $\text{Ker} A = (\text{Im} A^T)^{\perp}$
\end{itemize}

\begin{eg}
    Let $W$ be a subspace of $\mathbb{R}^n$ and let $(\vec{w_1}, \ldots, \vec{w_p})$ a basis of $W$. To find a basis of $W^{\perp}$, we can use the theorem above to get:
    \[
        W = \text{Im} A \quad \iff \quad W^{\perp} = \left(\text{Im} A\right)^{\perp} = \text{Ker} A^T
    \]
    where:
    \[
        A^T = \begin{bmatrix}
            - & (\vec{w_1})^T & - \\
            - & (\vec{w_2})^T & - \\
            & \vdots & \\
            - & (\vec{w_p})^T & -   
        \end{bmatrix}
    \]
    Therefore, a basis of $W^{\perp}$ can be found by finding the kernel of $A^T$.
\end{eg}

\subsection{Orthogonal Family}
\begin{definition}[Orthogonal Family]
    A family of vectors $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ in $\mathbb{R}^n$ is said to be orthogonal if:
    \[
        \vec{u_i} \cdot \vec{u_j} = 0 \quad \text{for all } i \neq j
    \]
\end{definition}
Note that since the null vector is orthogonal to any vector, an orthogonal family can contain the null vector.

\begin{eg}
    Let $\vec{v_1} = \begin{bmatrix}
        2 \\ 1
    \end{bmatrix}$ and $\vec{v_2} = \begin{bmatrix}
        2 \\ -4
    \end{bmatrix}$ in $\mathbb{R}^2$. Then:
    \[
        \vec{v_1} \cdot \vec{v_2} = 2 \cdot 2 + 1 \cdot (-4) = 4 - 4 = 0
    \]
    Thus, $(\vec{v_1}, \vec{v_2})$ is an orthogonal family.
\end{eg}

\begin{eg}
    Let $\vec{v_1} = \begin{bmatrix}
        1 \\ 0 \\ 0
    \end{bmatrix}, \vec{v_2} = \begin{bmatrix}
        0 \\ 1 \\ 0
    \end{bmatrix}$ and $\vec{v_3} = \begin{bmatrix}
        0 \\ 0 \\ 1
    \end{bmatrix}$ in $\mathbb{R}^3$. Then:
    \[
        \vec{v_1} \cdot \vec{v_2} = 1 \cdot 0 + 0 \cdot 1 + 0 \cdot 0 = 0
    \]
    \[
        \vec{v_1} \cdot \vec{v_3} = 1 \cdot 0 + 0 \cdot 0 + 0 \cdot 1 = 0
    \]
    \[
        \vec{v_2} \cdot \vec{v_3} = 0 \cdot 0 + 1 \cdot 0 + 0 \cdot 1 = 0
    \]
    Thus, $(\vec{v_1}, \vec{v_2}, \vec{v_3})$ is an orthogonal family.
\end{eg}

\begin{theorem}
    An orthogonal family of non-zero vectors is linearly independent.
\end{theorem}
\begin{proof}
    Let $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ be an orthogonal family of non-zero vectors in $\mathbb{R}^n$. We need to show that if:
    \[
        c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k} = \vec{0}
    \]
    for some scalars $c_1, c_2, \ldots, c_k$, then $c_1 = c_2 = \ldots = c_k = 0$.
    Taking the dot product of both sides with $\vec{u_j}$ for some $j \in \{1, 2, \ldots, k\}$, we get:
    \[
        (c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}) \cdot \vec{u_j} = \vec{0} \cdot \vec{u_j} = 0
    \]
    Using the distributive property of the dot product, we have:
    \[
        c_1 (\vec{u_1} \cdot \vec{u_j}) + c_2 (\vec{u_2} \cdot \vec{u_j}) + \ldots + c_k (\vec{u_k} \cdot \vec{u_j}) = 0
    \]
    Since the family is orthogonal, $\vec{u_i} \cdot \vec{u_j} = 0$ for all $i \neq j$, and $\vec{u_j} \cdot \vec{u_j} = \|\vec{u_j}\|^2 > 0$ (since $\vec{u_j}$ is non-zero). Thus, the above equation simplifies to:
    \[
        c_j \|\vec{u_j}\|^2 = 0
    \]
    Since $\|\vec{u_j}\|^2 > 0$, it follows that $c_j = 0$.
    Since this holds for any $j$, we conclude that $c_1 = c_2 = \ldots = c_k = 0$.
    Therefore, the family $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ is linearly independent.
\end{proof}

\begin{definition}[Orthogonal Basis]
    An orthogonal basis of a subspace $W$ of $\mathbb{R}^n$ is a basis of $W$ that is also an orthogonal family.
\end{definition}

\begin{theorem}
    Let $W$ be a subspace of $\mathbb{R}^n$ and let $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ be an orthogonal basis of $W$. Then, for any vector $\vec{v} \in W$, the coordinates of $\vec{v}$ in this basis are given by:
    \[
        c_i = \frac{\vec{v} \cdot \vec{u_i}}{\vec{u_i} \cdot \vec{u_i}} \quad \text{for } i = 1, 2, \ldots, k
    \]
    such that:
    \[
        \vec{v} = c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}
    \]
\end{theorem}
\begin{proof}
    Since $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ is a basis of $W$, any vector $\vec{v} \in W$ can be expressed as:
    \[
        \vec{v} = c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}
    \]
    for some scalars $c_1, c_2, \ldots, c_k$.
    Taking the dot product of both sides with $\vec{u_j}$ for some $j \in \{1, 2, \ldots, k\}$, we get:
    \[
        \vec{v} \cdot \vec{u_j} = (c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}) \cdot \vec{u_j}
    \]
    Using the distributive property of the dot product, we have:
    \[
        \vec{v} \cdot \vec{u_j} = c_1 (\vec{u_1} \cdot \vec{u_j}) + c_2 (\vec{u_2} \cdot \vec{u_j}) + \ldots + c_k (\vec{u_k} \cdot \vec{u_j})
    \]
    Since the family is orthogonal, $\vec{u_i} \cdot \vec{u_j} = 0$ for all $i \neq j$, and $\vec{u_j} \cdot \vec{u_j} = \|\vec{u_j}\|^2$. Thus, the above equation simplifies to:
    \[
        \vec{v} \cdot \vec{u_j} = c_j (\vec{u_j} \cdot \vec{u_j})
    \]
    Therefore:
    \[
        c_j = \frac{\vec{v} \cdot \vec{u_j}}{\vec{u_j} \cdot \vec{u_j}}
    \]
    for each $j = 1, 2, \ldots, k$.
\end{proof}

\begin{eg}
    Let $W$ be the subspace of $\mathbb{R}^3$ spanned by the orthogonal basis:
    \[
        \vec{u_1} = \begin{bmatrix}
            1 \\ 0 \\ 0
        \end{bmatrix}, \quad \vec{u_2} = \begin{bmatrix}
            0 \\ 1 \\ 1
        \end{bmatrix}
    \]
    and let $\vec{v} = \begin{bmatrix}
        3 \\ 4 \\ 4
    \end{bmatrix} \in W$. To find the coordinates of $\vec{v}$ in this basis, we compute:
    \[
        c_1 = \frac{\vec{v} \cdot \vec{u_1}}{\vec{u_1} \cdot \vec{u_1}} = \frac{3 \cdot 1 + 4 \cdot 0 + 4 \cdot 0}{1^2 + 0^2 + 0^2} = \frac{3}{1} = 3
    \]
    \[
        c_2 = \frac{\vec{v} \cdot \vec{u_2}}{\vec{u_2} \cdot \vec{u_2}} = \frac{3 \cdot 0 + 4 \cdot 1 + 4 \cdot 1}{0^2 + 1^2 + 1^2} = \frac{8}{2} = 4
    \]
    Thus, the coordinates of $\vec{v}$ in the orthogonal basis $(\vec{u_1}, \vec{u_2})$ are $(3, 4)$, and we have:
    \[
        \vec{v} = 3\vec{u_1} + 4\vec{u_2} = 3\begin{bmatrix}
            1 \\ 0 \\ 0
        \end{bmatrix} + 4\begin{bmatrix}
            0 \\ 1 \\ 1
        \end{bmatrix} = \begin{bmatrix}
            3 \\ 4 \\ 4
        \end{bmatrix}
    \]
\end{eg}

\subsection{Orthogonal Projection}
\begin{definition}[Orthogonal Projection]
    Let $W$ be a subspace of $\mathbb{R}^n$ and let $\vec{v} \in \mathbb{R}^n$. The orthogonal projection of $\vec{v}$ onto $W$, denoted by $\text{proj}_W(\vec{v})$, is the vector in $W$ such that the vector $\vec{v} - \text{proj}_W(\vec{v})$ is orthogonal to $W$.
\end{definition}
Geometrically, the orthogonal projection of $\vec{y} \in \mathbb{R}^2$ onto the line $W$ spanned by the vector $\vec{w}$ is represented as follows:
\begin{center}
    \begin{tikzpicture}
        \draw[] (-3,-1.5) -- (3,1.5) node[very near start, below right] {$W = \text{span}(\vec{w})$};
        \draw[->, primary] (0,0) -- (2,1) node[midway, above] {$\vec{w}$};
        \draw[->, primary] (0,0) -- (-2,1) node[midway, above] {$\vec{y}$};

        \draw[dashed, secondary] (-6/5,-3/5) -- (-2,1);
        \draw[->, secondary, thick] (0,0) -- (-6/5,-3/5) node[midway, below right] {$\vec{\hat{y}} = \text{proj}_W(\vec{y})$};
    \end{tikzpicture}
\end{center}
Remark that:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item the vector $\vec{y} - \vec{\hat{y}}$ is orthogonal to $W$
    \item the vector $\vec{\hat{y}}$ is the orthogonal projection of $\vec{y}$ onto $W$, denoted by $\text{proj}_W(\vec{y})$
    \item the coordinates of $\vec{\hat{y}}$ are given by:
    \[
        \vec{\hat{y}} = \frac{\vec{y} \cdot \vec{w}}{\vec{w} \cdot \vec{w}} \vec{w}
    \]
\end{itemize}

\begin{eg}
    Let's take the same situation as above but in $\mathbb{R}^3$. We have the subspace $W$, a plane, spanned by the vectors $\vec{u_1}$ and $\vec{u_2}$. To find the coordinates of $\text{proj}_W(\vec{y})$ we can start with:
    \[
        \text{proj}_W(\vec{y}) = c_1 \vec{u_1} + c_2 \vec{u_2}
    \]
    and we also have:
    \[
        \begin{cases*}
            (\vec{y} - \text{proj}_W(\vec{y})) \cdot \vec{u_1} = 0 \\ 
            (\vec{y} - \text{proj}_W(\vec{y})) \cdot \vec{u_2} = 0
        \end{cases*} \quad \iff \quad \begin{cases*}
            \text{proj}_W(\vec{y}) \cdot \vec{u_1} = \vec{y} \cdot \vec{u_1} \\
            \text{proj}_W(\vec{y}) \cdot \vec{u_2} = \vec{y} \cdot \vec{u_2}
        \end{cases*}
    \]
    By substituting $\text{proj}_W(\vec{y})$ in the equations above, we get:
    \[
        \begin{cases*}
            (c_1 \vec{u_1} + c_2 \vec{u_2}) \cdot \vec{u_1} = \vec{y} \cdot \vec{u_1} \\
            (c_1 \vec{u_1} + c_2 \vec{u_2}) \cdot \vec{u_2} = \vec{y} \cdot \vec{u_2}
        \end{cases*}
    \]
    which simplifies to:
    \[
        \begin{cases*}
            c_1 (\vec{u_1} \cdot \vec{u_1}) + c_2 (\vec{u_2} \cdot \vec{u_1}) = \vec{y} \cdot \vec{u_1} \\
            c_1 (\vec{u_1} \cdot \vec{u_2}) + c_2 (\vec{u_2} \cdot \vec{u_2}) = \vec{y} \cdot \vec{u_2}
        \end{cases*}
    \]
    In matrix form, we have:
    \[
        \begin{bmatrix}
            \vec{u_1} \cdot \vec{u_1} & \vec{u_2} \cdot \vec{u_1} \\
            \vec{u_1} \cdot \vec{u_2} & \vec{u_2} \cdot \vec{u_2}
        \end{bmatrix} \begin{bmatrix}
            c_1 \\ c_2
        \end{bmatrix} = \begin{bmatrix}
            \vec{y} \cdot \vec{u_1} \\ \vec{y} \cdot \vec{u_2}
        \end{bmatrix}
    \]
    Therefore, by solving this system of equations, we can find the coefficients $c_1$ and $c_2$, and thus the orthogonal projection $\text{proj}_W(\vec{y})$.
\end{eg}

\begin{theorem}
    Let $W$ be a subspace of $\mathbb{R}^n$ and let $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ be an orthogonal basis of $W$. Then, for any vector $\vec{v} \in \mathbb{R}^n$, the orthogonal projection of $\vec{v}$ onto $W$ is given by:
    \[
        \text{proj}_W(\vec{v}) = \sum_{i=1}^{k} \frac{\vec{v} \cdot \vec{u_i}}{\vec{u_i} \cdot \vec{u_i}} \vec{u_i}
    \]
\end{theorem}
\begin{proof}
    Since $(\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k})$ is a basis of $W$, any vector in $W$ can be expressed as:
    \[
        \text{proj}_W(\vec{v}) = c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}
    \]
    for some scalars $c_1, c_2, \ldots, c_k$.
    To find the coefficients $c_i$, we use the property that $\vec{v} - \text{proj}_W(\vec{v})$ is orthogonal to $W$. Thus, for each $j = 1, 2, \ldots, k$, we have:
    \[
        (\vec{v} - \text{proj}_W(\vec{v})) \cdot \vec{u_j} = 0
    \]
    which implies:
    \[
        \text{proj}_W(\vec{v}) \cdot \vec{u_j} = \vec{v} \cdot \vec{u_j}
    \]
    Substituting the expression for $\text{proj}_W(\vec{v})$, we get:
    \[
        (c_1 \vec{u_1} + c_2 \vec{u_2} + \ldots + c_k \vec{u_k}) \cdot \vec{u_j} = \vec{v} \cdot \vec{u_j}
    \]
    Using the distributive property of the dot product, we have:
    \[
        c_1 (\vec{u_1} \cdot \vec{u_j}) + c_2 (\vec{u_2} \cdot \vec{u_j}) + \ldots + c_k (\vec{u_k} \cdot \vec{u_j}) = \vec{v} \cdot \vec{u_j}
    \]
    Since the family is orthogonal, $\vec{u_i} \cdot \vec{u_j} = 0$ for all $i \neq j$, and $\vec{u_j} \cdot \vec{u_j} = \|\vec{u_j}\|^2$. Thus, the above equation simplifies to:
    \[
        c_j (\vec{u_j} \cdot \vec{u_j}) = \vec{v} \cdot \vec{u_j}
    \]
    Therefore:
    \[
        c_j = \frac{\vec{v} \cdot \vec{u_j}}{\vec{u_j} \cdot \vec{u_j}}
    \]
    for each $j = 1, 2, \ldots, k$.
    Substituting these coefficients back into the expression for $\text{proj}_W(\vec{v})$, we get:
    \[
        \text{proj}_W(\vec{v}) = \sum_{i=1}^{k} \frac{\vec{v} \cdot \vec{u_i}}{\vec{u_i} \cdot \vec{u_i}} \vec{u_i}
    \]
\end{proof}