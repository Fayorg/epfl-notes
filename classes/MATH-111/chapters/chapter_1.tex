\chapter{Linear Equations}


\begin{theorem}
    A linear system can only have zero, one or an infinity of solutions.
\end{theorem}

\begin{theorem}
    Elementary operations can be applied to a system without changing the solution of the given system. The following operations can be used:

    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Switch equations between them.
        \item Multiply an equation by a reel number not null.
        \item Add a multple of another equation from the system.
    \end{itemize}
\end{theorem}

\begin{definition}[Leading coefficient]
    For a matrix $n \times m$, the leading coefficient of a non-null line is the left most coefficient.
\end{definition}

\begin{definition}[Row echelon form]
    A matrix is said in row echelon form if:

    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item All null lines (if present) are at the bottom of the matrix.
        \item The leading coefficient of a non-null line is always to the right of the leading coefficient of the line above.
        \item All coefficients below a leading coefficient are null.
    \end{itemize}
\end{definition}

\begin{definition}[Reduced row echelon form]
    A matrix is said in reduced row echelon form if:

    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item It is in row echelon form.
        \item All leading coefficients are equal to 1.
        \item All coefficients above a leading coefficient are null.
    \end{itemize}
\end{definition}

\begin{definition}[Lines equivalent]
    A matrix is said lines equivalent to another matrix if we can go from the first one to the second one only with elementary operations.
\end{definition}
All lines equivalent matrix when in reduced row echelon form will have the same leading coefficients at the same positions.

\section{Vectorials equations}

\begin{definition}[$\mathbb{R}^n$]
    $\mathbb{R}^n$ is the set of all column matrices (vectors) of size $n \times 1$.
\end{definition}

\begin{definition}[Vector of $\mathbb{R}^n$]
    An element of $\mathbb{R}^n$ is called a vector of $\mathbb{R}^n$.
\end{definition}

\begin{definition}[Components]
    The components of a vector are the coefficients of the vector.
\end{definition}

\begin{eg}
    Each vector of $\mathbb{R}^n$ can be identified with a point in an n-dimensional space. For example, the vector $\begin{bmatrix} 2 \\ 3 \\ 5 \end{bmatrix}$ can be identified with the point (2, 3, 5) in a 3-dimensional space.
\end{eg}
A vector can also be called a column vector and row vectors are $1 \times n$ matrices.

\subsection{Vector operations}
Let $\vec{u} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}$ and $\vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$ be two vectors of $\mathbb{R}^n$.
\begin{definition}[Addition]
    The addition of two vectors of $\mathbb{R}^n$ is the vector obtained by adding the components of the two vectors.
    \[
    \vec{u} + \vec{v} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix}
    \]
\end{definition}

\begin{eg}
    $\newline$
    Let $\vec{a} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ and $\vec{b} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$, then:
    \[
    \vec{a} + \vec{b} = \begin{bmatrix}1 + 4 \\ 2 + 5 \\ 3 + 6 \\ \end{bmatrix} = \begin{bmatrix} 5 \\ 7 \\ 9 \end{bmatrix}
    \]
\end{eg}

\begin{definition}[Substraction]
    The subtraction of two vectors of $\mathbb{R}^n$ is the vector obtained by subtracting the components of the two vectors.
    \[
    \vec{u} - \vec{v} = \begin{bmatrix} u_1 - v_1 \\ u_2 - v_2 \\ \vdots \\ u_n - v_n \end{bmatrix}
    \]
\end{definition}

\begin{eg}
    $\newline$
    Let $\vec{a} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ and $\vec{b} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$, then:
    \[
    \vec{a} - \vec{b} = \begin{bmatrix}1 - 4 \\ 2 - 5 \\ 3 - 6 \\ \end{bmatrix} = \begin{bmatrix} -3 \\ -3 \\ -3 \end{bmatrix}
    \]
\end{eg}

\begin{definition}[Scalar mutiplication]
    Vectors can be mutiplied by a scalar $c \in \mathbb{R}$ by multiplying each component of the vector by $c$.
    \[
    c \cdot \vec{u} = c \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix} = \begin{bmatrix} c \cdot  u_1 \\ c \cdot  u_2 \\ \vdots \\ c \cdot  u_n \end{bmatrix}
    \]
\end{definition}

\begin{eg}
    $\newline$
    Let $\vec{a} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ and $c = 3$, then:
    \[
    c \cdot \vec{a} = 3 \cdot \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 3 \cdot 1 \\ 3 \cdot 2 \\ 3 \cdot 3 \end{bmatrix} = \begin{bmatrix} 3 \\ 6 \\ 9 \end{bmatrix}
    \]
\end{eg}
The following properties hold for vectors $\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^n$ and scalars $c, d \in \mathbb{R}$:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $\vec{u} + \vec{v} = \vec{v} + \vec{u}$ (Commutative property of addition)
    \item $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$ (Associative property of addition)
    \item There exists a zero vector $\vec{0} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$ such that $\vec{u} + \vec{0} = \vec{u}$ (Existence of additive identity)
    \item For every vector $\vec{u}$, there exists an additive inverse $-\vec{u}$ such that $\vec{u} + (-\vec{u}) = \vec{0}$ (Existence of additive inverse)
    \item $c \cdot (\vec{u} + \vec{v}) = c \cdot \vec{u} + c \cdot \vec{v}$ (Distributive property of scalar multiplication over vector addition)
    \item $(c + d) \cdot \vec{u} = c \cdot \vec{u} + d \cdot \vec{u}$ (Distributive property of scalar addition over vector multiplication)
    \item $(c \cdot d) \cdot \vec{u} = c \cdot (d \cdot \vec{u})$ (Associative property of scalar multiplication)
    \item $1 \cdot \vec{u} = \vec{u}$ (Existence of multiplicative identity)
\end{itemize}

% TODO: MISSING LECTURES HERE

\section{Linear combinations}
\begin{definition}[Linear combination]
    A linear combination of vectors $\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k} \in \mathbb{R}^n$ is a vector of the form:
    \[
    c_1 \cdot \vec{u_1} + c_2 \cdot \vec{u_2} + \ldots + c_k \cdot \vec{u_k}
    \]
    where $c_1, c_2, \ldots, c_k$ are scalars in $\mathbb{R}$ called coefficients (or weights).
\end{definition}

\begin{eg}
    Let $v_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $v_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ be two vectors in $\mathbb{R}^2$. A linear combination of these vectors could be:
    \[
    \frac{5}{2} \cdot v_1 - v_2 = \frac{5}{2} \cdot \begin{bmatrix} 1 \\ 0 \end{bmatrix} -  \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{5}{2} \\ -1 \end{bmatrix}
    \]
    Let's now find the coefficients $c_1$ and $c_2$ such that the linear combination of $v_1$ and $v_2$ equals the vector $\begin{bmatrix} 5 \\ 2 \end{bmatrix}$:
    \[
    c_1 \cdot v_1 + c_2 \cdot v_2 = \begin{bmatrix} 5 \\ 2 \end{bmatrix}
    \]
    This gives us the following system of equations:
    \[
    \begin{cases}
    c_1 = 5 \\
    c_2 = 2
    \end{cases}
    \]
\end{eg}

\begin{eg}
    Let $v_1 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}$ and $v_2 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$ be two vectors in $\mathbb{R}^2$. A linear combination of these vectors with the coefficients $c_1 = \frac{5}{2}$ and $c_2 = -2$ is:
    \[
        \frac{5}{2} \cdot v_1 - 2 \cdot v_2 = \frac{5}{2} \cdot \begin{bmatrix} -1 \\ 1 \end{bmatrix} - 2 \cdot \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} -\frac{13}{2} \\ \frac{1}{2} \end{bmatrix}
    \]
\end{eg}
Like in the last example, finding the coefficients of a linear combination that equals a given vector often involves solving a system of linear equations so we could turn the question into:
\begin{eg}
    Does there exist $x_1, x_2 \in \mathbb{R}$ such that:
    \[
        x_1 \cdot \begin{bmatrix} -1 \\ 1 \end{bmatrix} + x_2 \cdot \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} -x_1 \\ x_1 \end{bmatrix} + \begin{bmatrix} 2x_2 \\ x_2 \end{bmatrix} = \begin{bmatrix} -x_1 + 2x_2 \\ x_1 + x_2 \end{bmatrix} = \begin{bmatrix} 5 \\ 2 \end{bmatrix}
    \]
    This gives us the following system of equations:
    \[
        \begin{cases}
        -x_1 + 2x_2 = 5 \\
        x_1 + x_2 = 2
        \end{cases}
    \]
    Solving this system, we find $c_1 = -\frac{1}{3}$ and $c_2 = \frac{7}{3}$.
\end{eg}
A vectorial equation with $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_k} \in \mathbb{R}^n$ and $\vec{b} \in \mathbb{R}^n$ can be written as:
\[
    x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} + \ldots + x_k \cdot \vec{a_k} = \vec{b}
\]
It has the same set of solutions as the linear system whose augmented matrix is:
\[
    \begin{bmatrix}
    | & | & & | & | \\
    \vec{a_1} & \vec{a_2} & \ldots & \vec{a_k} & \vec{b} \\
    | & | & & | & |
    \end{bmatrix}
\]
In particular, $\vec{b}$ can be generated by $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_k}$ if and only if the system has (at least) one solution.

\begin{eg}
    Let $\vec{a_1} = \begin{bmatrix}1 \\ -2 \\ -5\end{bmatrix}$, $\vec{a_2} = \begin{bmatrix}2 \\ 5 \\ 6\end{bmatrix}$ and $\vec{b} = \begin{bmatrix}7 \\ 4 \\ -3\end{bmatrix}$ be three vectors in $\mathbb{R}^3$. Is $\vec{b}$ a linear combination of $\vec{a_1}$ and $\vec{a_2}$? In other words, do there exist $x_1, x_2 \in \mathbb{R}$ such that:
    \[
        x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} = \vec{b}
    \]
    This gives us the following system of equations:
    \[
        \begin{cases}
        x_1 + 2x_2 = 7 \\
        -2x_1 + 5x_2 = 4 \\
        -5x_1 + 6x_2 = -3
        \end{cases}
    \]
    We can translate this system into an augmented matrix:
    \[
        \begin{bmatrix}
        1 & 2 & 7 \\
        -2 & 5 & 4 \\
        -5 & 6 & -3
        \end{bmatrix}
    \]
    We notice that the first column is the vector $\vec{a_1}$, the second column is the vector $\vec{a_2}$ and the last column is the vector $\vec{b}$ which allows us to skip writing the equations. We could now use elementary operations to reduce the matrix and solve the system.
\end{eg}

\subsection{Span of vectors}
\begin{definition}[Span]
    The span of vectors $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_k} \in \mathbb{R}^n$ is the set of all linear combinations of these vectors. It is denoted by:
    \[
        \text{span}\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_k}\} = \{ c_1 \cdot \vec{a_1} + c_2 \cdot \vec{a_2} + \ldots + c_k \cdot \vec{a_k} \mid c_1, c_2, \ldots, c_k \in \mathbb{R} \}
    \]
    In other words, the span is the set of all vectors that can be expressed as a linear combination of the given vectors.
\end{definition}
By convention, the span of the empty set is $\{\vec{0}\}$ and the vector $\vec{0}$ can always be generated by any set of vectors since we can take all coefficients equal to zero.
\begin{figure}
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \begin{tikzpicture}[scale=1]
            % Axes
            \draw[->] (-2,0,0) -- (2,0,0) node[right] {$x_2$};
            \draw[->] (0,-2,0) -- (0,2,0) node[above] {$x_3$};
            \draw[->] (0,0,-2) -- (0,0,2) node[above] {$x_1$};

            % Span of v
            \foreach \t in {-1.2,1.2} {
                \draw[thick,secondary] (0,0,0) -- (\t*1,\t*2,\t*1);
            }

            % Vector v
            \draw[thick,->,primary] (0,0,0) -- (1,2,1) node[midway,right] {$\vec{v}$};
        \end{tikzpicture}

        \caption{Span of a vector $\vec{v}$ in $\mathbb{R}^3$}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \begin{tikzpicture}[scale=1]
            % Axes
            \draw[->] (-2,0,0) -- (2,0,0) node[right] {$x_2$};
            \draw[->] (0,-2,0) -- (0,2,0) node[above] {$x_3$};
            \draw[->] (0,0,-2) -- (0,0,2) node[above] {$x_1$};

            % Span of v1 and v2
            \fill[secondary!40,opacity=0.3] (-1.8,-1.2,-1.7) -- (1,2,1) -- (2,1,1) -- (-1,-2,-1) -- cycle;
            \draw[thick,secondary] (-1.8,-1.2,-1.7) -- (1,2,1) -- (2,1,1) -- (-1,-2,-1) -- cycle;

            % Vectors v1 and v2
            \draw[thick,->,primary] (0,0,0) -- (1,2,1) node[above] {$\vec{v_1}$};
            \draw[thick,->,primary] (0,0,0) -- (2,1,1) node[right] {$\vec{v_2}$};
        \end{tikzpicture}

        \caption{Span of two vectors $\vec{v_1}$ and $\vec{v_2}$ in $\mathbb{R}^3$}
    \end{subfigure}
\end{figure}

\section{Matrix equations}
\begin{definition}[Matrix equation]
    A matrix equation is an equation of the form:
    \[
        A \cdot \vec{x} = \vec{b}
    \]
    where $A$ is a matrix of size $m \times n$ composed of $n$ vectors in $\mathbb{R}^m$, $\vec{x}$ is a vector of size $n \times 1$ (the variable) and $\vec{b}$ is a vector of size $m \times 1$ (the result). \\
    More explicitly, $A \vec{x}$ can be written as:
    \[
        A \cdot \vec{x} = \begin{bmatrix} | & | & | & | \\ \vec{a_1} & \vec{a_2} & \ldots & \vec{a_n} \\ | & | & | & | \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} + \ldots + x_n \cdot \vec{a_n}
    \]
    where $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}$ are the column vectors of $A$.
\end{definition}
The product is only defined if the number of columns of $A$ is equal to the number of coefficients of $\vec{x}$.

\begin{eg}
    Let $A = \begin{bmatrix}3 & -2 & 1 \\ 2 & 9 & 8\end{bmatrix}$ and $\vec{x} = \begin{bmatrix} 2 \\ 4 \\ 3 \end{bmatrix}$, then:
    \[
        A \cdot \vec{x} = \begin{bmatrix}3 & -2 & 1 \\ 2 & 9 & 8\end{bmatrix} \cdot \begin{bmatrix} 2 \\ 4 \\ 3 \end{bmatrix} = 2 \cdot \begin{bmatrix} 3 \\ 2 \end{bmatrix} + 4 \cdot \begin{bmatrix} -2 \\ 9 \end{bmatrix} + 3 \cdot \begin{bmatrix} 1 \\ 8 \end{bmatrix} = \begin{bmatrix} 3(2) - 2(4) + 1(3) \\ 2(2) + 9(4) + 8(3) \end{bmatrix} = \begin{bmatrix} 1 \\ 64 \end{bmatrix}
    \]
\end{eg}

\begin{eg}
    Let $A = \begin{bmatrix}a_{11} & ... & a_{1n} \\ \vdots &  & \vdots \\ a_{m1} & ... & a_{mn} \end{bmatrix}$ and $\vec{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}$, then:
    \[
        A \cdot \vec{x} = \begin{bmatrix}a_{11} & ... & a_{1n} \\ \vdots &  & \vdots \\ a_{m1} & ... & a_{mn} \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = x_1 \cdot \begin{bmatrix} a_{11} \\ \vdots \\ a_{m1} \end{bmatrix} + ... + x_n \cdot \begin{bmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{bmatrix} = \begin{bmatrix} a_{11}x_1 + ... + a_{1n}x_n \\ \vdots \\ a_{m1}x_1 + ... + a_{mn}x_n \end{bmatrix}
    \]
\end{eg}

\begin{eg}
    Let the system of equations be:
    \[
        \begin{cases}
            3x_1 - 2x_2 + x_3 = b_1 \\
            2x_1 + 9x_2 + 8x_3 = b_2
        \end{cases}
    \]
    Can be written as the matrix equation:
    \[
        \begin{bmatrix}3 & -2 & 1 \\ 2 & 9 & 8\end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix} = A \cdot \vec{x}
    \]
    where $A \in \mathbb{R}_{2 \times 3}$, $\vec{x} \in \mathbb{R}_{3 \times 1}$ and $\vec{b} \in \mathbb{R}_{2 \times 1}$.
\end{eg}

\subsection{Equivalence of matrix equations, vectorial equations and linear systems}
For a matrix $A \in \mathbb{R}_{m \times n}$ with column vectors $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n} \in \mathbb{R}^m$ and a vector $\vec{b} \in \mathbb{R}^m$, the matrix equation 
\[
    A \cdot \vec{x} = \vec{b}
\]
is equivalent (has the same set of solutions) as the vectorial equation 
\[
    x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} + \ldots + x_n \cdot \vec{a_n} = \vec{b}
\]
and the linear system whose augmented matrix is
\[
    \begin{bmatrix}
    | & | & & | & | \\
    \vec{a_1} & \vec{a_2} & \ldots & \vec{a_n} & \vec{b} \\
    | & | & & | & | 
    \end{bmatrix}
\]
It can also be expressed as a linear system of equations:
\[
    \begin{cases}
        a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n = b_1 \\
        \vdots \\
        a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n = b_m
    \end{cases}
\]
If a solution (at least one) exists, it means that $\vec{b}$ can be generated by the column vectors of $A$ and thus $\vec{b} \in \text{span}\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}\}$.
\begin{theorem}
    Let $A$ be a matrix of size $m \times n$. The following statements are equivalent:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item For every $\vec{b} \in \mathbb{R}^m$, the equation $A \cdot \vec{x} = \vec{b}$ has at least one solution.
        \item Every $\vec{b} \in \mathbb{R}^m$ is a linear combination of the columns of $A$.
        \item The columns of $A$ span $\mathbb{R}^m$.
        \item $A$ has a pivot position in every row.
    \end{itemize}
\end{theorem}

\subsection{Properties of matrix equations}
\begin{theorem}
    Let $A$ be a matrix of size $m \times n$, $\vec{u}, \vec{v} \in \mathbb{R}^n$ and $c \in \mathbb{R}$. Then:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $A(\vec{u} + \vec{v}) = A\vec{u} + A\vec{v}$ (Distributive property over vector addition)
        \item $A(c \vec{u}) = c(A\vec{u})$ (Distributive property over scalar multiplication)
    \end{itemize}
\end{theorem}
\begin{proof}
    Let $A = \begin{bmatrix} | & | & & | \\ \vec{a_1} & \vec{a_2} & \ldots & \vec{a_n} \\ | & | & & | \end{bmatrix}$, $\vec{u} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}$ and $\vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$. \\
    $\circ$ Distributive property over vector addition:
    \begin{align*}
        A(\vec{u} + \vec{v}) &= A \cdot \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix} = (u_1 + v_1) \cdot \vec{a_1} + (u_2 + v_2) \cdot \vec{a_2} + \ldots + (u_n + v_n) \cdot \vec{a_n} \\
        &= (u_1 \cdot \vec{a_1} + u_2 \cdot \vec{a_2} + \ldots + u_n \cdot \vec{a_n}) + (v_1 \cdot \vec{a_1} + v_2 \cdot \vec{a_2} + \ldots + v_n \cdot \vec{a_n}) = A\vec{u} + A\vec{v}
    \end{align*}
    $\circ$ Distributive property over scalar multiplication:
    \begin{align*}
        A(c \vec{u}) &= A \cdot \begin{bmatrix} c u_1 \\ c u_2 \\ \vdots \\ c u_n \end{bmatrix} = (c u_1) \cdot \vec{a_1} + (c u_2) \cdot \vec{a_2} + \ldots + (c u_n) \cdot \vec{a_n} \\
        &= c (u_1 \cdot \vec{a_1} + u_2 \cdot \vec{a_2} + \ldots + u_n \cdot \vec{a_n}) = c(A\vec{u})
    \end{align*}
\end{proof}

\subsection{Homogeneous systems}
\begin{definition}
    A homogeneous system is a linear system that can be written as:
    \[
        A \cdot \vec{x} = \vec{0}
    \]
    where $A$ is a matrix of size $m \times n$ and $\vec{0}$ is the zero vector of size $m \times 1$.
\end{definition}
A homogeneous system always has at least one solution, called the trivial solution, which is $\vec{x} = \vec{0}$ (all coefficients equal to zero). If there exists another solution, it is called a non-trivial solution. If a homogeneous system has a non-trivial solution, it must have an infinity of solutions.
\begin{theorem}
    A homogeneous system $A \cdot \vec{x} = \vec{0}$ has a non-trivial solution if and only if the system has at least one free variable.
\end{theorem}

\begin{eg}
    Let the system of equations be:
    \[
        \begin{cases}
            3x_1 + 5x_2 - 4x_3 = 0 \\
            -3x_1 - 2x_2 + 4x_3 = 0 \\
            6x_1 + x_2 - 8x_3 = 0
        \end{cases}
    \]
    The augmented matrix is:
    \[
        \begin{bmatrix}
            3 & 5 & -4 & 0 \\
            -3 & -2 & 4 & 0 \\
            6 & 1 & -8 & 0
        \end{bmatrix}
    \]
    We can use elementary operations to reduce the matrix:
    \[
        \begin{bmatrix}
            1 & 0 & \frac{-4}{3} & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 0
        \end{bmatrix}
    \]
    We can see that $x_3$ is a free variable, thus there exists a non-trivial solution. We can express $x_1$ and $x_2$ in terms of $x_3$:
    \[
        \begin{cases}
            x_1 = \frac{4}{3}x_3 \\
            x_2 = 0 \\
            x_3 = x_3
        \end{cases}
    \]
    Letting $x_3 = t$ where $t \in \mathbb{R}$, we can express the solution set as:
    \[
        \vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} \frac{4}{3}t \\ 0 \\ t \end{bmatrix} = t \cdot \begin{bmatrix} \frac{4}{3} \\ 0 \\ 1 \end{bmatrix}, \quad t \in \mathbb{R}
    \]
    The solution set can be expressed as the span of the vector $\vec{v} = \begin{bmatrix} \frac{4}{3} \\ 0 \\ 1 \end{bmatrix}$ thus it is a line in $\mathbb{R}^3$ passing through the origin and directed by the vector $\vec{v}$.
\end{eg}

\begin{eg}
    Let the system of equations be:
    \[
        \begin{cases}
            10x_1 - 3x_2 - 2x_3 = 0 \\
        \end{cases}
    \]
    The augmented matrix is:
    \[
        \begin{bmatrix}
            10 & -3 & -2 & 0
        \end{bmatrix}
    \]
    We can see that $x_2$ and $x_3$ are free variables, thus there exists a non-trivial solution. We can express $x_1$ in terms of $x_2$ and $x_3$:
    \[
        \begin{cases}
            x_1 = \frac{3}{10}x_2 + \frac{1}{5}x_3 \\
            x_2 = x_2 \\
            x_3 = x_3
        \end{cases}
    \]
    Letting $x_2 = s$ and $x_3 = t$ where $s, t \in \mathbb{R}$, we can express the solution set as:
    \[
        \vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} \frac{3}{10}s + \frac{1}{5}t \\ s \\ t \end{bmatrix} = s \cdot \begin{bmatrix} \frac{3}{10} \\ 1 \\ 0 \end{bmatrix} + t \cdot \begin{bmatrix} \frac{1}{5} \\ 0 \\ 1 \end{bmatrix}, \quad s, t \in \mathbb{R}
    \]
    The solution set can be expressed as the span of the vectors $\vec{v_1} = \begin{bmatrix} \frac{3}{10} \\ 1 \\ 0 \end{bmatrix}$ and $\vec{v_2} = \begin{bmatrix} \frac{1}{5} \\ 0 \\ 1 \end{bmatrix}$ thus it is a plane in $\mathbb{R}^3$ passing through the origin and directed by the vectors $\vec{v_1}$ and $\vec{v_2}$.
\end{eg}

\subsection{Non-homogeneous systems}
In the more general case of a non-homogeneous system $A \cdot \vec{x} = \vec{b}$, it is sufficient to find one particular solution $\vec{p}$ to the equation and all solutions of the associated homogeneous equation $A \cdot \vec{x} = \vec{0}$ to express the solution set of the more general non-homogeneous system.
\begin{theorem}
    If $\vec{p}$ is a particular solution of the non-homogeneous system $A \cdot \vec{x} = \vec{b}$, then the solution set of the system is:
    \[
        \{ \vec{p} + \vec{h} \mid \vec{h} \text{ is a solution of } A \cdot \vec{x} = \vec{0} \}
    \]
    In other words, the solution set of the non-homogeneous system is obtained by translating the solution set of the associated homogeneous system by the vector $\vec{p}$.
\end{theorem}
% Figure of the translation with a vector here
\begin{proof}
    Let $\vec{p}$ be a particular solution of the non-homogeneous system $A \cdot \vec{x} = \vec{b}$ and let $\vec{h}$ be a solution of the associated homogeneous system $A \cdot \vec{x} = \vec{0}$. We want to show that $\vec{p} + \vec{h}$ is a solution of the non-homogeneous system. We have:
    \[
        A(\vec{p} + \vec{h}) = A\vec{p} + A\vec{h} = \vec{b} + \vec{0} = \vec{b}
    \]
    Thus, $\vec{p} + \vec{h}$ is indeed a solution of the non-homogeneous system. Conversely, let $\vec{s}$ be any solution of the non-homogeneous system. We want to show that there exists a solution $\vec{h}$ of the associated homogeneous system such that $\vec{s} = \vec{p} + \vec{h}$. We have:
    \[
        A\vec{s} = \vec{b}
    \]
    and
    \[
        A\vec{p} = \vec{b}
    \]
    Subtracting these two equations, we get:
    \[
        A(\vec{s} - \vec{p}) = A\vec{s} - A\vec{p} = \vec{b} - \vec{b} = \vec{0}
    \]
    Thus, $\vec{s} - \vec{p}$ is a solution of the associated homogeneous system. Letting $\vec{h} = \vec{s} - \vec{p}$, we have $\vec{s} = \vec{p} + \vec{h}$ where $\vec{h}$ is a solution of the associated homogeneous system. This completes the proof.
\end{proof}

\begin{eg}
    If $\vec{p}$ is a solution of the non-homogeneous system $A \cdot \vec{x} = \vec{b}$ and all solutions of the associated homogeneous system $A \cdot \vec{x} = \vec{0}$ are given by:
    \[
        \vec{h} = s \cdot \vec{u} + t \cdot \vec{v}, \quad s, t \in \mathbb{R}
    \]
    where $\vec{u}$ and $\vec{v}$ are two vectors in $\mathbb{R}^n$, then the solution set of the non-homogeneous system is:
    \[
        \{ \vec{p} + s \cdot \vec{u} + t \cdot \vec{v} \mid s, t \in \mathbb{R} \}
    \]
\end{eg}

\section{Linear independence}
Let a matrix $A$ with column vectors $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n} \in \mathbb{R}^m$. The homogeneous system $A \cdot \vec{x} = \vec{0}$ can be written as the vectorial equation:
\[
    x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} + \ldots + x_n \cdot \vec{a_n} = \vec{0}
\]
Asking if $A \cdot \vec{x} = \vec{0}$ has a non-trivial solution is the same as asking if one of the vectors $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}$ can be generated by the others.
\begin{definition}[Linear independence]
    The set of vectors $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n} \in \mathbb{R}^m$ is said to be linearly independent (or free) if the vectorial equation:
    \[
        x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} + \ldots + x_n \cdot \vec{a_n} = \vec{0}
    \]
    has only the trivial solution $x_1 = x_2 = \ldots = x_n = 0$. Otherwise, the set of vectors is said to be linearly dependent.
\end{definition}

\begin{eg}
    Let $\vec{a_1} = \begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix}$ and $\vec{a_2} = \begin{bmatrix}2 \\ 1 \\ 0\end{bmatrix}$ be two vectors in $\mathbb{R}^3$. The augmented matrix of the associated homogeneous system is:
    \[
        \begin{bmatrix}
            1 & 2 & 0 \\
            1 & 1 & 0 \\
            1 & 0 & 0
        \end{bmatrix}
    \]
    We can use elementary operations to reduce the matrix:
    \[
        \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 0
        \end{bmatrix}
    \]
    We can see that there are no free variables (or that the only solution is the trivial one), thus the set of vectors $\{\vec{a_1}, \vec{a_2}\}$ is linearly independent.
\end{eg}
A family with a single vector $\{\vec{a_1}\}$ is linearly dependent if and only if $\vec{a_1} = \vec{0}$. A family with two vectors $\{\vec{a_1}, \vec{a_2}\}$ is linearly dependent if and only if one of the vectors is a multiple of the other (or they are not co-linear).
\begin{proof}
    Let $\vec{a_1}, \vec{a_2} \in \mathbb{R}^m$. The vectorial equation:
    \[
        x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} = \vec{0}
    \]
    has a non-trivial solution if and only if there exist $c_1, c_2 \in \mathbb{R}$, not both zero, such that:
    \[
        c_1 \cdot \vec{a_1} + c_2 \cdot \vec{a_2} = \vec{0}
    \]
    If $c_1 \neq 0$, we can rewrite this as:
    \[
        \vec{a_1} = -\frac{c_2}{c_1} \cdot \vec{a_2}
    \]
    which means that $\vec{a_1}$ is a multiple of $\vec{a_2}$. Similarly, if $c_2 \neq 0$, we can rewrite this as:
    \[
        \vec{a_2} = -\frac{c_1}{c_2} \cdot \vec{a_1}
    \]
    which means that $\vec{a_2}$ is a multiple of $\vec{a_1}$. Conversely, if one of the vectors is a multiple of the other, then there exist $k \in \mathbb{R}$ such that $\vec{a_1} = k \cdot \vec{a_2}$ or $\vec{a_2} = k' \cdot \vec{a_1}$ for some $k' \in \mathbb{R}$. In either case, we can find non-zero coefficients $c_1$ and $c_2$ such that:
    \[
        c_1 \cdot \vec{a_1} + c_2 \cdot \vec{a_2} = 0
    \]
    Thus, the set of vectors $\{\vec{a_1}, \vec{a_2}\}$ is linearly dependent.
\end{proof}

\begin{theorem}
    If a set of vectors $\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}\}$ in $\mathbb{R}^m$ are linearly dependent, then at least one of the vectors can be expressed as a linear combination of the others.
\end{theorem}
\begin{proof}
    Let $\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}\}$ be a set of linearly dependent vectors in $\mathbb{R}^m$. By definition, there exist scalars $c_1, c_2, \ldots, c_n$, not all zero, such that:
    \[
        c_1 \cdot \vec{a_1} + c_2 \cdot \vec{a_2} + \ldots + c_n \cdot \vec{a_n} = \vec{0}
    \]
    Without loss of generality, assume that $c_k \neq 0$ for some $k$ (where $1 \leq k \leq n$). We can rearrange the equation to express $\vec{a_k}$ as a linear combination of the other vectors:
    \[
        c_k \cdot \vec{a_k} = - (c_1 \cdot \vec{a_1} + c_2 \cdot \vec{a_2} + \ldots + c_{k-1} \cdot \vec{a_{k-1}} + c_{k+1} \cdot \vec{a_{k+1}} + \ldots + c_n \cdot \vec{a_n})
    \]
    Dividing both sides by $c_k$, we get:
    \[
        \vec{a_k} = -\frac{c_1}{c_k} \cdot \vec{a_1} -\frac{c_2}{c_k} \cdot \vec{a_2} - \ldots -\frac{c_{k-1}}{c_k} \cdot \vec{a_{k-1}} -\frac{c_{k+1}}{c_k} \cdot \vec{a_{k+1}} - \ldots -\frac{c_n}{c_k} \cdot \vec{a_n}
    \]
    Thus, $\vec{a_k}$ can be expressed as a linear combination of the other vectors in the set. This completes the proof.
\end{proof}
The converse is also true:
\begin{theorem}
    If at least one of the vectors $\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}\}$ in $\mathbb{R}^m$ is a linear combination of the ones before it, then the set of vectors is linearly dependent.
\end{theorem}
\begin{proof}
    Let $\vec{a_k}$ be a vector in the set that can be expressed as a linear combination of the previous vectors:
    \[
        \vec{a_k} = d_1 \cdot \vec{a_1} + d_2 \cdot \vec{a_2} + \ldots + d_{k-1} \cdot \vec{a_{k-1}}
    \]
    for some scalars $d_1, d_2, \ldots, d_{k-1}$. We can rearrange this equation to form a linear combination that equals the zero vector:
    \[
        -\vec{a_k} + d_1 \cdot \vec{a_1} + d_2 \cdot \vec{a_2} + \ldots + d_{k-1} \cdot \vec{a_{k-1}} = \vec{0}
    \]
    This can be rewritten as:
    \[
        0 \cdot \vec{a_1} + 0 \cdot \vec{a_2} + \ldots + (-1) \cdot \vec{a_k} + 0 \cdot \vec{a_{k+1}} + \ldots + 0 \cdot \vec{a_n} = 0
    \]
    Here, we have found a non-trivial solution (not all coefficients are zero) to the vectorial equation:
    \[
        c_1 \cdot \vec{a_1} + c_2 \cdot \vec{a_2} + \ldots + c_n \cdot \vec{a_n} = \vec{0}
    \]
    where $c_k = -1$ and all other $c_i = 0$. Thus, the set of vectors $\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}\}$ is linearly dependent. This completes the proof.
\end{proof}
Let $\vec{v_1}, ..., \vec{v_n}$ be linearly independent vectors and $\vec{w}$ be another vector. The set $\{\vec{v_1}, ..., \vec{v_n}, \vec{w}\}$ is linearly dependent if and only if $\vec{w}$ is in the span of $\{\vec{v_1}, ..., \vec{v_n}\}$.

\begin{theorem}
    If a set of vectors $\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}\}$ in $\mathbb{R}^m$ contains the zero vector, then the set is linearly dependent.
\end{theorem}
\begin{proof}
    Let $\vec{a_k} = \vec{0}$ (where $1 \leq k \leq n$) be the zero vector in the set. We can form a linear combination that equals the zero vector:
    \[
        0 \cdot \vec{a_1} + 0 \cdot \vec{a_2} + \ldots + 1 \cdot \vec{a_k} + 0 \cdot \vec{a_{k+1}} + \ldots + 0 \cdot \vec{a_n} = \vec{0}
    \]
    Here, we have found a non-trivial solution (not all coefficients are zero) to the vectorial equation:
    \[
        c_1 \cdot \vec{a_1} + c_2 \cdot \vec{a_2} + \ldots + c_n \cdot \vec{a_n} = \vec{0}
    \]
    where $c_k = 1$ and all other $c_i = 0$. Thus, the set of vectors $\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}\}$ is linearly dependent. This completes the proof.
\end{proof}

\begin{theorem}
    A family of $n$ vectors in $\mathbb{R}^m$ is linearly dependent if $n > m$.
\end{theorem}
\begin{proof}
    Let $\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}\}$ be a set of $n$ vectors in $\mathbb{R}^m$ where $n > m$. The associated homogeneous system can be represented by the augmented matrix:
    \[
        \begin{bmatrix}
            | & | & & | \\
            \vec{a_1} & \vec{a_2} & \ldots & \vec{a_n} \\
            | & | & & |
        \end{bmatrix}
    \]
    Since there are more vectors (columns) than dimensions (rows), the system must have at least one free variable. This is because the maximum number of pivot positions in a matrix with $m$ rows is $m$, and since $n > m$, there must be at least one column without a pivot position. Therefore, the homogeneous system has a non-trivial solution. By the definition of linear dependence, this means that the set of vectors $\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}\}$ is linearly dependent. This completes the proof.
\end{proof}

\section{Transformations}
A matrix $A$ of size $m \times n$ defines a transformation (or function) for each vector $\vec{x} \in \mathbb{R}^n$ to a vector $A \cdot \vec{x} \in \mathbb{R}^m$.

\begin{definition}[transformation]
    A transformation $T$ from $\mathbb{R}^n$ to $\mathbb{R}^m$ is a rule that assigns to each vector $\vec{x} \in \mathbb{R}^n$ a vector $T(\vec{x}) \in \mathbb{R}^m$. We can define it as:
    \[
        T: \mathbb{R}^n \to \mathbb{R}^m, \quad \vec{x} \mapsto T(\vec{x})
    \]
\end{definition}
For $x$ in $\mathbb{R}^n$, $T(\vec{x})$ is called the image of $\vec{x}$ under the transformation $T$. The set of all images of vectors in $\mathbb{R}^n$ is called the range of $T$.

\subsection{Matrix transformations}
\begin{definition}[Matrix transformation]
    A matrix transformation is a transformation $T$ from $\mathbb{R}^n$ to $\mathbb{R}^m$ such that there exists a matrix $A$ of size $m \times n$ such that for every vector $\vec{x} \in \mathbb{R}^n$, $T(\vec{x}) = A \cdot \vec{x}$.
\end{definition}

\begin{eg}
    Let $A = \begin{bmatrix}
        1 \\ -3
        3 \\ 5
        -1 \\ 7
    \end{bmatrix}, T: \mathbb{R}^2 \to \mathbb{R}^3$ be the matrix transformation defined by $T$. Then:
    \[
        T(\vec{x}) = A \cdot \vec{x} = \begin{bmatrix}
            1 & -3 \\
            3 & 5 \\
            -1 & 7
        \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix}
            x_1 - 3x_2 \\
            3x_1 + 5x_2 \\
            -x_1 + 7x_2
        \end{bmatrix}
    \]
    Let $\vec{u} = \begin{bmatrix} 2 \\ -1 \end{bmatrix}$, then:
    \[
        T(\vec{u}) = A \cdot \vec{u} = \begin{bmatrix}
            1 & -3 \\
            3 & 5 \\
            -1 & 7
        \end{bmatrix} \cdot \begin{bmatrix} 2 \\ -1 \end{bmatrix} = \begin{bmatrix}
            2 - 3(-1) \\
            3(2) + 5(-1) \\
            -2 + 7(-1)
        \end{bmatrix} = \begin{bmatrix}
            5 \\ 1 \\ -9
        \end{bmatrix}
    \]
    Let $\vec{b} = \begin{bmatrix} 3 \\ 2 \\ -5 \end{bmatrix}$ in $\mathbb{R}^3$. To find if there exists a vector (or vectors) $\vec{x} \in \mathbb{R}^2$ such that $T(\vec{x}) = \vec{b}$, we need to solve the equation:
    \[
        A \cdot \vec{x} = \vec{b} \quad \Leftrightarrow \quad \begin{bmatrix}
            1 & -3 \\
            3 & 5 \\
            -1 & 7
        \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 3 \\ 2 \\ -5 \end{bmatrix}
    \]
    The augmented matrix is:
    \[
        \begin{bmatrix}
            1 & -3 & 3 \\
            3 & 5 & 2 \\
            -1 & 7 & -5
        \end{bmatrix}
    \]
    We can use elementary operations to reduce the matrix:
    \[
        \begin{bmatrix}
            1 & 0 & \frac{3}{2} \\
            0 & 1 & -\frac{1}{2} \\
            0 & 0 & 0
        \end{bmatrix}
    \]
    Thus, the solution set is:
    \[
        \begin{cases}
            x_1 = \frac{3}{2} \\
            x_2 = -\frac{1}{2}
        \end{cases}
    \]
    Therefore, there exists a unique vector $\vec{x} = \begin{bmatrix} \frac{3}{2} \\ -\frac{1}{2} \end{bmatrix}$ such that $T(\vec{x}) = \vec{b}$. \\
    Let $\vec{c} = \begin{bmatrix} 3 \\ 2 \\ 5 \end{bmatrix}$ be another vector in $\mathbb{R}^3$. To find if there exists a vector (or vectors) $\vec{x} \in \mathbb{R}^2$ such that $T(\vec{x}) = \vec{c}$, we need to solve the equation:
    \[
        A \cdot \vec{x} = \vec{c} \quad \Leftrightarrow \quad \begin{bmatrix}
            1 & -3 \\
            3 & 5 \\
            -1 & 7
        \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 3 \\ 2 \\ 5 \end{bmatrix}
    \]
    The augmented matrix is:
    \[
        \begin{bmatrix}
            1 & -3 & 3 \\
            3 & 5 & 2 \\
            -1 & 7 & 5
        \end{bmatrix}
    \]
    We can use elementary operations to reduce the matrix:
    \[
        \begin{bmatrix}
            1 & 0 & \frac{3}{2} \\
            0 & 1 & -\frac{1}{2} \\
            0 & 0 & \frac{5}{2}
        \end{bmatrix}
    \]
    We can see that the last row corresponds to the equation $\frac{5}{2} = 0$, which is a contradiction. Thus, there is no solution and there is no vector $\vec{x} \in \mathbb{R}^2$ such that $T(\vec{x}) = \vec{c}$.
\end{eg}

\begin{eg}
    Let $A = \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 0
    \end{bmatrix}$ and $T: \mathbb{R}^3 \to \mathbb{R}^3$ be the matrix transformation defined by $T$. Then:
    \[
        T(\vec{x}) = A \cdot \vec{x} = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 0
        \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix}
            x_1 \\ x_2 \\ 0
        \end{bmatrix}
    \]
    Geometrically, this transformation projects every vector in $\mathbb{R}^3$ onto the $x_1$-$x_2$ plane by setting the $x_3$-coordinate to zero. \\
    \begin{center}
        % Projection onto the x1-x2 plane
        \begin{tikzpicture}[scale=0.85]
            % Axes
            \draw[->] (0,0,0) -- (3,0,0) node[right] {$x_1$};
            \draw[->] (0,0,0) -- (0,2,0) node[above] {$x_3$};
            \draw[->] (0,0,0) -- (0,0,3) node[above] {$x_2$};

            % Projection onto x1-x2 plane
            \fill[secondary!40,opacity=0.3] (0,0,0) -- (3,0,0) -- (3,0,3) -- (0,0,3) -- cycle;

            % Point in space
            \fill[] (2,2,1) circle (2pt) node[above right] {$\vec{x}$};
            \fill[] (1,-1,2.5) circle (2pt) node[right] {$\vec{v}$};

            % Projection line
            \draw[dashed] (2,2,1) -- (2,0,1);
            \draw[dashed] (1,-1,2.5) -- (1,0,2.5);

            % Projection point
            \fill[primary] (2,0,1) circle (2pt) node[below right] {$T(\vec{x})$};
            \fill[primary] (1,0,2.5) circle (2pt) node[above right] {$T(\vec{v})$};
        \end{tikzpicture}
    \end{center}
\end{eg}

\subsection{Linear transformations}
% A matrix transformation follows the same rules as matrix multiplication, thus it preserves vector addition and scalar multiplication. It will be proven later that the converse is also true: any transformation that preserves vector addition and scalar multiplication can be represented by a matrix transformation.
\begin{definition}[Linear transformation]
    A transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ is said to be linear if for all vectors $\vec{u}, \vec{v} \in \mathbb{R}^n$ and all scalars $c \in \mathbb{R}$, the following properties hold:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Additivity: $T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v})$
        \item Homogeneity: $T(c \vec{u}) = c T(\vec{u})$
    \end{itemize}
\end{definition}
Two properties can be deduced from the two above:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $T(\vec{0}) = T(0 \cdot \vec{u}) = 0 \cdot T(\vec{u}) = \vec{0}$.
    \item $T(c \vec{u} + d \vec{v}) = T(c \cdot \vec{u}) + T(d \cdot \vec{v}) = c \cdot T(\vec{u}) + d \cdot T(\vec{v})$
\end{itemize}

\begin{eg}
    Let $r \in \mathbb{R}$ and $T: \mathbb{R}^2 \to \mathbb{R}^2$ be the transformation defined by:
    \[
        T(\vec{x}) = r \cdot \vec{x} = r \cdot \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} r x_1 \\ r x_2 \end{bmatrix}
    \]
    Let's show that $T$ is a linear transformation. Let $\vec{u} = \begin{bmatrix} u_1 \\ u_2 \end{bmatrix}$ and $\vec{v} = \begin{bmatrix} v_1 \\ v_2 \end{bmatrix}$ be two vectors in $\mathbb{R}^2$ and $c \in \mathbb{R}$ be a scalar. We have:
    \[
        T(\vec{u} + \vec{v}) = r \cdot (\vec{u} + \vec{v}) = r \cdot \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \end{bmatrix} = \begin{bmatrix} r(u_1 + v_1) \\ r(u_2 + v_2) \end{bmatrix} = \begin{bmatrix} r u_1 \\ r u_2 \end{bmatrix} + \begin{bmatrix} r v_1 \\ r v_2 \end{bmatrix} = T(\vec{u}) + T(\vec{v})
    \]
    and
    \[
        T(c \vec{u}) = r \cdot (c \vec{u}) = r \cdot \begin{bmatrix} c u_1 \\ c u_2 \end{bmatrix} = \begin{bmatrix} r(c u_1) \\ r(c u_2) \end{bmatrix} = c \cdot \begin{bmatrix} r u_1 \\ r u_2 \end{bmatrix} = c \cdot T(\vec{u})
    \]
    Thus, $T$ is a linear transformation. Geometrically, this transformation scales every vector in $\mathbb{R}^2$ by a factor of $r$. \\
    Note that if $r = 1$, $T$ is the identity transformation, if $r > 1$, $T$ is an expansion, if $0 < r < 1$, $T$ is a contraction, and if $r < 0$, $T$ is a reflection followed by a scaling.
\end{eg}

\begin{eg}
    Let $T(\vec{x}) = \begin{bmatrix}
        0 & -1 \\
        1 & 0
    \end{bmatrix} \begin{bmatrix}
        x_1 \\ x_2
    \end{bmatrix} = \begin{bmatrix}
        -x_2 \\ x_1
    \end{bmatrix}$ be a transformation from $\mathbb{R}^2$ to $\mathbb{R}^2$. It's visible that $T$ is a matrix transformation, thus it is a linear transformation. \\
    Let $\vec{u} = \begin{bmatrix}
        2 \\ 0
    \end{bmatrix}, \vec{v} = \begin{bmatrix}
        0 \\ 3
    \end{bmatrix}$ be two vectors in $\mathbb{R}^2$. We have:
    \[
        T(\vec{u}) = \begin{bmatrix}
            0 & -1 \\
            1 & 0
        \end{bmatrix} \begin{bmatrix}
            2 \\ 0
        \end{bmatrix} = \begin{bmatrix}
            0 \\ 2
        \end{bmatrix}
    \]
    and
    \[
        T(\vec{v}) = \begin{bmatrix}
            0 & -1 \\
            1 & 0
        \end{bmatrix} \begin{bmatrix}
            0 \\ 3
        \end{bmatrix} = \begin{bmatrix}
            -3 \\ 0
        \end{bmatrix}
    \]
    Geometrically, this transformation rotates every vector in $\mathbb{R}^2$ by 90 degrees counterclockwise about the origin.
\end{eg}

\begin{eg}
    Let $T: \mathbb{R}^2 \to \mathbb{R}^2$ be a linear transformation and let $\vec{u}, \vec{v}$ be two linearly independent vectors in $\mathbb{R}^2$ and $\vec{w} = c_1 \vec{u} + c_2 \vec{v}$ be a linear combination of $\vec{u}$ and $\vec{v}$ where $0 \leq c_1, c_2 \leq 1$.
    \begin{center}
        \begin{tikzpicture}[scale=2.5]
            % Axes
            \draw[->] (-0.2,0) -- (1.2,0) node[right] {$x_1$};
            \draw[->] (0,-0.2) -- (0,1.2) node[above] {$x_2$};

            \draw[thick,->,primary] (0,0) -- (1,0) node[midway, below] {$\vec{u}$};
            \draw[thick,->,primary] (0,0) -- (0,1) node[midway, left] {$\vec{v}$};

            \draw[thick,->,secondary] (0,0) -- (0.7,0.5) node[midway, above] {$\vec{w}$};

            \fill[secondary!40,opacity=0.3] (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;
        \end{tikzpicture}
    \end{center}
    If we apply the transformation $T$ to $\vec{u}, \vec{v}$ and $\vec{w}$, we get:
    \begin{center}
        \begin{tikzpicture}[scale=2.5]
            % Axes
            \draw[->] (-0.2,0) -- (1.2,0) node[right] {$x_1$};
            \draw[->] (0,-0.2) -- (0,1.2) node[above] {$x_2$};

            \draw[thick,->,primary] (0,0) -- (0.8,0.1) node[midway, below=0.1cm] {$T(\vec{u})$};
            \draw[thick,->,primary] (0,0) -- (0.2,1.1) node[midway, left=0.2cm] {$T(\vec{v})$};

            \draw[thick,->,secondary] (0,0) -- (0.7,0.8) node[midway, above=0.3cm] {$T(\vec{w})$};

            \fill[secondary!40,opacity=0.3] (0,0) -- (0.8,0.1) -- (1,1.2) -- (0.2,1.1) -- cycle;
        \end{tikzpicture}
    \end{center}
    We can see that $T(\vec{w}) = T(c_1 \vec{u} + c_2 \vec{v}) = c_1 T(\vec{u}) + c_2 T(\vec{v})$ is a linear combination of $T(\vec{u})$ and $T(\vec{v})$. Thus, the image of the square defined by $\vec{u}$ and $\vec{v}$ is the parallelogram defined by $T(\vec{u})$ and $T(\vec{v})$.
\end{eg}
% For all linear transformations $T: \mathbb{R}^n \to \mathbb{R}^m$, there exists a matrix $A$ of size $m \times n$ such that $T(\vec{x}) = A \cdot \vec{x}$ for all $\vec{x} \in \mathbb{R}^n$. The columns of $A$ are the images of the columns of the identity matrix $I_n$ under the transformation $T$.

\subsection{Matrix representation of linear transformations}
\begin{theorem}
    Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation. Then there exists a unique matrix $A$ of size $m \times n$ such that for every vector $\vec{x} \in \mathbb{R}^n$, $T(\vec{x}) = A \cdot \vec{x}$. Where $A$ is given by:
    \[
        A = \begin{bmatrix} | & | && | \\ T(\vec{e_1}) & T(\vec{e_2}) & \ldots & T(\vec{e_n}) \\ | & | && | \end{bmatrix}
    \]
    where $\vec{e_1}, \vec{e_2}, \ldots, \vec{e_n}$ are the columns of the identity matrix $I_n$.
\end{theorem}
\begin{proof}
    Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation. Let $\vec{e_1}, \vec{e_2}, \ldots, \vec{e_n}$ be the columns of the identity matrix $I_n$. Define the matrix $A$ as:
    \[
        A = \begin{bmatrix} | & | && | \\ T(\vec{e_1}) & T(\vec{e_2}) & \ldots & T(\vec{e_n}) \\ | & | && | \end{bmatrix}
    \]
    For any vector $\vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \in \mathbb{R}^n$, we can express $\vec{x}$ as a linear combination of the standard basis vectors:
    \[
        \vec{x} = x_1 \vec{e_1} + x_2 \vec{e_2} + \ldots + x_n \vec{e_n}
    \]
    Applying the linear transformation $T$ to both sides, we have:
    \[
        T(\vec{x}) = T(x_1 \vec{e_1} + x_2 \vec{e_2} + \ldots + x_n \vec{e_n})
    \]
    By the properties of linear transformations (additivity and homogeneity), we can rewrite this as:
    \[
        T(\vec{x}) = x_1 T(\vec{e_1}) + x_2 T(\vec{e_2}) + \ldots + x_n T(\vec{e_n})
    \]
    This can be expressed in matrix form as:
    \[
        T(\vec{x}) = A \cdot \vec{x}
    \]
    where $A$ is the matrix defined above. Thus, for every vector $\vec{x} \in \mathbb{R}^n$, we have $T(\vec{x}) = A \cdot \vec{x}$. \\
    To show that $A$ is unique, suppose there exists another matrix $B$ of size $m \times n$ such that for every vector $\vec{x} \in \mathbb{R}^n$, $T(\vec{x}) = B \cdot \vec{x}$. In particular, this must hold for the standard basis vectors:
    \[
        T(\vec{e_i}) = B \cdot \vec{e_i} \quad \text{for } i = 1, 2, \ldots, n
    \]
    Since $B \cdot \vec{e_i}$ is simply the $i$-th column of $B$, it follows that the columns of $B$ must be equal to the columns of $A$. Therefore, $B = A$, proving the uniqueness of $A$. This completes the proof.
\end{proof}

\begin{eg}
    Let $e_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $e_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ be two column vectors of $I_2$ (the identity matrix of size 2). Let $T: \mathbb{R}^2 \to \mathbb{R}^3$ be a linear transformation such that:
    \[
        T(e_1) = \begin{bmatrix} 3 \\ -1 \\ 0 \end{bmatrix}, \quad T(e_2) = \begin{bmatrix} 2 \\ 2 \\ 6 \end{bmatrix}
    \]
    By linearity, for any vector $\vec{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb{R}^2$, we have:
    \[
        T(\vec{x}) = T(x_1 e_1 + x_2 e_2) = x_1 T(e_1) + x_2 T(e_2)
    \]
    Thus:
    \[
        T(\vec{x}) = x_1 \begin{bmatrix} 3 \\ -1 \\ 0 \end{bmatrix} + x_2 \begin{bmatrix} 2 \\ 2 \\ 6 \end{bmatrix} = \begin{bmatrix}3 & 2 \\ -1 & 2 \\ 0 & 6\end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = A \cdot \vec{x}
    \]
    where $A = \begin{bmatrix}3 & 2 \\ -1 & 2 \\ 0 & 6\end{bmatrix}$ is the matrix associated with the linear transformation $T$.
\end{eg}

\begin{eg}
    Let $T(\vec{x}) = 3 \cdot \vec{x}$ be an expansion transformation from $\mathbb{R}^2$ to $\mathbb{R}^2$. To find the matrix associated with $T$, we need to determine the images of the standard basis vectors $\vec{e_1}$ and $\vec{e_2}$ under the transformation $T$:
    \[
        T(\vec{e_1}) = 3 \cdot \vec{e_1} = 3 \cdot \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 3 \\ 0 \end{bmatrix}, \quad T(\vec{e_2}) = 3 \cdot \vec{e_2} = 3 \cdot \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 3 \end{bmatrix}
    \]
    Thus, the matrix associated with $T$ is:
    \[
        A = \begin{bmatrix} | & | \\ T(\vec{e_1}) & T(\vec{e_2}) \\ | & | \end{bmatrix} = \begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix}
    \]
\end{eg}

\section{Type of transformations}
For all the examples below, the starting graph would be a unit square defined by the vectors $\vec{u} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $\vec{v} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ in $\mathbb{R}^2$.
\begin{center}
    \begin{tikzpicture}[scale=2.5]
        % Axes
        \draw[->] (-0.2,0) -- (1.2,0) node[right] {$x_1$};
        \draw[->] (0,-0.2) -- (0,1.2) node[above] {$x_2$};

        % Original vectors
        \draw[thick,->,primary] (0,0) -- (1,0) node[midway, below] {$\vec{u}$};
        \draw[thick,->,primary] (0,0) -- (0,1) node[midway, left] {$\vec{v}$};

        % Shaded area
        \fill[secondary!40,opacity=0.3] (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;
    \end{tikzpicture}
\end{center}

\subsection{Scaling transformations}
\begin{definition}[Scaling transformation]
    A scaling transformation is a linear transformation $T: \mathbb{R}^n \to \mathbb{R}^n$ defined by $T(\vec{x}) = r \cdot \vec{x}$ for some scalar $r \in \mathbb{R}$.
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item If $r = 1$, it is called the identity transformation;
        \item If $r > 1$, it is called an expansion;
        \item If $0 < r < 1$, it is called a contraction;
        \item If $r < 0$, it is a reflection followed by a scaling.
    \end{itemize}
    Its associated matrix is $A = r \cdot I_n$, where $I_n$ is the identity matrix of size $n$.
\end{definition}

\begin{eg}
    Let $T: \mathbb{R}^2 \to \mathbb{R}^2$ be a horizontal contraction transformation. The associated matrix is:
    \[
        A = \begin{bmatrix} k & 0 \\ 0 & 1 \end{bmatrix}
    \]
    Geometrically, this transformation scales every vector in $\mathbb{R}^2$ by a factor of $k$ in the $x_1$ direction, where $0 < k < 1$. \\
    \begin{center}
        \begin{tikzpicture}[scale=2.5]
            % Axes
            \draw[->] (-0.2,0) -- (1.2,0) node[right] {$x_1$};
            \draw[->] (0,-0.2) -- (0,1.2) node[above] {$x_2$};

            % Original vectors
            %\draw[thick,->,primary] (0,0) -- (1,0) node[midway, below] {$\vec{u}$};
            \draw[thick,->,secondary] (0,0) -- (0.6,0) node[midway, below] {T($\vec{u}$)};
            \draw[thick,->,primary] (0,0) -- (0,1) node[midway, left] {$\vec{v}$};

            % Shaded area
            \fill[secondary!40,opacity=0.3] (0,0) -- (0.6,0) -- (0.6,1) -- (0,1) -- cycle;
            \draw[dashed] (1,0) -- (1,1) -- (0,1);
        \end{tikzpicture}
    \end{center}
    where $T(\vec{u}) = \begin{bmatrix} k \\ 0 \end{bmatrix}$ and $T(\vec{v}) = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$. \\
    Note that if $k > 1$, it would be a horizontal expansion transformation. \\
    In the case of a vertical contraction or expansion transformation, the associated matrix would be:
    \[
        A = \begin{bmatrix} 1 & 0 \\ 0 & k \end{bmatrix}
    \]
    and the graph would be contracted or expanded in the $x_2$ direction.
\end{eg}

\subsection{Reflection transformations}
\begin{definition}[Reflection transformation]
    A reflection transformation is a linear transformation $T: \mathbb{R}^n \to \mathbb{R}^n$ defined by $T(\vec{x}) = -\vec{x}$ in the case of reflection across the origin. Its associated matrix is $A = -I_n$, where $I_n$ is the identity matrix of size $n$.
\end{definition}

\begin{eg}
    Let $T: \mathbb{R}^2 \to \mathbb{R}^2$ be a reflection transformation across the $x_1$-axis. The associated matrix is:
    \[
        A = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
    \]
    Geometrically, this transformation reflects every vector in $\mathbb{R}^2$ across the $x_1$-axis by negating the $x_2$-coordinate. \\
    \begin{center}
        \begin{tikzpicture}[scale=2.5]
            % Axes
            \draw[->] (-0.2,0) -- (1.2,0) node[right] {$x_1$};
            \draw[->] (0,-1.2) -- (0,1.2) node[above] {$x_2$};

            % Original vectors
            \draw[thick,->,primary] (0,0) -- (1,0) node[midway, below] {$\vec{u}$};
            \draw[thick,->,primary] (0,0) -- (0,1) node[midway, left] {$\vec{v}$};
            \draw[thick,->,secondary] (0,0) -- (0,-1) node[midway, left] {$T(\vec{v})$};

            % Shaded area
            \fill[secondary!40,opacity=0.3] (0,0) -- (1,0) -- (1,-1) -- (0,-1) -- cycle;
            \draw[dashed] (1,0) -- (1,1) -- (0,1);
        \end{tikzpicture}
    \end{center}
    where $T(\vec{u}) = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $T(\vec{v}) = \begin{bmatrix} 0 \\ -1 \end{bmatrix}$. \\
    Note that if the reflection were across the $x_2$-axis, the associated matrix would be:
    \[
        A = \begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix}
    \]
    and the graph would be reflected across the $x_2$-axis by negating the $x_1$-coordinate. \\
    Other common reflections include reflection across the line $x_2 = x_1$, with associated matrix:
    \[
        A = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
    \]
    and the graph would be reflected across the line $x_2 = x_1$ by swapping the $x_1$ and $x_2$ coordinates. \\
    Relfection across the line $x_2 = -x_1$ has associated matrix:
    \[
        A = \begin{bmatrix} 0 & -1 \\ -1 & 0 \end{bmatrix}
    \]
    and the graph would be reflected across the line $x_2 = -x_1$ by swapping and negating the $x_1$ and $x_2$ coordinates. \\
    Reflection can be done across any line through the origin in $\mathbb{R}^2$ or any plane through the origin in $\mathbb{R}^3$, but the associated matrices would be more complex.
\end{eg}

\subsection{Transversion transformations}
\begin{definition}[Transversion transformation]
    A transvection keeps all points on a certain line (the “axis” of the transvection) fixed, and “slides” everything else parallel to that line. It's associated matrix in the case of $\mathbb{R}^2$ is of the form:
    \[
        A = \begin{bmatrix}
            1 & k \\
            0 & 1
        \end{bmatrix}
    \]
    for a horizontal transvection, or
    \[
        A = \begin{bmatrix}
            1 & 0 \\
            k & 1
        \end{bmatrix}
    \]
    for a vertical transvection, where $k \in \mathbb{R}$.
\end{definition}
\begin{eg}
    Let $T: \mathbb{R}^2 \to \mathbb{R}^2$ be a horizontal transversion transformation. The associated matrix is:
    \[
        A = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}
    \]
    Geometrically, this transformation shears every vector in $\mathbb{R}^2$ horizontally.
    \begin{center}
        \begin{tikzpicture}[scale=2.5]
            % Axes
            \draw[->] (-0.8,0) -- (1.2,0) node[right] {$x_1$};
            \draw[->] (0,-0.2) -- (0,1.2) node[above] {$x_2$};

            % Original vectors
            \draw[thick,->,primary] (0,0) -- (1,0) node[midway, below] {$\vec{u}$};
            \draw[thick,->,primary] (0,0) -- (0,1) node[midway, left] {$\vec{v}$};
            \draw[thick,->,secondary] (0,0) -- (-0.6,1) node[midway, left] {T($\vec{v})$};

            % Shaded area
            \fill[secondary!40,opacity=0.3] (0,0) -- (1,0) -- (0.4,1) -- (-0.6,1) -- cycle;
            \draw[dashed] (1,0) -- (1,1) -- (0,1);
        \end{tikzpicture}
    \end{center}
    where $T(\vec{u}) = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$, $T(\vec{v}) = \begin{bmatrix} k \\ 1 \end{bmatrix}$ and $k < 0$. If the transvection were such that $k > 0$, the graph would be sheared in the opposite direction (to the right). \\
    Reciprocally, in the case of a vertical transvection transformation, the associated matrix would be:
    \[
        A = \begin{bmatrix} 1 & 0 \\ k & 1 \end{bmatrix}
    \]
    and the graph would be sheared vertically.
\end{eg}

\subsection{Projection transformations}
\begin{definition}[Projection transformation]
    A projection transformation is a linear transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ defined by projecting every vector in $\mathbb{R}^n$ onto a subspace of $\mathbb{R}^m$.
\end{definition}
\begin{eg}
    Let $T: \mathbb{R}^2 \to \mathbb{R}^2$ be a projection transformation onto the $x_1$ axis. The associated matrix is:
    \[
        A = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}
    \]
    Geometrically, this transformation projects every vector in $\mathbb{R}^2$ onto the $x_1$ axis.
    \begin{center}
        \begin{tikzpicture}[scale=2.5]
            % Axes
            \draw[->] (-0.2,0) -- (1.2,0) node[right] {$x_1$};
            \draw[->] (0,-0.2) -- (0,1.2) node[above] {$x_2$};

            % Original vectors
            \draw[thick,->,primary] (0,0) -- (1,0) node[midway, below] {$\vec{u}$};
            \draw[thick,->,primary] (0,0) -- (0,1) node[midway, left] {$\vec{v}$};
            \fill[secondary] (0,0) circle (1pt) node[below left] {T($\vec{v})$};

            % Shaded area
            \fill[secondary!40,opacity=0.3] (0,0.05) -- (1,0.05) -- (1,-0.05) -- (0,-0.05) -- cycle;
            \draw[dashed] (1,0) -- (1,1) -- (0,1);
        \end{tikzpicture}
    \end{center}
    where $T(\vec{u}) = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $T(\vec{v}) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$. \\
    Note that if the projection were onto the $x_2$ axis, the associated matrix would be:
    \[
        A = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
    \]
    and the graph would be projected onto the $x_2$ axis.
\end{eg}

\section{Injectivity, surjectivity and bijectivity}
\begin{definition}[Injective transformation]
    A transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ is said to be injective (or one-to-one) if for every vector $\vec{y} \in \mathbb{R}^m$, there is at most one vector $\vec{x} \in \mathbb{R}^n$ such that $T(\vec{x}) = \vec{y}$.
\end{definition}

\begin{definition}[Surjective transformation]
    A transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ is said to be surjective (or onto) if for every vector $\vec{y} \in \mathbb{R}^m$, there is at least one vector $\vec{x} \in \mathbb{R}^n$ such that $T(\vec{x}) = \vec{y}$.
\end{definition}

\begin{theorem}
    Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation. $T$ is injective if and only if the equation $T(\vec{x}) = \vec{0}$ has only the trivial solution $\vec{x} = \vec{0}$.
\end{theorem}
\begin{proof}
    ($\Rightarrow$) Suppose $T$ is injective. We want to show that the equation $T(\vec{x}) = \vec{0}$ has only the trivial solution $\vec{x} = \vec{0}$. Assume there exists a non-trivial solution $\vec{x_1} \neq \vec{0}$ such that $T(\vec{x_1}) = \vec{0}$. Since $T(\vec{0}) = \vec{0}$, we have two distinct vectors $\vec{x_1}$ and $\vec{0}$ such that $T(\vec{x_1}) = T(\vec{0}) = \vec{0}$. This contradicts the injectivity of $T$. Therefore, the equation $T(\vec{x}) = \vec{0}$ has only the trivial solution $\vec{x} = \vec{0}$. \\
    ($\Leftarrow$) Suppose the equation $T(\vec{x}) = \vec{0}$ has only the trivial solution $\vec{x} = \vec{0}$. We want to show that $T$ is injective. Assume there exist two distinct vectors $\vec{x_1}, \vec{x_2} \in \mathbb{R}^n$ such that $T(\vec{x_1}) = T(\vec{x_2})$. Then:
    \[
        T(\vec{x_1}) - T(\vec{x_2}) = \vec{0}
    \]
    By the linearity of $T$, we have:
    \[
        T(\vec{x_1} - \vec{x_2}) = \vec{0}
    \]
    Since the equation $T(\vec{x}) = \vec{0}$ has only the trivial solution, it follows that $\vec{x_1} - \vec{x_2} = \vec{0}$, which implies $\vec{x_1} = \vec{x_2}$. This contradicts our assumption that $\vec{x_1}$ and $\vec{x_2}$ are distinct. Therefore, $T$ is injective.
\end{proof}

\begin{theorem}
    Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation and $A$ be its associated matrix. Then:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $T$ is injective if and only if the columns of $A$ are linearly independent (i.e., $A$ has a pivot in every column);
        \item $T$ is surjective if and only if the columns of $A$ span $\mathbb{R}^m$ (i.e., $A$ has a pivot in every row);
    \end{itemize}
\end{theorem}
\begin{proof}
    Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation and $A$ be its associated matrix. We have $T(\vec{x}) = A \cdot \vec{x}$ for all $\vec{x} \in \mathbb{R}^n$. \\ \\
    $\circ$ In the case of injectivity: \\
    ($\Rightarrow$) Suppose $T$ is injective. By the previous theorem, the equation $T(\vec{x}) = \vec{0}$ has only the trivial solution $\vec{x} = \vec{0}$. This means that the null space of $A$ contains only the zero vector, which implies that the columns of $A$ are linearly independent. Therefore, $A$ has a pivot in every column. \\
    ($\Leftarrow$) Suppose the columns of $A$ are linearly independent (i.e., $A$ has a pivot in every column). This means that the null space of $A$ contains only the zero vector, which implies that the equation $T(\vec{x}) = \vec{0}$ has only the trivial solution $\vec{x} = \vec{0}$. By the previous theorem, this means that $T$ is injective. \\ \\
    $\circ$ In the case of surjectivity: \\
    ($\Rightarrow$) Suppose $T$ is surjective. This means that for every vector $\vec{y} \in \mathbb{R}^m$, there exists at least one vector $\vec{x} \in \mathbb{R}^n$ such that $T(\vec{x}) = \vec{y}$. This implies that the range of $A$ is $\mathbb{R}^m$, which means that the columns of $A$ span $\mathbb{R}^m$. Therefore, $A$ has a pivot in every row. \\
    ($\Leftarrow$) Suppose the columns of $A$ span $\mathbb{R}^m$ (i.e., $A$ has a pivot in every row). This means that for every vector $\vec{y} \in \mathbb{R}^m$, there exists at least one vector $\vec{x} \in \mathbb{R}^n$ such that $A \cdot \vec{x} = \vec{y}$. This implies that for every vector $\vec{y} \in \mathbb{R}^m$, there exists at least one vector $\vec{x} \in \mathbb{R}^n$ such that $T(\vec{x}) = \vec{y}$. Therefore, $T$ is surjective.
\end{proof}

\section{Exercises}
This section gathers a selection of exercises related to Chapter \thechapter, taken from weekly assignments, past exams, textbooks, and other sources. The origin of each exercise will be indicated at its beginning.
% maybe exercise at the end of w3c1 classe
\begin{exercise}[Assignments Week 1, Exercise 2]
    Let's consider the following equation:
    \[
        \alpha x_1 + \beta x_2 = 1
    \]
    where $\alpha, \beta \in \mathbb{R}$. For what values of $\alpha$ and $\beta$ does the line defined by the equation above is parallel to the line defined by the equation $-x_1 + x_2 = -1$?
    \Answer
    Both line are parallel if their normal vectors ($(\alpha, \beta)$ and $(-1, 1)$) are scalar multiples of each other. Thus, we have:
    \[
        (\alpha, \beta) = k \cdot (-1, 1)
    \]
    for some scalar $k \in \mathbb{R}$. Therefore, $\alpha = -k$ and $\beta = k$ for some $k \in \mathbb{R}$.
\end{exercise}

\begin{exercise}[Assignments Week 1, Exercise 2]
    Let's find the value of $\alpha$ and $\beta$ such that the following system of equations:
    \[
        \begin{cases}
            -x_1 + x_2 =  -1 \\
            \alpha x_1 + \beta x_2 = 1 \\
            (\alpha -1)x_1 + (\beta + 1)x_2 = 0
        \end{cases}
    \]
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item has an infinite number of solutions;
        \item has no solution;
        \item has a unique solution.
    \end{itemize}
    \Answer
    Let's write the system in matrix form:
    \[
        \begin{bmatrix}
            -1 & 1 \\
            \alpha & \beta \\
            \alpha - 1 & \beta + 1
        \end{bmatrix}
        \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
        =
        \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}
    \]
    We can create the augmented matrix and row reduce it:
    \[
        \left[\begin{array}{cc|c}
            -1 & 1 & -1 \\
            \alpha & \beta & 1 \\
            \alpha - 1 & \beta + 1 & 0
        \end{array}\right]
        \sim
        \left[\begin{array}{cc|c}
            1 & -1 & 1 \\
            0 & \beta + \alpha & 1 - \alpha \\
            0 & \beta + \alpha & 1 - \alpha
        \end{array}\right]
        \sim
        \left[\begin{array}{cc|c}
            1 & -1 & 1 \\
            0 & \beta + \alpha & 1 - \alpha \\
            0 & 0 & 0
        \end{array}\right]
    \]
    We have that if $\alpha + \beta \neq 0$, the system has a unique solution. If $\alpha + \beta = 0$ the system is degenerate. In this case, if $-\alpha + 1 \neq 0$ the system has no solution, and if $-\alpha + 1 = 0$ (i.e. $\alpha = 1$ and $\beta = -1$) the system has an infinite number of solutions.
\end{exercise}

\begin{exercise}[Assignments Week 2, Exercise 5]
    Let's determine if the following homogeneous system of equations has a non-trivial solution:
    \[
        \begin{cases}
            -7x_1 + 37x_2 + 119x_3 = 0 \\
            5x_1 + 19x_2 + 57x_3 = 0
        \end{cases}
    \]
    \Answer
    We easily see that the system has less equations (two) than unknowns (three). Thus, the system has an infinite number of solutions, including non-trivial ones.
\end{exercise}

\begin{exercise}[Assignments Week 2, Exercise 13]
    Let $A$ be a matrix of size $m \times n$. Show that the columns of $A$ span $\mathbb{R}^m$ if and only if the echelon form of $A$ has a pivot in every row.
    \Answer
    Let's prove both directions of the equivalence:
    \begin{align*}
        &\text{The columns of $A$ span $\mathbb{R}^m$} \\
        \iff \quad &\text{For all vectors $\vec{b} \in \mathbb{R}^m$, the equation $A \vec{x} = \vec{b}$ has at least one solution $\vec{x} \in \mathbb{R}^n$} \\
        &\text{(i.e. $\vec{b}$ is a linear combination of the columns of $A$)} \\
        \iff \quad &\text{The row echelon form of the augmented matrix has no line of the form $\left[\ 0 \ \ldots \ c \ \right]$} \\
        &\text{with $c$ non-zero ; and $A$ has no free variables (i.e. no line of the form $\left[\ 0 \ \ldots \ 0 \ \right]$)} \\
        \iff \quad &\text{Each row has a pivot}
    \end{align*}
\end{exercise}

\begin{exercise}[Assignments Week 3, Exercise 3]
    Describe the row echelon form of a matrix $A$ of size $4 \times 2$, $A = \left[\ \vec{a_1} \ \vec{a_2} \ \right]$ and $\vec{a_2}$ is not multiple of $\vec{a_1}$.
    \Answer
    If $\vec{a_1} \neq \vec{0}$, then since $\vec{a_2}$ is not a multiple of $\vec{a_1}$, the row echelon form of $A$ is:
    \[
        \begin{bmatrix}
            1 & 0 \\
            0 & 1 \\
            0 & 0 \\
            0 & 0
        \end{bmatrix}
    \]
    Otherwise, if $\vec{a_1} = \vec{0}$ then $\vec{a_2} \neq \vec{0}$, then the row echelon form of $A$ is:
    \[
        \begin{bmatrix}
            0 & 1 \\
            0 & 0 \\
            0 & 0 \\
            0 & 0
        \end{bmatrix}
    \]
\end{exercise}

\begin{exercise}[Assignments Week 3, Exercise 8]
    For each of the following cases, determine if the transformation is injective, surjective or both:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $T: \mathbb{R}^2 \to \mathbb{R}^3$, $\begin{bmatrix}x_1 \\ x_2\end{bmatrix} \to \begin{bmatrix}4x_1 + 3x_2 \\ x_1 \\ x_2\end{bmatrix}$
        \item $T: \mathbb{R}^3 \to \mathbb{R}$, $\begin{bmatrix}x_1 \\ x_2 \\ x_3\end{bmatrix} \to x_1 + x_2 + x_3$
        \item $T: \mathbb{R}^3 \to \mathbb{R}^3$, $\begin{bmatrix}x_1 \\ x_2 \\ x_3\end{bmatrix} \to \begin{bmatrix}x_3 \\ x_2 \\ x_1\end{bmatrix}$
        \item $T: \mathbb{R}^2 \to \mathbb{R}^2$, $\begin{bmatrix}x_1 \\ x_2\end{bmatrix} \to \begin{bmatrix}x_1 + x_2 \\ x_1 + x_2\end{bmatrix}$
        \item $T: \mathbb{R}^2 \to \mathbb{R}^2$, $\begin{bmatrix}x_1 \\ x_2\end{bmatrix} \to \begin{bmatrix}x_1 + x_2 \\ x_1 - x_2\end{bmatrix}$
        \item $T: \mathbb{R}^2 \to \mathbb{R}^2$, $\begin{bmatrix}x_1 \\ x_2\end{bmatrix} \to \begin{bmatrix}(x_1)^2 + (x_2)^2 \\ x_1\end{bmatrix}$
    \end{itemize}
    \Answer
    Let's analyze each transformation using its associated matrix:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item The associated matrix is:
        \[
            A = \begin{bmatrix} 4 & 3 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}
        \]
        Since $A$ has a pivot in every column but not in every row, $T$ is injective but not surjective.
        \item The associated matrix is:
        \[
            A = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix}
        \]
        Since $A$ has a pivot in every row but not in every column, $T$ is surjective but not injective.
        \item The associated matrix is:
        \[
            A = \begin{bmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 \end{bmatrix}
        \]
        Since $A$ has a pivot in every row and every column, $T$ is both injective and surjective (bijective).
        \item The associated matrix is:
        \[
            A = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}
        \]
        Since $A$ has neither a pivot in every row nor in every column, $T$ is neither injective nor surjective.
        \item The associated matrix is:
        \[
            A = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}
        \]
        Since $A$ has a pivot in every row and every column, $T$ is both injective and surjective (bijective).
        \item The transformation is not linear (due to the square term), so we cannot determine injectivity or surjectivity using the associated matrix method.
    \end{itemize}
\end{exercise}