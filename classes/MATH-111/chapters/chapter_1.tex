\chapter{Linear Equations}


\begin{theorem}
    A linear system can only have zero, one or an infinity of solutions.
\end{theorem}

\begin{theorem}
    Elementary operations can be applied to a system without changing the solution of the given system. The following operations can be used:

    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Switch equations between them.
        \item Multiply an equation by a reel number not null.
        \item Add a multple of another equation from the system.
    \end{itemize}
\end{theorem}

\begin{definition}[Leading coefficient]
    For a matrix $n \times m$, the leading coefficient of a non-null line is the left most coefficient.
\end{definition}

\begin{definition}[Row echelon form]
    A matrix is said in row echelon form if:

    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item All null lines (if present) are at the bottom of the matrix.
        \item The leading coefficient of a non-null line is always to the right of the leading coefficient of the line above.
        \item All coefficients below a leading coefficient are null.
    \end{itemize}
\end{definition}

\begin{definition}[Reduced row echelon form]
    A matrix is said in reduced row echelon form if:

    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item It is in row echelon form.
        \item All leading coefficients are equal to 1.
        \item All coefficients above a leading coefficient are null.
    \end{itemize}
\end{definition}

\begin{definition}[Lines equivalent]
    A matrix is said lines equivalent to another matrix if we can go from the first one to the second one only with elementary operations.
\end{definition}
All lines equivalent matrix when in reduced row echelon form will have the same leading coefficients at the same positions.

\section{Vectorials equations}

\begin{definition}[$\mathbb{R}^n$]
    $\mathbb{R}^n$ is the set of all column matrices (vectors) of size $n \times 1$.
\end{definition}

\begin{definition}[Vector of $\mathbb{R}^n$]
    An element of $\mathbb{R}^n$ is called a vector of $\mathbb{R}^n$.
\end{definition}

\begin{definition}[Components]
    The components of a vector are the coefficients of the vector.
\end{definition}

\begin{eg}
    Each vector of $\mathbb{R}^n$ can be identified with a point in an n-dimensional space. For example, the vector $\begin{bmatrix} 2 \\ 3 \\ 5 \end{bmatrix}$ can be identified with the point (2, 3, 5) in a 3-dimensional space.
\end{eg}
A vector can also be called a column vector and row vectors are $1 \times n$ matrices.

\subsection{Vector operations}
Let $\vec{u} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}$ and $\vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$ be two vectors of $\mathbb{R}^n$.
\begin{definition}[Addition]
    The addition of two vectors of $\mathbb{R}^n$ is the vector obtained by adding the components of the two vectors.
    \[
    \vec{u} + \vec{v} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix}
    \]
\end{definition}

\begin{eg}
    $\newline$
    Let $\vec{a} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ and $\vec{b} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$, then:
    \[
    \vec{a} + \vec{b} = \begin{bmatrix}1 + 4 \\ 2 + 5 \\ 3 + 6 \\ \end{bmatrix} = \begin{bmatrix} 5 \\ 7 \\ 9 \end{bmatrix}
    \]
\end{eg}

\begin{definition}[Substraction]
    The subtraction of two vectors of $\mathbb{R}^n$ is the vector obtained by subtracting the components of the two vectors.
    \[
    \vec{u} - \vec{v} = \begin{bmatrix} u_1 - v_1 \\ u_2 - v_2 \\ \vdots \\ u_n - v_n \end{bmatrix}
    \]
\end{definition}

\begin{eg}
    $\newline$
    Let $\vec{a} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ and $\vec{b} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$, then:
    \[
    \vec{a} - \vec{b} = \begin{bmatrix}1 - 4 \\ 2 - 5 \\ 3 - 6 \\ \end{bmatrix} = \begin{bmatrix} -3 \\ -3 \\ -3 \end{bmatrix}
    \]
\end{eg}

\begin{definition}[Scalar mutiplication]
    Vectors can be mutiplied by a scalar $c \in \mathbb{R}$ by multiplying each component of the vector by $c$.
    \[
    c \cdot \vec{u} = c \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix} = \begin{bmatrix} c \cdot  u_1 \\ c \cdot  u_2 \\ \vdots \\ c \cdot  u_n \end{bmatrix}
    \]
\end{definition}

\begin{eg}
    $\newline$
    Let $\vec{a} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ and $c = 3$, then:
    \[
    c \cdot \vec{a} = 3 \cdot \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 3 \cdot 1 \\ 3 \cdot 2 \\ 3 \cdot 3 \end{bmatrix} = \begin{bmatrix} 3 \\ 6 \\ 9 \end{bmatrix}
    \]
\end{eg}
The following properties hold for vectors $\vec{u}, \vec{v}, \vec{w} \in \mathbb{R}^n$ and scalars $c, d \in \mathbb{R}$:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $\vec{u} + \vec{v} = \vec{v} + \vec{u}$ (Commutative property of addition)
    \item $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$ (Associative property of addition)
    \item There exists a zero vector $\vec{0} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}$ such that $\vec{u} + \vec{0} = \vec{u}$ (Existence of additive identity)
    \item For every vector $\vec{u}$, there exists an additive inverse $-\vec{u}$ such that $\vec{u} + (-\vec{u}) = \vec{0}$ (Existence of additive inverse)
    \item $c \cdot (\vec{u} + \vec{v}) = c \cdot \vec{u} + c \cdot \vec{v}$ (Distributive property of scalar multiplication over vector addition)
    \item $(c + d) \cdot \vec{u} = c \cdot \vec{u} + d \cdot \vec{u}$ (Distributive property of scalar addition over vector multiplication)
    \item $(c \cdot d) \cdot \vec{u} = c \cdot (d \cdot \vec{u})$ (Associative property of scalar multiplication)
    \item $1 \cdot \vec{u} = \vec{u}$ (Existence of multiplicative identity)
\end{itemize}

% MISSING LECTURES HERE

\section{Linear combinations}
\begin{definition}[Linear combination]
    A linear combination of vectors $\vec{u_1}, \vec{u_2}, \ldots, \vec{u_k} \in \mathbb{R}^n$ is a vector of the form:
    \[
    c_1 \cdot \vec{u_1} + c_2 \cdot \vec{u_2} + \ldots + c_k \cdot \vec{u_k}
    \]
    where $c_1, c_2, \ldots, c_k$ are scalars in $\mathbb{R}$ called coefficients (or weights).
\end{definition}

\begin{eg}
    Let $v_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $v_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$ be two vectors in $\mathbb{R}^2$. A linear combination of these vectors could be:
    \[
    \frac{5}{2} \cdot v_1 - v_2 = \frac{5}{2} \cdot \begin{bmatrix} 1 \\ 0 \end{bmatrix} -  \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{5}{2} \\ -1 \end{bmatrix}
    \]
    Let's now find the coefficients $c_1$ and $c_2$ such that the linear combination of $v_1$ and $v_2$ equals the vector $\begin{bmatrix} 5 \\ 2 \end{bmatrix}$:
    \[
    c_1 \cdot v_1 + c_2 \cdot v_2 = \begin{bmatrix} 5 \\ 2 \end{bmatrix}
    \]
    This gives us the following system of equations:
    \[
    \begin{cases}
    c_1 = 5 \\
    c_2 = 2
    \end{cases}
    \]
\end{eg}

\begin{eg}
    Let $v_1 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}$ and $v_2 = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$ be two vectors in $\mathbb{R}^2$. A linear combination of these vectors with the coefficients $c_1 = \frac{5}{2}$ and $c_2 = -2$ is:
    \[
        \frac{5}{2} \cdot v_1 - 2 \cdot v_2 = \frac{5}{2} \cdot \begin{bmatrix} -1 \\ 1 \end{bmatrix} - 2 \cdot \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} -\frac{13}{2} \\ \frac{1}{2} \end{bmatrix}
    \]
\end{eg}
Like in the last example, finding the coefficients of a linear combination that equals a given vector often involves solving a system of linear equations so we could turn the question into:
\begin{eg}
    Does there exist $x_1, x_2 \in \mathbb{R}$ such that:
    \[
        x_1 \cdot \begin{bmatrix} -1 \\ 1 \end{bmatrix} + x_2 \cdot \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} -x_1 \\ x_1 \end{bmatrix} + \begin{bmatrix} 2x_2 \\ x_2 \end{bmatrix} = \begin{bmatrix} -x_1 + 2x_2 \\ x_1 + x_2 \end{bmatrix} = \begin{bmatrix} 5 \\ 2 \end{bmatrix}
    \]
    This gives us the following system of equations:
    \[
        \begin{cases}
        -x_1 + 2x_2 = 5 \\
        x_1 + x_2 = 2
        \end{cases}
    \]
    Solving this system, we find $c_1 = -\frac{1}{3}$ and $c_2 = \frac{7}{3}$.
\end{eg}
A vectorial equation with $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_k} \in \mathbb{R}^n$ and $\vec{b} \in \mathbb{R}^n$ can be written as:
\[
    x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} + \ldots + x_k \cdot \vec{a_k} = \vec{b}
\]
It has the same set of solutions as the linear system whose augmented matrix is:
\[
    \begin{bmatrix}
    | & | & & | & | \\
    \vec{a_1} & \vec{a_2} & \ldots & \vec{a_k} & \vec{b} \\
    | & | & & | & |
    \end{bmatrix}
\]
In particular, $\vec{b}$ can be generated by $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_k}$ if and only if the system has (at least) one solution.

\begin{eg}
    Let $\vec{a_1} = \begin{bmatrix}1 \\ -2 \\ -5\end{bmatrix}$, $\vec{a_2} = \begin{bmatrix}2 \\ 5 \\ 6\end{bmatrix}$ and $\vec{b} = \begin{bmatrix}7 \\ 4 \\ -3\end{bmatrix}$ be three vectors in $\mathbb{R}^3$. Is $\vec{b}$ a linear combination of $\vec{a_1}$ and $\vec{a_2}$? In other words, do there exist $x_1, x_2 \in \mathbb{R}$ such that:
    \[
        x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} = \vec{b}
    \]
    This gives us the following system of equations:
    \[
        \begin{cases}
        x_1 + 2x_2 = 7 \\
        -2x_1 + 5x_2 = 4 \\
        -5x_1 + 6x_2 = -3
        \end{cases}
    \]
    We can translate this system into an augmented matrix:
    \[
        \begin{bmatrix}
        1 & 2 & 7 \\
        -2 & 5 & 4 \\
        -5 & 6 & -3
        \end{bmatrix}
    \]
    We notice that the first column is the vector $\vec{a_1}$, the second column is the vector $\vec{a_2}$ and the last column is the vector $\vec{b}$ which allows us to skip writing the equations. We could now use elementary operations to reduce the matrix and solve the system.
\end{eg}

\subsection{Span of vectors}
\begin{definition}[Span]
    The span of vectors $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_k} \in \mathbb{R}^n$ is the set of all linear combinations of these vectors. It is denoted by:
    \[
        \text{span}\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_k}\} = \{ c_1 \cdot \vec{a_1} + c_2 \cdot \vec{a_2} + \ldots + c_k \cdot \vec{a_k} \mid c_1, c_2, \ldots, c_k \in \mathbb{R} \}
    \]
    In other words, the span is the set of all vectors that can be expressed as a linear combination of the given vectors.
\end{definition}
By convention, the span of the empty set is $\{\vec{0}\}$ and the vector $\vec{0}$ can always be generated by any set of vectors since we can take all coefficients equal to zero.
\begin{figure}
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \begin{tikzpicture}[scale=1]
            % Axes
            \draw[->] (-2,0,0) -- (2,0,0) node[right] {$x_2$};
            \draw[->] (0,-2,0) -- (0,2,0) node[above] {$x_3$};
            \draw[->] (0,0,-2) -- (0,0,2) node[above] {$x_1$};

            % Span of v
            \foreach \t in {-1.2,1.2} {
                \draw[thick,secondary] (0,0,0) -- (\t*1,\t*2,\t*1);
            }

            % Vector v
            \draw[thick,->,primary] (0,0,0) -- (1,2,1) node[midway,right] {$\vec{v}$};
        \end{tikzpicture}

        \caption{Span of a vector $\vec{v}$ in $\mathbb{R}^3$}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \begin{tikzpicture}[scale=1]
            % Axes
            \draw[->] (-2,0,0) -- (2,0,0) node[right] {$x_2$};
            \draw[->] (0,-2,0) -- (0,2,0) node[above] {$x_3$};
            \draw[->] (0,0,-2) -- (0,0,2) node[above] {$x_1$};

            % Span of v1 and v2
            \fill[secondary!40,opacity=0.3] (-1.8,-1.2,-1.7) -- (1,2,1) -- (2,1,1) -- (-1,-2,-1) -- cycle;
            \draw[thick,secondary] (-1.8,-1.2,-1.7) -- (1,2,1) -- (2,1,1) -- (-1,-2,-1) -- cycle;

            % Vectors v1 and v2
            \draw[thick,->,primary] (0,0,0) -- (1,2,1) node[above] {$\vec{v_1}$};
            \draw[thick,->,primary] (0,0,0) -- (2,1,1) node[right] {$\vec{v_2}$};
        \end{tikzpicture}

        \caption{Span of two vectors $\vec{v_1}$ and $\vec{v_2}$ in $\mathbb{R}^3$}
    \end{subfigure}
\end{figure}

\section{Matrix equations}
\begin{definition}[Matrix equation]
    A matrix equation is an equation of the form:
    \[
        A \cdot \vec{x} = \vec{b}
    \]
    where $A$ is a matrix of size $m \times n$ composed of $n$ vectors in $\mathbb{R}^m$, $\vec{x}$ is a vector of size $n \times 1$ (the variable) and $\vec{b}$ is a vector of size $m \times 1$ (the result). \\
    More explicitly, $A \vec{x}$ can be written as:
    \[
        A \cdot \vec{x} = \begin{bmatrix} | & | & | & | \\ \vec{a_1} & \vec{a_2} & \ldots & \vec{a_n} \\ | & | & | & | \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} + \ldots + x_n \cdot \vec{a_n}
    \]
    where $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}$ are the column vectors of $A$.
\end{definition}
The product is only defined if the number of columns of $A$ is equal to the number of coefficients of $\vec{x}$.

\begin{eg}
    Let $A = \begin{bmatrix}3 & -2 & 1 \\ 2 & 9 & 8\end{bmatrix}$ and $\vec{x} = \begin{bmatrix} 2 \\ 4 \\ 3 \end{bmatrix}$, then:
    \[
        A \cdot \vec{x} = \begin{bmatrix}3 & -2 & 1 \\ 2 & 9 & 8\end{bmatrix} \cdot \begin{bmatrix} 2 \\ 4 \\ 3 \end{bmatrix} = 2 \cdot \begin{bmatrix} 3 \\ 2 \end{bmatrix} + 4 \cdot \begin{bmatrix} -2 \\ 9 \end{bmatrix} + 3 \cdot \begin{bmatrix} 1 \\ 8 \end{bmatrix} = \begin{bmatrix} 3(2) - 2(4) + 1(3) \\ 2(2) + 9(4) + 8(3) \end{bmatrix} = \begin{bmatrix} 1 \\ 64 \end{bmatrix}
    \]
\end{eg}

\begin{eg}
    Let $A = \begin{bmatrix}a_{11} & ... & a_{1n} \\ \vdots &  & \vdots \\ a_{m1} & ... & a_{mn} \end{bmatrix}$ and $\vec{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}$, then:
    \[
        A \cdot \vec{x} = \begin{bmatrix}a_{11} & ... & a_{1n} \\ \vdots &  & \vdots \\ a_{m1} & ... & a_{mn} \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = x_1 \cdot \begin{bmatrix} a_{11} \\ \vdots \\ a_{m1} \end{bmatrix} + ... + x_n \cdot \begin{bmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{bmatrix} = \begin{bmatrix} a_{11}x_1 + ... + a_{1n}x_n \\ \vdots \\ a_{m1}x_1 + ... + a_{mn}x_n \end{bmatrix}
    \]
\end{eg}

\begin{eg}
    Let the system of equations be:
    \[
        \begin{cases}
            3x_1 - 2x_2 + x_3 = b_1 \\
            2x_1 + 9x_2 + 8x_3 = b_2
        \end{cases}
    \]
    Can be written as the matrix equation:
    \[
        \begin{bmatrix}3 & -2 & 1 \\ 2 & 9 & 8\end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix} = A \cdot \vec{x}
    \]
    where $A \in \mathbb{R}_{2 \times 3}$, $\vec{x} \in \mathbb{R}_{3 \times 1}$ and $\vec{b} \in \mathbb{R}_{2 \times 1}$.
\end{eg}

\subsection{Equivalence of matrix equations, vectorial equations and linear systems}
For a matrix $A \in \mathbb{R}_{m \times n}$ with column vectors $\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n} \in \mathbb{R}^m$ and a vector $\vec{b} \in \mathbb{R}^m$, the matrix equation 
\[
    A \cdot \vec{x} = \vec{b}
\]
is equivalent (has the same set of solutions) as the vectorial equation 
\[
    x_1 \cdot \vec{a_1} + x_2 \cdot \vec{a_2} + \ldots + x_n \cdot \vec{a_n} = \vec{b}
\]
and the linear system whose augmented matrix is
\[
    \begin{bmatrix}
    | & | & & | & | \\
    \vec{a_1} & \vec{a_2} & \ldots & \vec{a_n} & \vec{b} \\
    | & | & & | & | 
    \end{bmatrix}
\]
It can also be expressed as a linear system of equations:
\[
    \begin{cases}
        a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n = b_1 \\
        \vdots \\
        a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n = b_m
    \end{cases}
\]
If a solution (at least one) exists, it means that $\vec{b}$ can be generated by the column vectors of $A$ and thus $\vec{b} \in \text{span}\{\vec{a_1}, \vec{a_2}, \ldots, \vec{a_n}\}$.
\begin{theorem}
    Let $A$ be a matrix of size $m \times n$. The following statements are equivalent:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item For every $\vec{b} \in \mathbb{R}^m$, the equation $A \cdot \vec{x} = \vec{b}$ has at least one solution.
        \item Every $\vec{b} \in \mathbb{R}^m$ is a linear combination of the columns of $A$.
        \item The columns of $A$ span $\mathbb{R}^m$.
        \item $A$ has a pivot position in every row.
    \end{itemize}
\end{theorem}

\subsection{Properties of matrix equations}
\begin{theorem}
    Let $A$ be a matrix of size $m \times n$, $\vec{u}, \vec{v} \in \mathbb{R}^n$ and $c \in \mathbb{R}$. Then:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $A(\vec{u} + \vec{v}) = A\vec{u} + A\vec{v}$ (Distributive property over vector addition)
        \item $A(c \vec{u}) = c(A\vec{u})$ (Distributive property over scalar multiplication)
    \end{itemize}
\end{theorem}
\begin{proof}
    Let $A = \begin{bmatrix} | & | & & | \\ \vec{a_1} & \vec{a_2} & \ldots & \vec{a_n} \\ | & | & & | \end{bmatrix}$, $\vec{u} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}$ and $\vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$. \\
    $\circ$ Distributive property over vector addition:
    \begin{align*}
        A(\vec{u} + \vec{v}) &= A \cdot \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix} = (u_1 + v_1) \cdot \vec{a_1} + (u_2 + v_2) \cdot \vec{a_2} + \ldots + (u_n + v_n) \cdot \vec{a_n} \\
        &= (u_1 \cdot \vec{a_1} + u_2 \cdot \vec{a_2} + \ldots + u_n \cdot \vec{a_n}) + (v_1 \cdot \vec{a_1} + v_2 \cdot \vec{a_2} + \ldots + v_n \cdot \vec{a_n}) = A\vec{u} + A\vec{v}
    \end{align*}
    $\circ$ Distributive property over scalar multiplication:
    \begin{align*}
        A(c \vec{u}) &= A \cdot \begin{bmatrix} c u_1 \\ c u_2 \\ \vdots \\ c u_n \end{bmatrix} = (c u_1) \cdot \vec{a_1} + (c u_2) \cdot \vec{a_2} + \ldots + (c u_n) \cdot \vec{a_n} \\
        &= c (u_1 \cdot \vec{a_1} + u_2 \cdot \vec{a_2} + \ldots + u_n \cdot \vec{a_n}) = c(A\vec{u})
    \end{align*}
\end{proof}

\subsection{Homogeneous systems}
\begin{definition}
    A homogeneous system is a linear system that can be written as:
    \[
        A \cdot \vec{x} = \vec{0}
    \]
    where $A$ is a matrix of size $m \times n$ and $\vec{0}$ is the zero vector of size $m \times 1$.
\end{definition}
A homogeneous system always has at least one solution, called the trivial solution, which is $\vec{x} = \vec{0}$ (all coefficients equal to zero). If there exists another solution, it is called a non-trivial solution. If a homogeneous system has a non-trivial solution, it must have an infinity of solutions.
\begin{theorem}
    A homogeneous system $A \cdot \vec{x} = \vec{0}$ has a non-trivial solution if and only if the system has at least one free variable.
\end{theorem}