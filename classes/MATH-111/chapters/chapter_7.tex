\chapter{Spectral Theory and SVD}
Remember the following definitions:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item A matrix is diagonalizable if it can be written as $A = PDP^{-1}$ where $D$ is a diagonal matrix.
    \item A matrix is symmetric if $A = A^T$.
\end{itemize}

\section{Diagonalization of Squared Matrices}

\begin{eg}
    Let $A = \begin{bmatrix}
        6 & -2 & -1 \\
        -2 & 6 & -1 \\
        -1 & -1 & 5
    \end{bmatrix}$. Clearly $A$ is symmetric since $A = A^T$. Let's diagonalize it. Let's compute its characteristic polynomial:
    \[\begin{aligned}
        p_A(\lambda) &= \det(A - \lambda I) \\
        &= -\lambda^3 + 17 \lambda^2 - 90 \lambda + 144 \\
        &= (8 - \lambda)(6 - \lambda)(3 - \lambda)
    \end{aligned}\]
    Thus, the eigenvalues of $A$ are $\lambda_1 = 8$, $\lambda_2 = 6$ and $\lambda_3 = 3$. Let's compute the eigenvectors for each eigenvalue:
    Let's start with $\lambda_1 = 8$:
    \[
        A - 8I = \begin{bmatrix}
            -2 & -2 & -1 \\
            -2 & -2 & -1 \\
            -1 & -1 & -3
        \end{bmatrix} \sim \begin{bmatrix}
            1 & 1 & 0 \\
            0 & 0 & 1 \\
            0 & 0 & 0
        \end{bmatrix}
    \]
    Thus the eigenvector corresponding to $\lambda_1 = 8$ is $v_1 = \begin{bmatrix}
        1 \\ -1 \\ 0
    \end{bmatrix}$ (remark that instead of putting $A$ in RREF, we could have noticed that if we add the first column to the second column, we get $0$ in the second column). Now let's compute the eigenvector for $\lambda_2 = 6$:
    \[
        A - 6I = \begin{bmatrix}
            0 & -2 & -1 \\
            -2 & 0 & -1 \\
            -1 & -1 & -1
        \end{bmatrix}
    \]
    Thus the eigenvector corresponding to $\lambda_2 = 6$ is $v_2 = \begin{bmatrix}
        \frac{1}{2} \\ \frac{1}{2} \\ -1
    \end{bmatrix}$. Finally, let's compute the eigenvector for $\lambda_3 = 3$:
    \[
        A - 3I = \begin{bmatrix}
            3 & -2 & -1 \\
            -2 & 3 & -1 \\
            -1 & -1 & 2
        \end{bmatrix}
    \]
    Thus the eigenvector corresponding to $\lambda_3 = 3$ is $v_3 = \begin{bmatrix}
        1 \\ 1 \\ 1
    \end{bmatrix}$. Thus we have:
    \[
        D = \begin{bmatrix}
            8 & 0 & 0 \\
            0 & 6 & 0 \\
            0 & 0 & 3
        \end{bmatrix}, \quad P = \begin{bmatrix}
            | & | & | \\
            v_1 & v_2 & v_3 \\
            | & | & |
        \end{bmatrix} = \begin{bmatrix}
            1 & \frac{1}{2} & 1 \\
            -1 & \frac{1}{2} & 1 \\
            0 & -1 & 1
        \end{bmatrix}
    \]
    Let's check that $v_1$, $v_2$ and $v_3$ are orthogonal:
    \[\begin{aligned}
        v_1 \cdot v_2 &= 1 \cdot \frac{1}{2} + (-1) \cdot \frac{1}{2} + 0 \cdot (-1) = 0 \\
        v_1 \cdot v_3 &= 1 \cdot 1 + (-1) \cdot 1 + 0 \cdot 1 = 0 \\
        v_2 \cdot v_3 &= \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot 1 + (-1) \cdot 1 = 0
    \end{aligned}\]
    Thus, the eigenvectors are orthogonal (note that this is not due to a lucky choice of the eigenvectors). We can normalize them to get an orthonormal basis:
    \[\begin{aligned}
        u_1 &= \frac{1}{\|v_1\|} v_1 = \frac{1}{\sqrt{2}} \begin{bmatrix}
            1 \\ -1 \\ 0
        \end{bmatrix} \\
        u_2 &= \frac{1}{\|v_2\|} v_2 = \frac{\sqrt{2}}{\sqrt{3}} \begin{bmatrix}
            \frac{1}{2} \\ \frac{1}{2} \\ -1
        \end{bmatrix} = \frac{1}{\sqrt{6}} \begin{bmatrix}
            1 \\ 1 \\ -2
        \end{bmatrix} \\
        u_3 &= \frac{1}{\|v_3\|} v_3 = \frac{1}{\sqrt{3}} \begin{bmatrix}
            1 \\ 1 \\ 1
        \end{bmatrix}
    \end{aligned}\]
    Remark that each $u_i$ vector is still an eigenvector of $A$ since scaling a vector does not change its direction. Thus, we can form the matrix:
    \[
        Q = \begin{bmatrix}
            | & | & | \\
            u_1 & u_2 & u_3 \\
            | & | & |
        \end{bmatrix} = \begin{bmatrix}
            \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \\
            -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \\
            0 & -\frac{2}{\sqrt{6}} & \frac{1}{\sqrt{3}}
        \end{bmatrix}
    \]
    Since the columns of $Q$ are orthonormal, we have that $Q^{-1} = Q^T$. Thus, we can write:
    \[A = Q D Q^T\]
\end{eg}
Remark that $A$ has to be symmetric for this to work. If $A = Q D Q^T$ where $D$ is diagonal and $Q$ is orthogonal, then:
\[
    A^T = (Q D Q^T)^T = (Q^T)^T D^T Q^T = Q D Q^T = A
\]

\subsection{Eigenvalues of Symmetric Matrices}

\begin{theorem}
    The eigenvalues of a symmetric matrix are real numbers. 
\end{theorem}
\begin{proof}
    Let $A \in \mathbb{R}^{n \times n}$ be symmetric ($A = A^T$), let $\lambda \in \mathbb{C}$ be an eigenvalue of $A$ and let $\vec{v} \in \mathbb{C}^n$ be a corresponding eigenvector (i.e. $A v = \lambda v$). We want to show that $\lambda$ is a real number. Let's compute the following:
    \[
        \sum_{k = 1}^n \overline{v_k} (A \vec{v})_k = \sum_{k = 1}^{n} \sum_{l = 1}^{n} a_{k l} \overline{v_k} v_l = \underbrace{\sum_{l = 1}^{n} a_{l l} |v_l|^2}_{\in \mathbb{R}} + \underbrace{\sum_{k = 1}^{n} \sum_{l > k} a_{k l} \left(\overline{v_k} v_l + \overline{v_l} v_k\right)}_{\in \mathbb{R}}
    \]
    and:
    \[
        \sum_{k = 1}^n \overline{v_k} (A \vec{v})_k = \sum_{k = 1}^{n} \overline{v_k} (\lambda \vec{v})_k = \lambda \sum_{k = 1}^{n} \overline{v_k} v_k = \lambda \underbrace{\sum_{k = 1}^{n} |v_k|^2}_{\in \mathbb{R}^+}
    \]
    Since both expressions are equal, we have:
    \[
        \lambda \sum_{k = 1}^{n} |v_k|^2 \in \mathbb{R} \quad \implies \quad \lambda \in \mathbb{R}
    \]
\end{proof}

\subsection{Orthogonality of Eigenvectors}
\begin{theorem}
    Let $A$ be a symmetric matrix ($A = A^T$). If $v_1$ and $v_2$ are eigenvectors of $A$ corresponding to distinct eigenvalues $\lambda_1$ and $\lambda_2$, then $v_1$ and $v_2$ are orthogonal i.e:
    \[
        \lambda_1 \neq \lambda_2 \quad \implies \quad v_1 \cdot v_2 = 0
    \]
\end{theorem}
\begin{proof}
    Since $v_1$ and $v_2$ are eigenvectors of $A$, we have:
    \[
        A v_1 = \lambda_1 v_1 \quad \text{and} \quad A v_2 = \lambda_2 v_2
    \]
    Now, consider the expression $v_1 \cdot (A v_2)$. Using the second eigenvalue equation, we have:
    \[
        v_1 \cdot (A v_2) = v_1 \cdot (\lambda_2 v_2) = \lambda_2 (v_1 \cdot v_2)
    \]
    On the other hand, since $A$ is symmetric, we can also write:
    \[
        v_1 \cdot (A v_2) = (A^T v_1) \cdot v_2 = (A v_1) \cdot v_2 = (\lambda_1 v_1) \cdot v_2 = \lambda_1 (v_1 \cdot v_2)
    \]
    Equating the two expressions for $v_1 \cdot (A v_2)$, we get:
    \[
        \lambda_2 (v_1 \cdot v_2) = \lambda_1 (v_1 \cdot v_2)
    \]
    Rearranging this gives:
    \[
        (\lambda_2 - \lambda_1)(v_1 \cdot v_2) = 0
    \]
    Since $\lambda_1 \neq \lambda_2$ by assumption, it follows that:
    \[
        v_1 \cdot v_2 = 0
    \]
    Thus, the eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.
\end{proof}

\subsection{Spectral Theorem}
\begin{theorem}[Spectral Theorem]
    For all symmetric matrices $A \in \mathbb{R}^{n \times n}$, the following holds:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $A$ has $n$ distinct real eigenvalues (counting multiplicities).
        \item For each eigenvalue $\lambda_i$, the dimension of the eigenspace corresponding to $\lambda_i$ equals the multiplicity of $\lambda_i$ as a root of the characteristic polynomial of $A$.
        \item The subspaces corresponding to distinct eigenvalues are two by two orthogonal i.e. if $v_i$ and $v_j$ are eigenvectors corresponding to distinct eigenvalues $\lambda_i$ and $\lambda_j$, then $v_i \cdot v_j = 0$.
        \item $A$ is diagonalizable in an orthogonal basis.
    \end{itemize}
\end{theorem}

\begin{eg}
    Let $A = \begin{bmatrix}
        3 & -2 & 4  \\
        -2 & 6 & 2 \\
        4 & 2 & 3
    \end{bmatrix}$. Let's diagonalize it. First, let's compute its characteristic polynomial:
    \[
        p_A(\lambda) = \det(A - \lambda I) = (7 - \lambda)^2 (-2 - \lambda)
    \]
    Thus, the eigenvalues of $A$ are $\lambda_1 = \lambda_2 = 7$ and $\lambda_3 = -2$. Let's compute the eigenvectors for the eigenvalue $\lambda_3 = -2$:
    \[
        A + 2I = \begin{bmatrix}
            5 & -2 & 4 \\
            -2 & 8 & 2 \\
            4 & 2 & 5
        \end{bmatrix}
    \]
    Thus, the eigenvector corresponding to $\lambda_3 = -2$ is $v_3 = \begin{bmatrix}
        -2 \\ -1 \\ 2
    \end{bmatrix}$. Now let's compute the eigenvectors for the eigenvalue $\lambda_1 = 7$:
    \[
        A - 7I = \begin{bmatrix}
            -4 & -2 & 4 \\
            -2 & -1 & 2 \\
            4 & 2 & -4
        \end{bmatrix}
    \]
    Thus, the eigenvectors corresponding to $\lambda_1 = 7$ are $v_1 = \begin{bmatrix}
        1 \\ 0 \\ 1
    \end{bmatrix}$ and $v_2 = \begin{bmatrix}
        1 \\ -2 \\ 0
    \end{bmatrix}$. Let's check that $v_1$, $v_2$ and $v_3$ are orthogonal:
    \[\begin{aligned}
        v_1 \cdot v_2 &= 1 \cdot 1 + 0 \cdot (-2) + 1 \cdot 0 = 1 \\
        v_1 \cdot v_3 &= 1 \cdot (-2) + 0 \cdot (-1) + 1 \cdot 2 = 0 \\
        v_2 \cdot v_3 &= 1 \cdot (-2) + (-2) \cdot (-1) + 0 \cdot 2 = 0
    \end{aligned}\]
    Thus, the eigenvectors associated to distinct eigenvalues are orthogonal but $v_1$ and $v_2$ are not orthogonal. We can use the Gram-Schmidt process to orthogonalize them:
    \[\begin{aligned}
        y_1 &= v_1 = \begin{bmatrix}
            1 \\ 0 \\ 1
        \end{bmatrix} \\
        y_2 &= v_2 - \frac{v_2 \cdot y_1}{y_1 \cdot y_1} y_1 = \begin{bmatrix}
            1 \\ -2 \\ 0
        \end{bmatrix} - \frac{1}{2} \begin{bmatrix}
            1 \\ 0 \\ 1
        \end{bmatrix} = \begin{bmatrix}
            \frac{1}{2} \\ -2 \\ -\frac{1}{2}
        \end{bmatrix}
    \end{aligned}\]
    Now we can normalize them to get an orthonormal basis. Thus we have:
    \[
        A = P D P^{-1} = P D P^T
    \]
    Since the columns of $P$ are orthonormal, we have $P^{-1} = P^T$. Where:
    \[
        D = \begin{bmatrix}
            7 & 0 & 0 \\
            0 & 7 & 0 \\
            0 & 0 & -2
        \end{bmatrix}, \quad P = \begin{bmatrix}
            | & | & | \\
            u_1 & u_2 & u_3 \\
            | & | & |
        \end{bmatrix}
    \]
\end{eg}

\begin{theorem}[Spectral Decomposition]
    Let $A$ be a symmetric matrix with eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$ and corresponding orthonormal eigenvectors $u_1, u_2, \ldots, u_n$. Then, $A$ can be expressed as:
    \[
        A = \sum_{i=1}^{n} \lambda_i u_i u_i^T
    \]
    where $u_i u_i^T$ is the outer product of $u_i$ with itself.
\end{theorem}
\begin{proof}
    Since $A$ is symmetric, by the Spectral Theorem, it can be diagonalized as:
    \[
        A = Q D Q^T
    \]
    where $Q$ is the orthogonal matrix whose columns are the orthonormal eigenvectors $u_1, u_2, \ldots, u_n$, and $D$ is the diagonal matrix with the eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$ on its diagonal. We can express $D$ as:
    \[
        D = \begin{bmatrix}
            \lambda_1 & 0 & \cdots & 0 \\
            0 & \lambda_2 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \lambda_n
        \end{bmatrix}
    \]
    Thus, we can write:
    \begin{align*}
        A &= \begin{bmatrix}
            | & | & \cdots & | \\
            u_1 & u_2 & \cdots & u_n \\
            | & | & \cdots & |
        \end{bmatrix} \begin{bmatrix}
            \lambda_1 & 0 & \cdots & 0 \\
            0 & \lambda_2 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \lambda_n
        \end{bmatrix} \begin{bmatrix}
            - & u_1^T & - \\
            - & u_2^T & - \\
            \vdots & \vdots & \vdots \\
            - & u_n^T & -
        \end{bmatrix} \\
        &= \begin{bmatrix}
            | & | & \cdots & | \\
            \lambda_1 u_1 & \lambda_2 u_2 & \cdots & \lambda_n u_n \\
            | & | & \cdots & |
        \end{bmatrix} \begin{bmatrix}
            - & u_1^T & - \\
            - & u_2^T & - \\
            \vdots & \vdots & \vdots \\
            - & u_n^T & -
        \end{bmatrix} \\
        &= \sum_{i=1}^{n} \lambda_i u_i u_i^T
    \end{align*}
\end{proof}

\section{Singular Value Decomposition (SVD)}

\begin{theorem}[Singular Value Decomposition]
    For any matrix $A \in \mathbb{R}^{m \times n}$, there exist orthogonal matrices $U \in \mathbb{R}^{m \times m}$ and $V \in \mathbb{R}^{n \times n}$, and a diagonal matrix $\Sigma \in \mathbb{R}^{m \times n}$ with non-negative real numbers on the diagonal, such that:
    \[
        A = U \Sigma V^T
    \]
    The diagonal entries of $\Sigma$ are called the singular values of $A$, and the columns of $U$ and $V$ are called the left and right singular vectors of $A$, respectively.
\end{theorem}

\begin{eg}
    Let $A = \begin{bmatrix}
        4 & 11 & 14 \\ 8 & 7 & -2
    \end{bmatrix}$. Since $A$ is not symmetric, we cannot use the spectral theorem to diagonalize it but $A^T A$ is symmetric. Let's compute $A^T A$:
    \[
        A^T A = \begin{bmatrix}
            80 & 100 & 40 \\ 100 & 170 & 140 \\ 40 & 140 & 200
        \end{bmatrix}
    \]
    Thus we can diagonalize $A^T A$. Let's compute its characteristic polynomial:
    \[
        \det(A^T A - \lambda I) = -\lambda (\lambda^2 -450 \lambda + 32400)
    \]
    Thus, the eigenvalues of $A^T A$ are $\lambda_1 = 360$, $\lambda_2 = 90$ and $\lambda_3 = 0$. We can compute the eigenvectors corresponding to each eigenvalue (for brevity, we skip the calculations):
    \[\begin{aligned}
        v_1 &= \begin{bmatrix}
            1/3 \\ 2/3 \\ 2/3
        \end{bmatrix}, \quad v_2 = \begin{bmatrix}
            -2/3 \\ -1/3 \\ 2/3
        \end{bmatrix}, \quad v_3 = \begin{bmatrix}
            2/3 \\ -2/3 \\ 1/3
        \end{bmatrix}
    \end{aligned}\]
    Note that $v_1$, $v_2$ and $v_3$ are orthonormal. Now, let's compute $A \vec{v}_i$ for each $i$ and it's norm:
    \[
        A \vec{v_1} = \begin{bmatrix}
        4 & 11 & 14 \\ 8 & 7 & -2
        \end{bmatrix} \begin{bmatrix}
            1/3 \\ 2/3 \\ 2/3
        \end{bmatrix} = \begin{bmatrix}
            18 \\ 6
        \end{bmatrix}, \quad \|A v_1\|^2 = 360 = \lambda_1
    \]
    \[
        A \vec{v_2} = \begin{bmatrix}
        4 & 11 & 14 \\ 8 & 7 & -2
        \end{bmatrix} \begin{bmatrix}
            -2/3 \\ -1/3 \\ 2/3
        \end{bmatrix} = \begin{bmatrix}
            3 \\ -9
        \end{bmatrix}, \quad \|A v_2\|^2 = 90 = \lambda_2
    \]
    \[
        A \vec{v_3} = \begin{bmatrix}
        4 & 11 & 14 \\ 8 & 7 & -2
        \end{bmatrix} \begin{bmatrix}
            2/3 \\ -2/3 \\ 1/3
        \end{bmatrix} = \begin{bmatrix}
            0 \\ 0
        \end{bmatrix}, \quad \|A v_3\|^2 = 0 = \lambda_3
    \]
    Remark that this is not a coincidence because we have:
    \[
        \|A v_i\|^2 = v_i^T A^T A v_i = v_i^T (\lambda_i v_i) = \lambda_i (v_i^T v_i) = \lambda_i
    \]
    Let's now observe that the vectors $A \vec{v_i}$ and $A \vec{v_j}$ are orthogonal for $i \neq j$:
    \[\begin{aligned}
        (A v_1) \cdot (A v_2) = 0 \\
        (A v_1) \cdot (A v_3) = 0 \\
        (A v_2) \cdot (A v_3) = 0
    \end{aligned}\]
    Remark that here again this is not a coincidence because:
    \[\begin{aligned}
        (A v_i) \cdot (A v_j) &= (A v_i)^T (A v_j) = v_i^T A^T A v_j = v_i^T (\lambda_j v_j) = \lambda_j (v_i^T v_j) = 0
    \end{aligned}\]
    since $v_i$ and $v_j$ (for $i \neq j$) are orthogonal. Now, we can normalize the vectors $A v_i$ that are not zero, to get an orthonormal basis:
    \[\begin{aligned}
        u_1 &= \frac{1}{\|A v_1\|} A v_1 = \frac{1}{\sqrt{360}} \begin{bmatrix}
            18 \\ 6
        \end{bmatrix} = \begin{bmatrix}
            \frac{3}{\sqrt{10}} \\ \frac{1}{\sqrt{10}}
        \end{bmatrix} \\
        u_2 &= \frac{1}{\|A v_2\|} A v_2 = \frac{1}{\sqrt{90}} \begin{bmatrix}
            3 \\ -9
        \end{bmatrix} = \begin{bmatrix}
            \frac{1}{\sqrt{10}} \\ -\frac{3}{\sqrt{10}}
        \end{bmatrix}
    \end{aligned}\]
    Now, we can find the singular values of $A$:
    \[\begin{aligned}
        \sigma_1 &= \|A v_1\| = \sqrt{360} = 6 \sqrt{10} \\
        \sigma_2 &= \|A v_2\| = \sqrt{90} = 3 \sqrt{10} \\
    \end{aligned}\]
    Thus, we have:
    \[
        A \vec{v_1} = \sigma_1 \vec{u_1}, \quad A \vec{v_2} = \sigma_2 \vec{u_2}, \quad A \vec{v_3} = 0
    \]
    We can now form the matrices:
    \[\begin{aligned}
        U &= \begin{bmatrix}
            | & | \\
            \vec{u_1} & \vec{u_2} \\
            | & |
        \end{bmatrix} = \frac{1}{\sqrt{10}}\begin{bmatrix}
            3 & 1 \\
            1 & -3
        \end{bmatrix} \\
        V &= \begin{bmatrix}
            | & | & | \\
            \vec{v_1} & \vec{v_2} & \vec{v_3} \\
            | & | & |
        \end{bmatrix} = \frac{1}{3}\begin{bmatrix}
            1 & -2 & 2 \\
            2 & -1 & -2 \\
            2 & 2 & 1
        \end{bmatrix} \\
        \Sigma &= \begin{bmatrix}
            \sigma_1 & 0 & 0 \\
            0 & \sigma_2 & 0
        \end{bmatrix} = \begin{bmatrix}
            6 \sqrt{10} & 0 & 0 \\
            0 & 3 \sqrt{10} & 0
        \end{bmatrix}
    \end{aligned}\]
    Thus, we have:
    \[
        AV = A \begin{bmatrix}
            | & | & | \\
            \vec{v_1} & \vec{v_2} & \vec{v_3} \\
            | & | & |
        \end{bmatrix} = \begin{bmatrix}
            | & | & | \\
            A\vec{v_1} & A\vec{v_2} & A\vec{v_3} \\
            | & | & |
        \end{bmatrix} = \begin{bmatrix}
            | & | & | \\
            \sigma_1 \vec{u_1} & \sigma_2 \vec{u_2} & 0 \\
            | & | & |
        \end{bmatrix} = U \Sigma
    \]
    Since $V$ is orthogonal, we have $V^T V = I$. Thus, we can write:
    \[
        A = U \Sigma V^T
    \]
\end{eg}
More generally, if $A$ is an $m \times n$ matrix of rank $r$, then:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item The symmetric matrix $A^T A$ of size $n \times n$ has $n$ real non-negative eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$ (counting multiplicities) and corresponding orthonormal eigenvectors $v_1, v_2, \ldots, v_n$ (let's choose to order the eigenvalues such that $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n \geq 0$).
    \item Since the eigenvalues are non-negative, we can define the singular values of $A$ as $\sigma_i = \sqrt{\lambda_i}$ for $i = 1, 2, \ldots, n$.
    \item It is possible that some of the singular values are zero. In fact, since the rank of $A$ is $r$, we have $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r > 0$ and $\sigma_{r+1} = \sigma_{r+2} = \ldots = \sigma_n = 0$.
    \item For each $i = 1, 2, \ldots, r$, we can compute the vectors $u_i = \frac{1}{\sigma_i} A v_i$. The vectors $u_1, u_2, \ldots, u_r$ are orthonormal.
    \item We can extend the set $\{u_1, u_2, \ldots, u_r\}$ to an orthonormal basis of $\mathbb{R}^m$ by adding vectors $u_{r+1}, u_{r+2}, \ldots, u_m$ (note that we can use the Gram-Schmidt process).
    \item We can then form the matrices:
    \[\begin{aligned}
        U &= \begin{bmatrix}
            | & | & & | \\
            u_1 & u_2 & \cdots & u_m \\
            | & | & & |
        \end{bmatrix} \in \mathbb{R}^{m \times m} \\
        V &= \begin{bmatrix}
            | & | & & | \\
            v_1 & v_2 & \cdots & v_n \\
            | & | & & |
        \end{bmatrix} \in \mathbb{R}^{n \times n} \\
        \Sigma &= \begin{bmatrix}
            \sigma_1 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
            0 & \sigma_2 & \cdots & 0 & 0 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots & \vdots & & \vdots \\
            0 & 0 & \cdots & \sigma_r & 0 & \cdots & 0 \\
            0 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
            \vdots & \vdots & & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & 0 & 0 & \cdots & 0
        \end{bmatrix} \in \mathbb{R}^{m \times n}
    \end{aligned}\]
    \item Finally, we have:
    \[
        A = U \Sigma V^T
    \]
\end{itemize}
Remark that the decomposition can also be written as:
\[
    A = U \Sigma V^T = \sum_{i=1}^{r} \sigma_i \vec{u_i} \vec{v_i}^T
\]
Thus for any $\vec{x} \in \mathbb{R}^n$, we have:
\[
    A \vec{x} = \sum_{i=1}^{r} \sigma_i \vec{u_i} (\vec{v_i}^T \vec{x}) = \sum_{i=1}^{r} \sigma_i (\vec{v_i} \cdot \vec{x}) \vec{u_i}
\]
This shows that the image of $A$ is spanned by the left singular vectors $\vec{u_1}, \vec{u_2}, \ldots, \vec{u_r}$ (i.e. $\text{Im} A \subseteq \{\vec{u_1}, \vec{u_2}, \ldots, \vec{u_r} \}$). The inclusion can be shown to be an equality since for every $i$ in $1, 2, \ldots, r$, we have:
\[
    \vec{u_i} = \frac{1}{\sigma_i}A \vec{v_i} = A \left(\frac{1}{\sigma_i} \vec{v_i}\right)
\]
Thus, $\vec{u_i} \in \text{Im} A$ for every $i$ in $1, 2, \ldots, r$. We can conclude that the set $\{\vec{u_1}, \vec{u_2}, \ldots, \vec{u_r} \}$ forms an orthonormal basis of $\text{Im} A$.

\subsection{Consequences of SVD}
When decomposing $A$ as $A = U \Sigma V^T$, we have:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item The set of vectors $\{\vec{u_1}, \ldots, \vec{u_r}\}$ forms an orthonormal basis of $\text{Im} A$.
    \item $\text{rank} A = r =$ number of non-zero singular values of $A$.
    \item The set of vectors $\{\vec{u_{r + 1}}, \ldots, \vec{u_{m}}\}$ forms an orthonormal basis of $\text{Ker} A^T = (\text{Im A})^\perp$.
    \item $A = \sum_{i=1}^{r} \sigma_i \vec{u_i} \vec{v_i}^T$, thus $A^T = \sum_{i=1}^{r} \sigma_i \vec{v_i} \vec{u_i}^T$.
    \item The set of vectors $\{\vec{v_1}, \ldots, \vec{v_r}\}$ forms an orthonormal basis of $\text{Im} A^T = (\text{Ker A})^\perp$.
    \item The set of vectors $\{\vec{v_{r + 1}}, \ldots, \vec{v_{n}}\}$ forms an orthonormal basis of $\text{Ker} A = (\text{Im} A^T)^\perp$.
\end{itemize}

\subsection{General Cases with SVD Decomposition}
In all the next examples, the matrix $A$ is decomposed as $A = U \Sigma V^T$ using the SVD decomposition.

\begin{eg}
    In the most general case, where no condition is put on $A$, we have:
    \[
        A\vec{x} = \vec{b} \quad \iff \quad U \Sigma V^T \vec{x} = \vec{b} \quad \iff \quad \Sigma \underbrace{(V^T \vec{x})}_{\vec{y}} = \underbrace{(U^T \vec{b})}_{\vec{c}} \quad \iff \quad \Sigma \vec{y} = \vec{c} \text{ and } \vec{x} = V \vec{y}
    \]
    We can write this as a system of equations:
    \[
        \begin{cases*}
            \sigma_1 y_1 = c_1 \\
            \sigma_2 y_2 = c_2 \\
            \vdots \\
            \sigma_r y_r = c_r \\
            0 = c_{r+1} \\
            \vdots \\
            0 = c_m
        \end{cases*}
    \]
    Thus, a solution exists if and only if $c_{r+1} = c_{r+2} = \ldots = c_m = 0$ (i.e. $\vec{b} \in \text{Im} A$). In this case, we can find a particular solution $\vec{y_p}$ by setting:
    \[\begin{aligned}
        y_i &= \frac{c_i}{\sigma_i} \quad \text{for } i = 1, 2, \ldots, r \\
        y_i &= 0 \quad \text{for } i = r+1, r+2, \ldots, n
    \end{aligned}\]
    The general solution is then given by:
    \[
        \vec{x} = V \vec{y_p} + \vec{w}
    \]
    where $\vec{w} \in \text{Ker} A$ (i.e. $\vec{w}$ is any linear combination of the vectors $\vec{v_{r+1}}, \vec{v_{r+2}}, \ldots, \vec{v_n}$).
\end{eg}

\begin{eg}
    Let's now consider the case with least squares solutions. We want to solve:
    \[
        A^T A \vec{x} = A^T \vec{b} \quad \iff \quad V \Sigma^T U^T U \Sigma V^T \vec{x} = V \Sigma^T U^T \vec{b} \quad \iff \quad \Sigma^T \Sigma \underbrace{V^T \vec{x}}_{\vec{y}} = \Sigma^T \underbrace{U^T \vec{b}}_{\vec{c}}
    \]
    We can write this as a system of equations:
    \[
        \begin{cases*}
            \sigma_1^2 y_1 = \sigma_1 c_1 \\
            \sigma_2^2 y_2 = \sigma_2 c_2 \\
            \vdots \\
            \sigma_r^2 y_r = \sigma_r c_r \\
            0 = 0 \\
            \vdots \\
            0 = 0
        \end{cases*}
    \]
    Thus, we can find a particular solution $\vec{y_p}$ by setting:
    \[\begin{aligned}
        y_i &= \frac{c_i}{\sigma_i} \quad \text{for } i = 1, 2, \ldots, r \\
        y_i &= 0 \quad \text{for } i = r+1, r+2, \ldots, n
    \end{aligned}\]
    The general solution is then given by:
    \[
        \vec{x} = V \vec{y_p} + \vec{w}
    \]
    where $\vec{w} \in \text{Ker} A$ (i.e. $\vec{w}$ is any linear combination of the vectors $\vec{v_{r+1}}, \vec{v_{r+2}}, \ldots, \vec{v_n}$).
\end{eg}

\begin{eg}
    Let's now consider the case where $A$ is a square matrix. We can compute its determinant:
    \[
        \det(A) = \det(U \Sigma V^T) = \det(U) \det(\Sigma) \det(V^T) = \det(U) \det(V) \prod_{i=1}^{n} \sigma_i = \pm \prod_{i=1}^{n} \sigma_i
    \]
    since both $U$ and $V$ are orthogonal matrices (thus their determinants are either $1$ or $-1$). We can conclude that $A$ is invertible if and only if all its singular values are non-zero (i.e. $\sigma_i \neq 0$ for all $i = 1, 2, \ldots, n$). In this case, we can compute the inverse of $A$:
    \[
        A^{-1} = (U \Sigma V^T)^{-1} = V \Sigma^{-1} U^T
    \]
    where:
    \[
        \Sigma^{-1} = \begin{bmatrix}
            \frac{1}{\sigma_1} & 0 & \cdots & 0 \\
            0 & \frac{1}{\sigma_2} & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & \frac{1}{\sigma_n}
        \end{bmatrix}
    \]
    and thus the singular values of $A^{-1}$ are $\frac{1}{\sigma_1}, \frac{1}{\sigma_2}, \ldots, \frac{1}{\sigma_n}$.
\end{eg}