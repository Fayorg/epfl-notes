\chapter{Spectral Theory and SVD}
Remember the following definitions:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item A matrix is diagonalizable if it can be written as $A = PDP^{-1}$ where $D$ is a diagonal matrix.
    \item A matrix is symmetric if $A = A^T$.
\end{itemize}

\section{Diagonalization of Squared Matrices}

\begin{eg}
    Let $A = \begin{bmatrix}
        6 & -2 & -1 \\
        -2 & 6 & -1 \\
        -1 & -1 & 5
    \end{bmatrix}$. Clearly $A$ is symmetric since $A = A^T$. Let's diagonalize it. Let's compute its characteristic polynomial:
    \[\begin{aligned}
        p_A(\lambda) &= \det(A - \lambda I) \\
        &= -\lambda^3 + 17 \lambda^2 - 90 \lambda + 144 \\
        &= (8 - \lambda)(6 - \lambda)(3 - \lambda)
    \end{aligned}\]
    Thus, the eigenvalues of $A$ are $\lambda_1 = 8$, $\lambda_2 = 6$ and $\lambda_3 = 3$. Let's compute the eigenvectors for each eigenvalue:
    Let's start with $\lambda_1 = 8$:
    \[
        A - 8I = \begin{bmatrix}
            -2 & -2 & -1 \\
            -2 & -2 & -1 \\
            -1 & -1 & -3
        \end{bmatrix} \sim \begin{bmatrix}
            1 & 1 & 0 \\
            0 & 0 & 1 \\
            0 & 0 & 0
        \end{bmatrix}
    \]
    Thus the eigenvector corresponding to $\lambda_1 = 8$ is $v_1 = \begin{bmatrix}
        1 \\ -1 \\ 0
    \end{bmatrix}$ (remark that instead of putting $A$ in RREF, we could have noticed that if we add the first column to the second column, we get $0$ in the second column). Now let's compute the eigenvector for $\lambda_2 = 6$:
    \[
        A - 6I = \begin{bmatrix}
            0 & -2 & -1 \\
            -2 & 0 & -1 \\
            -1 & -1 & -1
        \end{bmatrix}
    \]
    Thus the eigenvector corresponding to $\lambda_2 = 6$ is $v_2 = \begin{bmatrix}
        \frac{1}{2} \\ \frac{1}{2} \\ -1
    \end{bmatrix}$. Finally, let's compute the eigenvector for $\lambda_3 = 3$:
    \[
        A - 3I = \begin{bmatrix}
            3 & -2 & -1 \\
            -2 & 3 & -1 \\
            -1 & -1 & 2
        \end{bmatrix}
    \]
    Thus the eigenvector corresponding to $\lambda_3 = 3$ is $v_3 = \begin{bmatrix}
        1 \\ 1 \\ 1
    \end{bmatrix}$. Thus we have:
    \[
        D = \begin{bmatrix}
            8 & 0 & 0 \\
            0 & 6 & 0 \\
            0 & 0 & 3
        \end{bmatrix}, \quad P = \begin{bmatrix}
            | & | & | \\
            v_1 & v_2 & v_3 \\
            | & | & |
        \end{bmatrix} = \begin{bmatrix}
            1 & \frac{1}{2} & 1 \\
            -1 & \frac{1}{2} & 1 \\
            0 & -1 & 1
        \end{bmatrix}
    \]
    Let's check that $v_1$, $v_2$ and $v_3$ are orthogonal:
    \[\begin{aligned}
        v_1 \cdot v_2 &= 1 \cdot \frac{1}{2} + (-1) \cdot \frac{1}{2} + 0 \cdot (-1) = 0 \\
        v_1 \cdot v_3 &= 1 \cdot 1 + (-1) \cdot 1 + 0 \cdot 1 = 0 \\
        v_2 \cdot v_3 &= \frac{1}{2} \cdot 1 + \frac{1}{2} \cdot 1 + (-1) \cdot 1 = 0
    \end{aligned}\]
    Thus, the eigenvectors are orthogonal (note that this is not due to a lucky choice of the eigenvectors). We can normalize them to get an orthonormal basis:
    \[\begin{aligned}
        u_1 &= \frac{1}{\|v_1\|} v_1 = \frac{1}{\sqrt{2}} \begin{bmatrix}
            1 \\ -1 \\ 0
        \end{bmatrix} \\
        u_2 &= \frac{1}{\|v_2\|} v_2 = \frac{\sqrt{2}}{\sqrt{3}} \begin{bmatrix}
            \frac{1}{2} \\ \frac{1}{2} \\ -1
        \end{bmatrix} = \frac{1}{\sqrt{6}} \begin{bmatrix}
            1 \\ 1 \\ -2
        \end{bmatrix} \\
        u_3 &= \frac{1}{\|v_3\|} v_3 = \frac{1}{\sqrt{3}} \begin{bmatrix}
            1 \\ 1 \\ 1
        \end{bmatrix}
    \end{aligned}\]
    Remark that each $u_i$ vector is still an eigenvector of $A$ since scaling a vector does not change its direction. Thus, we can form the matrix:
    \[
        Q = \begin{bmatrix}
            | & | & | \\
            u_1 & u_2 & u_3 \\
            | & | & |
        \end{bmatrix} = \begin{bmatrix}
            \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \\
            -\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \\
            0 & -\frac{2}{\sqrt{6}} & \frac{1}{\sqrt{3}}
        \end{bmatrix}
    \]
    Since the columns of $Q$ are orthonormal, we have that $Q^{-1} = Q^T$. Thus, we can write:
    \[A = Q D Q^T\]
\end{eg}
Remark that $A$ has to be symmetric for this to work. If $A = Q D Q^T$ where $D$ is diagonal and $Q$ is orthogonal, then:
\[
    A^T = (Q D Q^T)^T = (Q^T)^T D^T Q^T = Q D Q^T = A
\]

\subsection{Eigenvalues of Symmetric Matrices}

\begin{theorem}
    The eigenvalues of a symmetric matrix are real numbers. 
\end{theorem}
\begin{proof}
    Let $A \in \mathbb{R}^{n \times n}$ be symmetric ($A = A^T$), let $\lambda \in \mathbb{C}$ be an eigenvalue of $A$ and let $\vec{v} \in \mathbb{C}^n$ be a corresponding eigenvector (i.e. $A v = \lambda v$). We want to show that $\lambda$ is a real number. Let's compute the following:
    \[
        \sum_{k = 1}^n \overline{v_k} (A \vec{v})_k = \sum_{k = 1}^{n} \sum_{l = 1}^{n} a_{k l} \overline{v_k} v_l = \underbrace{\sum_{l = 1}^{n} a_{l l} |v_l|^2}_{\in \mathbb{R}} + \underbrace{\sum_{k = 1}^{n} \sum_{l > k} a_{k l} \left(\overline{v_k} v_l + \overline{v_l} v_k\right)}_{\in \mathbb{R}}
    \]
    and:
    \[
        \sum_{k = 1}^n \overline{v_k} (A \vec{v})_k = \sum_{k = 1}^{n} \overline{v_k} (\lambda \vec{v})_k = \lambda \sum_{k = 1}^{n} \overline{v_k} v_k = \lambda \underbrace{\sum_{k = 1}^{n} |v_k|^2}_{\in \mathbb{R}^+}
    \]
    Since both expressions are equal, we have:
    \[
        \lambda \sum_{k = 1}^{n} |v_k|^2 \in \mathbb{R} \quad \implies \quad \lambda \in \mathbb{R}
    \]
\end{proof}

\subsection{Orthogonality of Eigenvectors}
\begin{theorem}
    Let $A$ be a symmetric matrix ($A = A^T$). If $v_1$ and $v_2$ are eigenvectors of $A$ corresponding to distinct eigenvalues $\lambda_1$ and $\lambda_2$, then $v_1$ and $v_2$ are orthogonal i.e:
    \[
        \lambda_1 \neq \lambda_2 \quad \implies \quad v_1 \cdot v_2 = 0
    \]
\end{theorem}
\begin{proof}
    Since $v_1$ and $v_2$ are eigenvectors of $A$, we have:
    \[
        A v_1 = \lambda_1 v_1 \quad \text{and} \quad A v_2 = \lambda_2 v_2
    \]
    Now, consider the expression $v_1 \cdot (A v_2)$. Using the second eigenvalue equation, we have:
    \[
        v_1 \cdot (A v_2) = v_1 \cdot (\lambda_2 v_2) = \lambda_2 (v_1 \cdot v_2)
    \]
    On the other hand, since $A$ is symmetric, we can also write:
    \[
        v_1 \cdot (A v_2) = (A^T v_1) \cdot v_2 = (A v_1) \cdot v_2 = (\lambda_1 v_1) \cdot v_2 = \lambda_1 (v_1 \cdot v_2)
    \]
    Equating the two expressions for $v_1 \cdot (A v_2)$, we get:
    \[
        \lambda_2 (v_1 \cdot v_2) = \lambda_1 (v_1 \cdot v_2)
    \]
    Rearranging this gives:
    \[
        (\lambda_2 - \lambda_1)(v_1 \cdot v_2) = 0
    \]
    Since $\lambda_1 \neq \lambda_2$ by assumption, it follows that:
    \[
        v_1 \cdot v_2 = 0
    \]
    Thus, the eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.
\end{proof}

\subsection{Spectral Theorem}
\begin{theorem}[Spectral Theorem]
    For all symmetric matrices $A \in \mathbb{R}^{n \times n}$, the following holds:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $A$ has $n$ distinct real eigenvalues (counting multiplicities).
        \item For each eigenvalue $\lambda_i$, the dimension of the eigenspace corresponding to $\lambda_i$ equals the multiplicity of $\lambda_i$ as a root of the characteristic polynomial of $A$.
        \item The subspaces corresponding to distinct eigenvalues are two by two orthogonal i.e. if $v_i$ and $v_j$ are eigenvectors corresponding to distinct eigenvalues $\lambda_i$ and $\lambda_j$, then $v_i \cdot v_j = 0$.
        \item $A$ is diagonalizable in an orthogonal basis.
    \end{itemize}
\end{theorem}

\begin{eg}
    Let $A = \begin{bmatrix}
        3 & -2 & 4  \\
        -2 & 6 & 2 \\
        4 & 2 & 3
    \end{bmatrix}$. Let's diagonalize it. First, let's compute its characteristic polynomial:
    \[
        p_A(\lambda) = \det(A - \lambda I) = (7 - \lambda)^2 (-2 - \lambda)
    \]
    Thus, the eigenvalues of $A$ are $\lambda_1 = \lambda_2 = 7$ and $\lambda_3 = -2$. Let's compute the eigenvectors for the eigenvalue $\lambda_3 = -2$:
    \[
        A + 2I = \begin{bmatrix}
            5 & -2 & 4 \\
            -2 & 8 & 2 \\
            4 & 2 & 5
        \end{bmatrix}
    \]
    Thus, the eigenvector corresponding to $\lambda_3 = -2$ is $v_3 = \begin{bmatrix}
        -2 \\ -1 \\ 2
    \end{bmatrix}$. Now let's compute the eigenvectors for the eigenvalue $\lambda_1 = 7$:
    \[
        A - 7I = \begin{bmatrix}
            -4 & -2 & 4 \\
            -2 & -1 & 2 \\
            4 & 2 & -4
        \end{bmatrix}
    \]
    Thus, the eigenvectors corresponding to $\lambda_1 = 7$ are $v_1 = \begin{bmatrix}
        1 \\ 0 \\ 1
    \end{bmatrix}$ and $v_2 = \begin{bmatrix}
        1 \\ -2 \\ 0
    \end{bmatrix}$. Let's check that $v_1$, $v_2$ and $v_3$ are orthogonal:
    \[\begin{aligned}
        v_1 \cdot v_2 &= 1 \cdot 1 + 0 \cdot (-2) + 1 \cdot 0 = 1 \\
        v_1 \cdot v_3 &= 1 \cdot (-2) + 0 \cdot (-1) + 1 \cdot 2 = 0 \\
        v_2 \cdot v_3 &= 1 \cdot (-2) + (-2) \cdot (-1) + 0 \cdot 2 = 0
    \end{aligned}\]
    Thus, the eigenvectors associated to distinct eigenvalues are orthogonal but $v_1$ and $v_2$ are not orthogonal. We can use the Gram-Schmidt process to orthogonalize them:
    \[\begin{aligned}
        y_1 &= v_1 = \begin{bmatrix}
            1 \\ 0 \\ 1
        \end{bmatrix} \\
        y_2 &= v_2 - \frac{v_2 \cdot y_1}{y_1 \cdot y_1} y_1 = \begin{bmatrix}
            1 \\ -2 \\ 0
        \end{bmatrix} - \frac{1}{2} \begin{bmatrix}
            1 \\ 0 \\ 1
        \end{bmatrix} = \begin{bmatrix}
            \frac{1}{2} \\ -2 \\ -\frac{1}{2}
        \end{bmatrix}
    \end{aligned}\]
    Now we can normalize them to get an orthonormal basis.
\end{eg}