\chapter{Function of Real Numbers}

\section{Functions}
\begin{definition}[Real Function]
    A function $f : E \to F$, where $E$ and $F$ are subsets of $\mathbb{R}$, is a rule that assigns to each element $x \in E$ a unique element $f(x) \in F$. The set $E = \text{D}(f)$ is called the domain of the function, and the set $F$ is called the codomain.
\end{definition}

\begin{definition}[Graph of a Function]
    The graph of a function $f : E \to F$ is the set of points in the Cartesian plane defined by:
    \[
        G(f) = \{(x, f(x)) \mid x \in E\}
    \]
\end{definition}
\begin{eg}
    The graph of the function $f(x) = x^2$ is the set of points:
    \[
        G(f) = \{(x, x^2) \mid x \in \mathbb{R}\}
    \]
    Thus:
    \begin{center}
        \begin{tikzpicture}[scale=1.5]
            \draw[->] (-2, 0) -- (2, 0) node[right] {$x$};
            \draw[->] (0, -0.5) -- (0, 2.75) node[above] {$y$};
            \draw[thick, primary, domain=-1.5:1.5, samples=50] plot (\x, {(\x)^2});
        \end{tikzpicture}
    \end{center}
\end{eg}

\subsection{Properties of Functions}
% \begin{definition}[Growing Function]
%     A function $f : E \to \mathbb{R}$ is said to be increasing ($f(x) \uparrow$) on an interval $I \subseteq E$ if for all $x_1, x_2 \in I$ such that $x_1 < x_2$, we have:
%     \[
%         f(x_1) \leq f(x_2)
%     \]
%     If the inequality is strict, i.e., $f(x_1) < f(x_2)$, then $f$ is said to be strictly increasing on $I$.
% \end{definition}

% \begin{definition}[Decreasing Function]
%     A function $f : E \to \mathbb{R}$ is said to be decreasing ($f(x) \downarrow$) on an interval $I \subseteq E$ if for all $x_1, x_2 \in I$ such that $x_1 < x_2$, we have:
%     \[
%         f(x_1) \geq f(x_2)
%     \]
%     If the inequality is strict, i.e., $f(x_1) > f(x_2)$, then $f$ is said to be strictly decreasing on $I$.
% \end{definition}

\begin{definition}[Increasing and Decreasing Functions]
    A function $f : E \to \mathbb{R}$ is said to be increasing on an interval $I \subseteq E$ if for all $x_1, x_2 \in I$ such that $x_1 < x_2$, we have:
    \[
        f(x_1) \leq f(x_2)
    \]
    If the inequality is strict, i.e., $f(x_1) < f(x_2)$, then $f$ is said to be strictly increasing on $I$. \\
    Similarly, a function $f : E \to \mathbb{R}$ is said to be decreasing on an interval $I \subseteq E$ if for all $x_1, x_2 \in I$ such that $x_1 < x_2$, we have:
    \[
        f(x_1) \geq f(x_2)
    \]
    If the inequality is strict, i.e., $f(x_1) > f(x_2)$, then $f$ is said to be strictly decreasing on $I$.
\end{definition}
Note that a function (strictly) increasing or (strictly) decreasing on an interval $I$ is also called (strictly) monotonic on $I$.

\begin{definition}[Even and Odd Functions]
    A function $f : E \to \mathbb{R}$ is said to be even if for all $x \in E$, we have:
    \[
        f(-x) = f(x)
    \]
    A function $f : E \to \mathbb{R}$ is said to be odd if for all $x \in E$, we have:
    \[
        f(-x) = -f(x)
    \]
    % TODO: Add graphs for examples of even and odd functions
\end{definition}
\begin{eg}
    The function $f(x) = x^2$ is even since for all $x \in \mathbb{R}$, we have:
    \[
        f(-x) = (-x)^2 = x^2 = f(x)
    \]
    Graphically, even functions are symmetric with respect to the y-axis.
    \begin{center}
        \begin{tikzpicture}[scale=1.5]
            \draw[->] (-2, 0) -- (2, 0) node[right] {$x$};
            \draw[->, secondary, thick] (0, -0.5) -- (0, 2.75) node[above] {$y$};
            \draw[thick, primary, domain=-1.5:1.5, samples=50] plot (\x, {(\x)^2});
        \end{tikzpicture}
    \end{center}
    The function $g(x) = x^3$ is odd since for all $x \in \mathbb{R}$, we have:
    \[
        g(-x) = (-x)^3 = -x^3 = -g(x)
    \]
    Graphically, odd functions are symmetric with respect to the origin.
    \begin{center}
        \begin{tikzpicture}[scale=1.5]
            \draw[->] (-2, 0) -- (2, 0) node[right] {$x$};
            \draw[->] (0, -2) -- (0, 2) node[above] {$y$};
            \draw[thick, primary, domain=-1.2:1.2, samples=50] plot (\x, {(\x)^3});
            \draw[secondary, thick, domain=-1.7:1.7, samples=2] plot (\x, {\x});
        \end{tikzpicture}
    \end{center}
\end{eg}

\begin{definition}[Periodic Function]
    A function $f : E \to \mathbb{R}$ is said to be periodic with period $T > 0$ if for all $x \in E$ such that $x \pm T \in E$, we have:
    \[
        f(x \pm T) = f(x)
    \]
\end{definition}
\begin{eg}
    The function $f(x) = \sin(x)^2$ is periodic with period $2\pi$, since we have:
    \[
        \sin(x)^2 = \left(\frac{e^{ix} - e^{-ix}}{2i}\right)^2 = \frac{e^{2ix} - 2 + e^{-2ix}}{-4} = \frac{1}{2} - \frac{1}{2}\cos(2x)
    \]
    Since $\cos(x)$ is periodic with period $2\pi$, we have that $\sin(x)^2$ is also periodic with a period of $\pi$ (or $\sin(x)^2$ is $\pi$-periodic).
    Graphically, we have:
    \begin{center}
        \begin{tikzpicture}[scale=1.5]
            \draw[->] (-2, 0) -- (2, 0) node[right] {$x$};
            \draw[->] (0, -0.5) -- (0, 1.5) node[above] {$y$};
            \draw[thick, primary, domain=-1.5:1.5, samples=150] plot (\x, {sin(deg(\x * 4))^2});

            \draw[<->, secondary, thick] (-0.8, -0.1) -- (0, -0.1) node[midway, below] {$\pi$};
            \draw[<->, secondary, thick] (0.8, -0.1) -- (0, -0.1) node[midway, below] {$\pi$};
        \end{tikzpicture}
    \end{center}
\end{eg}
\begin{eg}
    Some functions are periodic but it's impossible to find the smallest period. For example, the function:
    \[
        f(x) = \begin{cases}
            0 & \text{if } x \in \mathbb{Q} \\
            1 & \text{if } x \notin \mathbb{Q}
        \end{cases}
    \]
    In other words, for any rational number $P$:
    \[
        \begin{cases}
            \text{rational} + P = \text{rational} \\
            \text{irrational} + P = \text{irrational}
        \end{cases}
    \]
    Thus, $f(x + P) = f(x)$ for any rational number $P$, making $f$ a periodic function without a smallest period.
\end{eg}

\subsection{Boundedness and Extrema of Functions}
\begin{definition}[Bounded Function]
    A function $f : E \to \mathbb{R}$ is said to be bounded above on a set $A \subseteq E$ if the set $f(A) \subset \mathbb{R}$ is bounded above, i.e., there exists a real number $M$ such that for all $x \in A$, we have:
    \[
        f(x) \leq M
    \]
    Similarly, $f$ is said to be bounded below on $A$ if the set $f(A) \subset \mathbb{R}$ is bounded below, i.e., there exists a real number $m$ such that for all $x \in A$, we have:
    \[
        f(x) \geq m
    \]
    If $f$ is both bounded above and bounded below on $A$, then it is said to be bounded on $A$.
\end{definition}

\begin{definition}[Supremum and Infimum]
    A function $f : E \to \mathbb{R}$ has a supremum (least upper bound) on a set $A \subseteq E$, denoted by $\sup_{x \in A} f(x)$, if the set $f(A) \subset \mathbb{R}$ has a supremum. This means that:
    \[
        \sup_{x \in A} f(x) = \sup \{f(x), x \in A\}
    \]
    Similarly, $f$ has an infimum (greatest lower bound) on $A$, denoted by $\inf_{x \in A} f(x)$, if the set $f(A) \subset \mathbb{R}$ has an infimum. This means that:
    \[
        \inf_{x \in A} f(x) = \inf \{f(x), x \in A\}
    \]
\end{definition}
\begin{eg}
    Let $f: (0,1) \to \mathbb{R}$ defined by $f(x) = x^2 + 3$ be bounded on $A = (0,1)$. Then:
    \[
        \sup_{x \in A} f(x) = \sup \{x^2 + 3, x \in A\} = 4 \notin f(A)
    \]
    Similarly:
    \[
        \inf_{x \in A} f(x) = \inf \{x^2 + 3, x \in A\} = 3 \notin f(A)
    \]
\end{eg}

\begin{definition}[Local Maximum and Minimum]
    A function $f: E \to \mathbb{R}$ has a local maximum at a point $x_0 \in E$ if there exists $\delta > 0$ such that for all $x \in E$ with $|x - x_0| < \delta$, we have:
    \[
        f(x) \leq f(x_0)
    \]
    Similarly, $f$ has a local minimum at a point $x_0 \in E$ if there exists $\delta > 0$ such that for all $x \in E$ with $|x - x_0| < \delta$, we have:
    \[
        f(x) \geq f(x_0)
    \]
\end{definition}

\begin{definition}[Global Maximum and Minimum]
    A function $f: E \to \mathbb{R}$ has a global maximum at a point $x_0 \in E$ if for all $x \in E$, we have:
    \[
        f(x) \leq f(x_0)
    \]
    Similarly, $f$ has a global minimum at a point $x_0 \in E$ if for all $x \in E$, we have:
    \[
        f(x) \geq f(x_0)
    \]
\end{definition}
\begin{eg}
    Let's show the difference between local and global extrema graphically:
    \begin{center}
        \begin{tikzpicture}[scale=1.3]
            \draw[->] (-0.5, 0) -- (3.5, 0) node[right] {$x$};
            \draw[->] (0, -0.5) -- (0, 2.5) node[above] {$y$};
            \draw[thick, primary, domain=0:2.5, samples=100] plot (\x, {-(\x - 1)^2 + 2});

            \filldraw[secondary, thick] (1, 2) circle (1.5pt) node[above right] {Global Max};
            \filldraw[secondary, thick] (0, 1) circle (1.5pt) node[above left] {Local Min};
            \filldraw[secondary, thick] (2.49, -0.2) circle (1.5pt) node[below right] {Global Min};
        \end{tikzpicture}
    \end{center}
\end{eg}
If the $\max_{x \in E} f(x)$ (or $\min_{x \in E} f(x)$) exists, then $f$ is bounded above (or below) on $E$ and $\sup_{x \in E} f(x) = \max_{x \in E} f(x)$ (or $\inf_{x \in E} f(x) = \min_{x \in E} f(x)$). \\
A bounded function on $E$ does not necessarily reach its bounds, i.e., the maximum or minimum may not exist.
\begin{eg}
    Let $f: [0, 1) \to \mathbb{R}$ defined by $f(x) = x^2 + 3$. We clearly see that $f$ is bounded but:
    \[
        \max_{x \in [0, 1)} f(x) = 4 \notin f([0, 1))
    \]
    i.e. $f$ does not reach its upper bound and:
    \[
        \min_{x \in [0, 1)} f(x) = 3 \in f([0, 1))
    \]
    i.e. $f$ reaches its lower bound at $x = 0$:
    \[
        f(0) = 3 = \min_{x \in [0, 1)} f = \inf_{x \in [0, 1)} f
    \]
\end{eg}

\subsection{Types of Functions}
\begin{definition}[Surjectivity]
    A function $f : E \to F$ is said to be surjective (onto) if for every $y \in F$, there exists at least one $x \in E$ such that $f(x) = y$.
\end{definition}
\begin{definition}[Injectivity]
    A function $f : E \to F$ is said to be injective (one-to-one) if for every $x_1, x_2 \in E$, whenever $f(x_1) = f(x_2)$, it follows that $x_1 = x_2$ (i.e. there exists at most one $x \in E$ for each $y \in F$ such that $f(x) = y$).
\end{definition}
If $f: E \to F$ is not injective, it can be made injective by restricting its domain $E$ and if $f$ is not surjective, it can be made surjective by adjusting its codomain $F$.

\begin{definition}[Bijectivity]
    A function $f : E \to F$ is said to be bijective if it is both injective and surjective. In this case, for every $y \in F$, there exists a unique $x \in E$ such that $f(x) = y$.
\end{definition}

\begin{definition}[Inverse Function]
    Let $f : E \to F$ be a bijective function. The inverse function of $f$, denoted by $f^{-1} : F \to E$, is defined by:
    \[
        f^{-1}(y) = x \quad \text{where} \quad f(x) = y
    \]
    for every $y \in F$.
\end{definition}
By convention, for the trigonometric functions, the inverse sine, cosine, and tangent functions are defined on restricted domains to ensure bijectivity:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $\sin : \left[-\frac{\pi}{2}, \frac{\pi}{2}\right] \to [-1, 1]$, reciprocally $\arcsin : [-1, 1] \to \left[-\frac{\pi}{2}, \frac{\pi}{2}\right]$
    \item $\cos : [0, \pi] \to [-1, 1]$, reciprocally $\arccos : [-1, 1] \to [0, \pi]$
    \item $\tan : \left(-\frac{\pi}{2}, \frac{\pi}{2}\right) \to \mathbb{R}$, reciprocally $\arctan : \mathbb{R} \to \left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$
    \item $\cot : (0, \pi) \to \mathbb{R}$, reciprocally $\text{arccot} : \mathbb{R} \to (0, \pi)$
\end{itemize}
\begin{eg}
    Graphically, the inverse function $f^{-1}$ of a bijective function $f$ can be obtained by reflecting the graph of $f$ across the line $y = x$:
    \begin{center}
        \begin{tikzpicture}[scale=1]
            \draw[->] (-0.5, 0) -- (3.5, 0) node[right] {$x$};
            \draw[->] (0, -0.5) -- (0, 3.5) node[above] {$y$};
            \draw[thick, primary, domain=0:2.5, samples=100] plot (\x, {0.5 * (\x)^2}) node[above] {$f$};
            \draw[thick, secondary, domain=0:3, samples=100] plot (\x, {sqrt(2 * \x)}) node[right] {$f^{-1}$};
            \draw[dashed] (0, 0) -- (3.5, 3.5) node[above right] {$y = x$};
        \end{tikzpicture}
    \end{center}
\end{eg}
\begin{eg}
    Let $f = \frac{1}{\cos(x)^2 + 1}$ defined on $\mathbb{R}$. Let's find the biggest interval containing $x= 1$ where $f$ is bijective. \\
    We have:
    \[
        \frac{1}{y + 1} \quad \text{is injective} \quad \forall y \in [0, +\infty)
    \]
    and 
    \[
        \cos(x)^2 \quad \text{is injective} \quad  \forall x \in [0, \frac{\pi}{2}]
    \]
    Since $1 \in [0, \frac{\pi}{2}]$, thus $f$ is injective on the interval $\left[0, \frac{\pi}{2}\right]$. We also have:
    \[
        f(x) = \frac{1}{\cos(x)^2 + 1} \quad x \in \left[0, \frac{\pi}{2}\right]
    \]
    is increasing since $\cos(x)^2$ is decreasing on $\left[0, \frac{\pi}{2}\right]$ and thus:
    \[        
        \inf_{x \in \left[0, \frac{\pi}{2}\right]} f(x) = f\left(\frac{\pi}{2}\right) = \frac{1}{2} \quad \text{and} \quad \sup_{x \in \left[0, \frac{\pi}{2}\right]} f(x) = f(0) = 1
    \]
    Therefore, $f : \left[0, \frac{\pi}{2}\right] \to \left[\frac{1}{2}, 1\right]$ is bijective and its inverse function is given by:
    \[
        f^{-1}(y) = \arccos\left(\sqrt{\frac{1}{y} - 1}\right) \quad y \in \left[\frac{1}{2}, 1\right]
    \]
\end{eg}

\subsection{Composite Functions}
\begin{definition}[Composite Function]
    Let $f : E \to F$ and $g : F \to G$ be two functions. The composite function of $f$ and $g$, denoted by $g \circ f : E \to G$, is defined by:
    \[
        (g \circ f)(x) = g(f(x))
    \]
    for every $x \in E$.
\end{definition}
\begin{eg}
    Let $f : \mathbb{R} \to \mathbb{R}$ be defined by $f(x) = 2x + 3$ and let $g : \mathbb{R} \to \mathbb{R}$ be defined by $g(x) = x^2$. Then, the composite function $g \circ f : \mathbb{R} \to \mathbb{R}$ is given by:
    \[
        (g \circ f)(x) = g(f(x)) = g(2x + 3) = (2x + 3)^2 = 4x^2 + 12x + 9
    \]
    Note that the order of composition matters, as $f \circ g$ would yield a different result:
    \[
        (f \circ g)(x) = f(g(x)) = f(x^2) = 2x^2 + 3
    \]
\end{eg}
\begin{eg}
    Let $f: E \to F$ be a bijective function with inverse $f^{-1} : F \to E$. Then, the composite functions $f \circ f^{-1} : F \to F$ and $f^{-1} \circ f : E \to E$ are given by:
    \[
        (f \circ f^{-1})(y) = f(f^{-1}(y)) = y \quad \forall y \in F
    \]
    and
    \[
        (f^{-1} \circ f)(x) = f^{-1}(f(x)) = x \quad \forall x \in E
    \]
    Thus, composing a function with its inverse yields the identity function on the respective domains.
\end{eg}

\section{Limits of Functions}
\begin{definition}[Neighborhood]
    A function $f: E \to F$ is defined on a neighborhood of a point $x_0 \in E$ if there exists $\delta > 0$ such that:
    \[
        \left\{x \in E : 0 < |x - x_0| < \delta\right\} \subseteq E
    \]
\end{definition}
Remark that the function does not need to be defined at $x_0$ itself, only in its vicinity.

\begin{eg}
    Let $f = \frac{\sin (x)}{x}$ is defined on a neighborhood of $x_0 = 0$ but not at $x_0 = 0$ itself since $f(0)$ is undefined.
\end{eg}

\begin{definition}[Limit of a Function at a Point]
    Let $f: E \to F$ be a function defined on a neighborhood of a point $x_0 \in E$. It is said that the limit of $f(x)$ as $x$ approaches $x_0$ is equal to $L \in F$, denoted by:
    \[
        \lim_{x \to x_0} f(x) = L
    \]
    if for every $\epsilon > 0$, there exists a $\delta > 0$ such that for all $x \in E$ with $0 < |x - x_0| < \delta$, we have:
    \[
        |f(x) - L| < \epsilon
    \]
    \begin{center}
        \begin{tikzpicture}
            \draw[->] (-0.5, 0) -- (3.5, 0) node[right] {$x$};
            \draw[->] (0, -0.5) -- (0, 3.5) node[above] {$y$};
            \draw[thick, primary, domain=0:3, samples=100] plot (\x, {sqrt(2 * \x)}) node[right] {$f$};

            \draw[dashed] (2, 2) -- (2, 0) node[below] {$x_0$};
            \draw[dashed] (2.8, 2.36) -- (2.8, 0) node[below] {$x_0 + \delta$};
            \draw[dashed] (1.2, 1.54) -- (1.2, 0) node[below] {$x_0 - \delta$};
            \draw[<->, secondary, thick] (1.2, 0.5) -- (2.8, 0.5) node[midway, below left] {$\delta$};

            \draw[dashed] (2, 2) -- (0, 2) node[left] {$L$};
            \draw[dashed] (1.2, 1.54) -- (0, 1.54) node[left] {$L - \epsilon$};
            \draw[dashed] (2.8, 2.36) -- (0, 2.36) node[left] {$L + \epsilon$};
            \draw[<->, secondary, thick] (0.5, 1.54) -- (0.5, 2.36) node[midway, below left] {$\epsilon$};
        \end{tikzpicture}
    \end{center}
\end{definition}

\begin{eg}
    Let $f(x) = 3x - 1$ and $x_0 = 1$. Then:
    \[
        \lim_{x \to 1} f(x) = \lim_{x \to 1} (3x - 1) = 2
    \]
    To show this using the $\epsilon$-$\delta$ definition, let $\epsilon > 0$. We need to find $\delta > 0$ such that for all $x$ with $0 < |x - 1| < \delta$, we have:
    \[
        |f(x) - 2| < \epsilon
    \]
    We have:
    \[
        |f(x) - 2| = |3x - 1 - 2| = |3x - 3| = 3|x - 1|
    \]
    Thus, we want:
    \[
        3|x - 1| < \epsilon \quad \implies \quad |x - 1| < \frac{\epsilon}{3}
    \]
    Therefore, we can choose $\delta = \frac{\epsilon}{3}$. Hence, for all $x$ with $0 < |x - 1| < \delta$, we have:
    \[
        |f(x) - 2| < \epsilon
    \]
    This confirms that:
    \[
        \lim_{x \to 1} f(x) = 2
    \]
\end{eg}

\begin{eg}
    Let $f(x) = \sqrt{x} = \sqrt{x_0}, \forall x_0 > 0$. Then:
    \[
        \lim_{x \to x_0} f(x) = \lim_{x \to x_0} \sqrt{x} = \sqrt{x_0}
    \]
    To show this using the $\epsilon$-$\delta$ definition, let $\epsilon > 0$. We need to find $\delta > 0$ such that for all $x$ with $0 < |x - x_0| < \delta$, we have:
    \[
        |\sqrt{x} - \sqrt{x_0}| < \epsilon
    \]
    We have:
    \[
        |\sqrt{x} - \sqrt{x_0}| = \frac{|\sqrt{x} - \sqrt{x_0}| \cdot |\sqrt{x} + \sqrt{x_0}|}{|\sqrt{x} + \sqrt{x_0}|} = \frac{|x - x_0|}{|\sqrt{x} + \sqrt{x_0}|}
    \]
    To ensure that $|\sqrt{x} - \sqrt{x_0}| < \epsilon$, we need:
    \[
        \frac{|x - x_0|}{|\sqrt{x} + \sqrt{x_0}|} < \epsilon \quad \implies \quad |x - x_0| < \epsilon |\sqrt{x} + \sqrt{x_0}|
    \]
    Since we are considering $x$ in a neighborhood of $x_0$, we can assume that $x$ is close to $x_0$. Thus, we can find a lower bound for $|\sqrt{x} + \sqrt{x_0}|$. For instance, if we restrict $x$ to be in the interval $\left[x_0 - 1, x_0 + 1\right]$ (assuming $x_0 > 1$ for simplicity), we have:
    \[
        |\sqrt{x} + \sqrt{x_0}| \geq \sqrt{x_0} \quad \text{for all } x \in \left[x_0 - 1, x_0 + 1\right]
    \]
    Therefore, we can choose $\delta = \epsilon \sqrt{x_0}$. Hence, for all $x$ with $0 < |x - x_0| < \delta$, we have:
    \[
        |\sqrt{x} - \sqrt{x_0}| < \epsilon
    \]
    This confirms that:
    \[
        \lim_{x \to x_0} f(x) = \sqrt{x_0}
    \]
\end{eg}

\subsection{Caracterization of Limits by Sequences}
\begin{theorem}[Caracterization of Limits by Sequences]
    Let $f: E \to F$ be a function defined on a neighborhood of a point $x_0 \in E$. Then, the limit of $f(x)$ as $x$ approaches $x_0$ is equal to $L \in F$, i.e.:
    \[
        \lim_{x \to x_0} f(x) = L
    \]
    if and only if for every sequence $(x_n)$ in $E$ such that $\lim_{n \to +\infty} x_n = x_0$ and $x_n \neq x_0$ for all $n$, we have:
    \[
        \lim_{n \to +\infty} f(x_n) = L
    \]
\end{theorem}
\begin{proof}
    \textbf{($\Rightarrow$)} Assume that $\lim_{x \to x_0} f(x) = L$. Let $(x_n)$ be a sequence in $E$ such that $\lim_{n \to +\infty} x_n = x_0$ and $x_n \neq x_0$ for all $n$. For every $\epsilon > 0$, there exists $\delta > 0$ such that for all $x \in E$ with $0 < |x - x_0| < \delta$, we have:
    \[
        |f(x) - L| < \epsilon
    \]
    Since $\lim_{n \to +\infty} x_n = x_0$, there exists $N \in \mathbb{N}$ such that for all $n \geq N$, we have:
    \[
        |x_n - x_0| < \delta
    \]
    Therefore, for all $n \geq N$, we have:
    \[
        |f(x_n) - L| < \epsilon
    \]
    This shows that $\lim_{n \to +\infty} f(x_n) = L$. \\
    \textbf{($\Leftarrow$)} By contraposition, assume that $\lim_{x \to x_0} f(x) \neq L$. Then, there exists $\epsilon_0 > 0$ such that for every $\delta > 0$, there exists $x \in E$ with $0 < |x - x_0| < \delta$ such that:
    \[
        |f(x) - L| \geq \epsilon_0
    \]
    For each $n \in \mathbb{N}$, let $\delta = \frac{1}{n}$. Then, there exists $x_n \in E$ with $0 < |x_n - x_0| < \frac{1}{n}$ such that:
    \[
        |f(x_n) - L| \geq \epsilon_0
    \]
    This defines a sequence $(x_n)$ in $E$ such that $\lim_{n \to +\infty} x_n = x_0$ and $x_n \neq x_0$ for all $n$. However, we have:
    \[
        |f(x_n) - L| \geq \epsilon_0
    \]
    for all $n$, which implies that $\lim_{n \to +\infty} f(x_n) \neq L$. This completes the proof.
\end{proof}
Remark that the property needs to be true for every sequence converging to $x_0$ and not just for some particular sequences.
\begin{eg}
    Let $f$ be defined by:
    \[
        f(x) = \begin{cases}
            1 & x = \frac{1}{n}, n \in \mathbb{N}^* \\
            0 & \text{otherwise}
        \end{cases}
    \]
    We clearly see that $\lim_{x \to 0} f(x)$ does not exist but for the sequence $x_n = \frac{1}{n}$, we have:
    \[
        \lim_{n \to +\infty} f(x_n) = 1
    \]
    Showing the importance of the "for every sequence" condition in the theorem.
\end{eg}
Let $f : E \to F$ defined on a neighborhood of $x_0$. Let's suppose that for all sequences $(x_n)$ in $E$ such that $\lim_{n \to +\infty} x_n = x_0$ and $x_n \neq x_0$ for all $n$, we have:
\[
    (f(x_n)) \text{ is convergent} \implies \lim_{x \to x_0} f(x) \text{ exists.}
\]
\begin{proof}
    Let's prove this by contradiction. Assume that $\lim_{x \to x_0} f(x)$ does not exist. Then, there exist two sequences $(x_n)$ and $(y_n)$ in $E$ such that $\lim_{n \to +\infty} x_n = x_0$, $\lim_{n \to +\infty} y_n = x_0$, and:
    \[
        \lim_{n \to +\infty} f(x_n) = L_1 \quad \text{and} \quad \lim_{n \to +\infty} f(y_n) = L_2
    \]
    with $L_1 \neq L_2$. Now, we can construct a new sequence $(z_n)$ by interleaving the terms of $(x_n)$ and $(y_n)$:
    \[
        z_{2n} = x_n \quad \text{and} \quad z_{2n+1} = y_n
    \]
    for all $n \in \mathbb{N}$. This new sequence $(z_n)$ also converges to $x_0$ since both $(x_n)$ and $(y_n)$ converge to $x_0$. However, the sequence $(f(z_n))$ does not converge because:
    \[
        \lim_{n \to +\infty} f(z_{2n}) = L_1 \quad \text{and} \quad \lim_{n \to +\infty} f(z_{2n+1}) = L_2
    \]
    with $L_1 \neq L_2$. This contradicts our assumption that for all sequences converging to $x_0$, the sequence $(f(x_n))$ is convergent. Therefore, our initial assumption that $\lim_{x \to x_0} f(x)$ does not exist must be false. Hence, we conclude that $\lim_{x \to x_0} f(x)$ exists.
\end{proof}

\begin{eg}
    Let $f(x) = x^p, p \in \mathbb{N}^*$. Then, we have:
    \[
        \lim_{x \to x_0} f(x) = \lim_{x \to x_0} x^p = x_0^p
    \]
    To show this using the caracterization by sequences, let $(x_n)$ be an arbitrary sequence such that $\lim_{n \to +\infty} x_n = x_0$ and $x_n \neq x_0$ for all $n$. Additionally, let $a_n = x_n - x_0$ for all $n \in \mathbb{N}$ such that $\lim_{n \to +\infty} a_n = 0$. We have:
    \[
        f(x_n) = (x_0 + a_n)^p = \sum_{k=0}^{p} \binom{p}{k} x_0^{p-k} a_n^k
    \]
    Since $\lim_{n \to +\infty} a_n = 0$, we have:
    \[
        \lim_{n \to +\infty} f(x_n) = \sum_{k=0}^{p} \binom{p}{k} x_0^{p-k} \cdot 0^k = x_0^p
    \]
    Therefore, by the caracterization of limits by sequences, we conclude that:
    \[
        \lim_{x \to x_0} f(x) = x_0^p
    \]
\end{eg}

\begin{definition}[Uniqueness of Limits]
    Let $f: E \to F$ be a function defined on a neighborhood of a point $x_0 \in E$. If the limit of $f(x)$ as $x$ approaches $x_0$ exists, then it is unique. In other words, if:
    \[
        \lim_{x \to x_0} f(x) = L_1 \quad \text{and} \quad \lim_{x \to x_0} f(x) = L_2
    \]
    then $L_1 = L_2$.
\end{definition}
\begin{proof}
    Assume that $\lim_{x \to x_0} f(x) = L_1$ and $\lim_{x \to x_0} f(x) = L_2$. Let $(x_n)$ be an arbitrary sequence such that $\lim_{n \to +\infty} x_n = x_0$ and $x_n \neq x_0$ for all $n$. By the caracterization of limits by sequences, we have:
    \[
        \lim_{n \to +\infty} f(x_n) = L_1 \quad \text{and} \quad \lim_{n \to +\infty} f(x_n) = L_2
    \]
    Since the limit of a sequence is unique, we conclude that $L_1 = L_2$. Therefore, the limit of $f(x)$ as $x$ approaches $x_0$ is unique.
\end{proof}

\begin{theorem}[Cauchy Criterion for Limits of Functions]
    Let $f: E \to F$ be a function defined on a neighborhood of a point $x_0 \in E$. Then, the limit of $f(x)$ as $x$ approaches $x_0$ exists if and only if for every $\epsilon > 0$, there exists a $\delta > 0$ such that for all $x, y \in E$ with $0 < |x - x_0| < \delta$ and $0 < |y - x_0| < \delta$, we have:
    \[
        |f(x) - f(y)| < \epsilon
    \]
\end{theorem}

\subsection{Operations on Limits}
\begin{definition}
    Let $f: E \to F$ and $g: E \to F$ be two functions defined on a neighborhood of a point $x_0 \in E$ such that the limits $\lim_{x \to x_0} f(x) = l_1$ and $\lim_{x \to x_0} g(x) = l_2$, then we can define the following operations on their limits:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Sum: $\lim_{x \to x_0} (\alpha f(x) + \beta g(x)) = \alpha l_1 + \beta l_2$
        \item Product: $\lim_{x \to x_0} (f(x) \cdot g(x)) = l_1 \cdot l_2$
        \item Quotient: If $l_2 \neq 0$, then $\lim_{x \to x_0} \left(\frac{f(x)}{g(x)}\right) = \frac{l_1}{l_2}$
    \end{itemize}
\end{definition}
For all polynomial function $P$ and rational function $R = \frac{P_1}{P_2}$ (with $P_2 \neq 0$) defined for all $x_0 \in \mathbb{R}$ except the roots of $P_2$, we have:
\[
    \lim_{x \to x_0} P(x) = P(x_0) \quad \text{and} \quad \lim_{x \to x_0} R(x) = R(x_0)
\]

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to 1} \frac{2x^5 - 3x^3 + 2x + 1}{x^4 + 2x^2 - 6x}
    \]
    We have:
    \[
        \lim_{x \to 1} (2x^5 - 3x^3 + 2x + 1) = 2(1)^5 - 3(1)^3 + 2(1) + 1 = 2 - 3 + 2 + 1 = 2
    \]
    and:
    \[
        \lim_{x \to 1} (x^4 + 2x^2 - 6x) = (1)^4 + 2(1)^2 - 6(1) = 1 + 2 - 6 = -3
    \]
    Therefore, we can apply the quotient rule:
    \[
        \lim_{x \to 1} \frac{2x^5 - 3x^3 + 2x + 1}{x^4 + 2x^2 - 6x} = \frac{2}{-3} = -\frac{2}{3}
    \]
\end{eg}

\begin{theorem}[Sandwich Theorem for Functions]
    Let $f, g, h: E \to F$ be three functions defined on a neighborhood of a point $x_0 \in E$. If there exists $\alpha > 0$ such that for all $x \in \left\{x\in E: 0 < |x-x_0| \leq \alpha\right\}$, we have:
    \[
        f(x) \leq g(x) \leq h(x)
    \]
    and if:
    \[
        \lim_{x \to x_0} f(x) = \lim_{x \to x_0} h(x) = L
    \]
    then:
    \[
        \lim_{x \to x_0} g(x) = L
    \]
\end{theorem}
\begin{proof}
    Let $(a_n) \in \left\{x \in E - \left\{x_0\right\}\right\}$ be a sequence such that $\lim_{n \to +\infty} a_n = x_0$. There exists $m \in \mathbb{N}: \forall n \geq m$ we have:
    \[
        0 < |a_n - x_0| < \alpha
    \]
    Thus, for all $n \geq m$, we have:
    \[
        f(a_n) \leq g(a_n) \leq h(a_n)
    \]
    Since $\lim_{x \to x_0} f(x) = L$ and $\lim_{x \to x_0} h(x) = L$, by the caracterization of limits by sequences, we have:
    \[
        \lim_{n \to +\infty} f(a_n) = L \quad \text{and} \quad \lim_{n \to +\infty} h(a_n) = L
    \]
    Therefore, by the Sandwich Theorem for Sequences, we conclude that:
    \[
        \lim_{n \to +\infty} g(a_n) = L
    \]
    Since the sequence $(a_n)$ was arbitrary, by the caracterization of limits by sequences, we conclude that:
    \[
        \lim_{x \to x_0} g(x) = L
    \]
\end{proof}

\begin{eg}
    Let $g: \mathbb{R} \to \mathbb{R}$ a function such that $\lim_{x \to 0} g(x) = 0$, then:
    \[
        \lim_{x \to 0} \sqrt{1 + g(x)} = 1
    \]
    To show this, we can use the Sandwich Theorem. We know that:
    \[
        1 - |t| \leq \sqrt{1 + t} \leq \left|1 + \frac{1}{2}t\right|, \forall t \geq -1
    \]
    Since:
    \[
        \sqrt{1 + t} \leq \sqrt{1 + t + \frac{t^2}{4}} = \sqrt{\left(1 + \frac{t}{2}\right)^2} = \left|1 + \frac{1}{2}t\right|, \forall t \in \mathbb{R}
    \]
    and:
    \[
        t \geq 0 \implies 1 - |t| = 1 - t \leq 1 \leq \sqrt{1 + t} \quad \text{and} \quad -1 < t < 0 \implies 1 - |t| = 1 + t \leq \sqrt{1 + t}
    \]
    Thus, since $\lim_{x \to 0} g(x) = 0$, there exists $\delta > 0$ such that if $|x| < \delta$, we have:
    \[
        |g(x)| \leq 1 \quad \text{and} \quad g(x) \geq -1
    \]
    Therefore, we have:
    \[
        \underbrace{1 - |g(x)|}_{\to 1 \ (x \to 0)} \leq \sqrt{1 + g(x)} \leq \underbrace{\left|1 + \frac{1}{2}g(x)\right|}_{\to 1 \ (x \to 0)}
    \]
    By the Sandwich Theorem, we conclude that:
    \[
        \lim_{x \to 0} \sqrt{1 + g(x)} = 1
    \]
\end{eg}

\begin{eg}
    Let $f(x) = x^2 \cdot \cos\left(\frac{1}{x}\right)$ be a function not defined in $x = 0$. We want to compute:
    \[
        \lim_{x \to 0} f(x)
    \]
    We have:
    \[
        -1 \leq \cos\left(\frac{1}{x}\right) \leq 1 \quad \underbrace{\implies}_{x^2 \geq 0} \quad -x^2 \leq x^2 \cdot \cos\left(\frac{1}{x}\right) \leq x^2
    \]
    Since:
    \[
        \lim_{x \to 0} -x^2 = 0 \quad \text{and} \quad \lim_{x \to 0} x^2 = 0
    \]
    By the Sandwich Theorem, we conclude that:
    \[
        \lim_{x \to 0} x^2 \cdot \cos\left(\frac{1}{x}\right) = 0
    \]
\end{eg}
From the example above, we can also visualize the functions:
\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \begin{center}
            \begin{tikzpicture}[scale=1.5]
                \draw[->] (-1.75, 0) -- (1.75, 0) node[right] {$x$};
                \draw[->] (0, -1.25) -- (0, 1.25) node[above] {$y$};
                \draw[thick, primary, domain=-1.5:1.5, samples=300] plot (\x, {cos(deg(1 / \x))});
            \end{tikzpicture}
        \end{center}
        \caption*{Graph of $\cos\left(\frac{1}{x}\right)$}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \begin{center}
            \begin{tikzpicture}[scale=2.35]
                \draw[->] (-1.2, 0) -- (1.2, 0) node[right] {$x$};
                \draw[->] (0, -0.3) -- (0, 1.25) node[above] {$y$};
                \draw[thick, primary, domain=-1.05:1.05, samples=200] plot (\x, {cos(deg(1 / \x)) * (\x * \x)});
            \end{tikzpicture}
        \end{center}
        \caption*{Graph of $x^2 \cdot \cos\left(\frac{1}{x}\right)$}
    \end{minipage}
\end{figure}

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to 0} \frac{\sin x}{x}
    \]
    For $x$ close to $0$ ($x \neq 0$ thus $\sin x \neq 0$), we can bound $\sin x$ by:
    \[
        |\sin x| \leq |x| \leq |\tan x|
    \]
    Since $\sin x \neq 0$, we can divide the inequalities by $|\sin x|$:
    \[
        1 \leq \left|\frac{x}{\sin x}\right| \leq \left|\frac{1}{\cos x}\right| \quad \iff \quad 1 \leq \frac{x}{\sin x} \leq \frac{1}{\cos x}
    \]
    We have:
    \[
        \cos 2\alpha = (\cos \alpha)^2 - (\sin \alpha)^2 = 1 - 2(\sin \alpha)^2 \quad \implies \quad 1 - \cos 2\alpha = 2 (\sin \alpha)^2
    \]
    and:
    \[
        |1 - \cos x| = 2 \left|\left(\sin \frac{x}{2}\right)^2\right| = \left(\sin \frac{x}{2}\right)^2 \leq 2 \left|\frac{x}{2}\right|^2 = \frac{x^2}{2} \quad \implies \quad |\cos x - 1| \leq \frac{x^2}{2}
    \]
    By the Squeeze Theorem, we have:
    \[
        0 \leq |1 - \cos x| \leq \frac{x^2}{2} \quad \implies \quad \lim_{x \to 0} |1 - \cos x| = 0 \quad \implies \quad \lim_{x \to 0} \cos x = 1
    \]
    and:
    \[
        \lim_{x \to 0} \frac{1}{\cos x} = 1
    \]
    Finally, by the Squeeze Theorem, we conclude that:
    \[
        1 \leq \frac{x}{\sin x} \leq \underbrace{\frac{1}{\cos x}}_{= 1} \quad \implies \quad \lim_{x \to 0} \frac{x}{\sin x} = 1 \quad \implies \quad \lim_{x \to 0} \frac{\sin x}{x} = 1
    \]
\end{eg}

\subsection{Limits of Composite Functions}
\begin{theorem}[Limits of Composite Functions]
    Let $f: E \to F$ and $g: F \to G$ be two functions such that $\lim_{x \to x_0} f(x) = y_0$ and $\lim_{y \to y_0} g(y) = L$. If there exists $\alpha > 0$ such that for all $x \in \left\{y \in E: 0 < |y - x_0| < \alpha\right\}$, we have $f(x) \neq y_0$, then:
    \[
        \lim_{x \to x_0} g(f(x)) = L
    \]
\end{theorem}
Remark that the condition $f(x) \neq y_0$ is necessary because $g(y_0)$ might not be defined and thus we cannot have $f(x)$ approaching $y_0$ directly.
\begin{proof}
    Let $\lim_{y \to y_0} g(y) = L$, we have for a given $\epsilon > 0$, there exists $\delta_1 > 0$ such that for all $y \in F$ with $0 < |y - y_0| < \delta_1$, we have:
    \[
        |g(y) - L| < \epsilon
    \]
    Since $\lim_{x \to x_0} f(x) = y_0$, there exists $\delta_2 > 0$ such that for all $x \in E$ with $0 < |x - x_0| < \delta_2$, we have:
    \[
        |f(x) - y_0| < \delta_1
    \]
    Let $\delta = \min(\alpha, \delta_2)$, then for all $x \in E$ with $0 < |x - x_0| < \delta$, we have:
    \[
        |f(x) - y_0| < \delta_1
    \]
    and since $f(x) \neq y_0$, we can apply the first inequality:
    \[
        |g(f(x)) - L| < \epsilon
    \]
    Therefore, we conclude that:
    \[
        \lim_{x \to x_0} g(f(x)) = L
    \]
\end{proof}
This theorem allows to change the variable in a limit computation.

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to 0} \frac{\cos(4x) - \cos(2x)}{x^2}
    \]
    If we compute the limit directly, we get the indeterminate form $\frac{0}{0}$. To resolve this, we can use the trigonometric identity ($\cos (2\alpha) = 1 - 2 (\sin \alpha)^2$), we get:
    \[
        \lim_{x \to 0} \frac{1 - (\sin(2x))^2 - 1 + 2(\sin x)^2}{x^2} = \lim_{x \to 0} 2 \cdot \frac{(\sin x)^2 - (\sin(2x))^2}{x^2}
    \]
    Thus, we can rewrite the limit as:
    \[
        \lim_{x \to 0} 2\left(\underbrace{\left(\frac{\sin x}{x}\right)^2}_{\to 1^2} - \underbrace{\left(\frac{\sin(2x)}{2x}\right)^2}_{\to 1^2} \cdot 4\right) = 2(1-4) = -6
    \]
\end{eg}

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to 2} \frac{\sqrt{x + 2} - \sqrt{2x}}{\sqrt{x -1} -1}
    \]
    Again, if we compute the limit directly, we get the indeterminate form $\frac{0}{0}$. To resolve this, we can multiply the numerator and denominator by the conjugate of each:
    \[
        \lim_{x \to 2} \frac{(\sqrt{x + 2} - \sqrt{2x})(\sqrt{x + 2} + \sqrt{2x})}{(\sqrt{x -1} -1)(\sqrt{x -1} +1)} = \lim_{x \to 2} \frac{(x + 2) - 2x}{(x -1) - 1} \cdot \frac{\sqrt{x -1} +1}{\sqrt{x + 2} + \sqrt{2x}}
    \]
    Thus, we can rewrite the limit as:
    \[
        \lim_{x \to 2} \frac{2 - x}{x - 2} \cdot \lim_{x \to 2} \frac{\sqrt{x -1} +1}{\sqrt{x + 2} + \sqrt{2x}} = -1 \cdot \frac{\sqrt{2 -1} +1}{\sqrt{2 + 2} + \sqrt{4}} = -\frac{1 + 1}{2 + 2} = -\frac{1}{2}
    \]
\end{eg}
Remark that by the theorem above, $\lim_{x \to 0} \frac{\sin (t(x))}{t(x)} = 1$ if $\lim_{x \to 0} t(x) = 0$ and $t(x) \neq 0$ for $x \neq 0$ close to $0$.

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to 3} \frac{\sin(\sqrt{x + 1} - 2)}{x -3}
    \]
    Since $\lim_{x \to 3} \sqrt{x + 1} - 2 = 0$, we can rewrite the limit as:
    \[
        \lim_{x \to 3} \underbrace{\frac{\sin(\sqrt{x + 1} -2)}{\sqrt{x + 1} -2}}_{\to 1} \cdot \frac{\sqrt{x + 1} -2}{x -3} = \lim_{x \to 3} \frac{\sin(\sqrt{x + 1} -2)}{\sqrt{x + 1} -2} \cdot \frac{x + 1 - 4}{\sqrt{x + 1} + 2} \cdot \frac{1}{x-3}
    \]
    Thus, we can rewrite the limit as:
    \[
        \lim_{x \to 3} \frac{\sin(\sqrt{x + 1} -2)}{\sqrt{x + 1} -2} \cdot \frac{x -3}{x -3 } \cdot \frac{1}{\sqrt{x + 1} + 2} = 1 \cdot 1 \cdot \frac{1}{\sqrt{4} + 2} = \frac{1}{4}
    \]
\end{eg}
Remark that the $\sin$ simplification would not be valid, for example, if $t(x) = x^2 \cos(\frac{1}{x})$ because the function $t(x)$ is not defined in the neighborhood of $0$.

\begin{eg}
    Let's prove that the limit $\lim_{x \to 0} \sin(\frac{1}{x})$ does not exists. To show this, we can consider two sequences $(a_n)$ and $(b_n)$ defined by:
    \[
        a_n = \frac{1}{\pi n} \quad \text{and} \quad b_n = \frac{1}{\frac{\pi}{2} + 2\pi n}
    \]
    for all $n \in \mathbb{N}^*$. We have:
    \[
        \lim_{n \to +\infty} a_n = \lim_{n \to +\infty} b_n = 0
    \]
    However, we have:
    \[
        \lim_{n \to +\infty} \sin\left(\frac{1}{a_n}\right) = \lim_{n \to +\infty} \sin(\pi n) = 0
    \]
    and:
    \[
        \lim_{n \to +\infty} \sin\left(\frac{1}{b_n}\right) = \lim_{n \to +\infty} \sin\left(\frac{\pi}{2} + 2\pi n\right) = 1
    \]
    Since the two sequences $(\sin(\frac{1}{a_n}))$ and $(\sin(\frac{1}{b_n}))$ converge to different limits, by the caracterization of limits by sequences, we conclude that the limit $\lim_{x \to 0} \sin(\frac{1}{x})$ does not exists.
\end{eg}
In general, to prove that a limit does not exits, two sequences converging to the same point can be used to show that the function values converge to different limits.

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to 0} x \cdot \sin\left(\frac{1}{x}\right) \quad \text{where} \quad D(f) = \mathbb{R} - \left\{0\right\}
    \]
    By using the Squeeze Theorem, we have:
    \[
        -1 \leq \sin \frac{1}{x} \leq 1 \quad \iff \quad 0 \leq \left|\sin \frac{1}{x}\right| \leq 1
    \]
    Multplying by $|x|$ (which is positive when $x \to 0$), we get:
    \[
        0 \leq \left|x \cdot \sin\frac{1}{x}\right| \leq \underbrace{|x|}_{\to 0}
    \]
    Thus, by the Squeeze Theorem, we conclude that:
    \[
        \lim_{x \to 0} x \cdot \sin\left(\frac{1}{x}\right) = 0
    \]
\end{eg}

\subsection{Limits at Infinity and Infinite Limits}
\begin{definition}[Neighborhoods of Infinity]
    Let $f: E \to F$ is defined on a neighborhood of $+\infty$ (resp. $-\infty$) if there exists $\alpha \in \mathbb{R}: (\alpha, +\infty) \subset E$ (resp. $(-\infty, \alpha) \subset E$).
\end{definition}

\begin{definition}[Limit at Infinity] 
    Let $f: E \to F$ be a function defined on a neighborhood of $+\infty$ (resp. $-\infty$). It is said that the limit of $f(x)$ as $x$ approaches $+\infty$ (resp. $-\infty$) is equal to $L \in F$ if for every $\epsilon > 0$, there exists $M > 0$ such that for all $x \in E$ with $x > M$ (resp. $x < -M$), we have:
    \[
        |f(x) - L| < \epsilon
    \]
    This is denoted by:
    \[
        \lim_{x \to +\infty} f(x) = L \quad \left(\text{resp. } \lim_{x \to -\infty} f(x) = L\right)
    \]
    In this case, it is said that the function $f$ has a horizontal asymptote $y = L$ at $+\infty$ (resp. $-\infty$).
\end{definition}
% TODO: add graph

\begin{eg}
    Let's prove that the limit:
    \[
        \lim_{x \to \infty} \frac{1}{x^2} = 0
    \]
    To show this, let $\epsilon > 0$ be given. We need to find $M > 0$ such that for all $x > M$, we have:
    \[
        \left|\frac{1}{x^2} - 0\right| < \epsilon
    \]
    We have:
    \[
        \left|\frac{1}{x^2} - 0\right| = \frac{1}{x^2} < \epsilon \quad \iff \quad x^2 > \frac{1}{\epsilon} \quad \iff \quad x > \frac{1}{\sqrt{\epsilon}}
    \]
    Thus, we can choose $M = \frac{1}{\sqrt{\epsilon}}$. Therefore, for all $x > M$, we have:
    \[
        \left|\frac{1}{x^2} - 0\right| < \epsilon
    \]
    Thus, we conclude that:
    \[
        \lim_{x \to \infty} \frac{1}{x^2} = 0
    \]
\end{eg}

\begin{definition}[Infinite Limits]
    Let $f: E \to F$ be a function defined on a neighborhood of $x_0 \in E$. It is said that the limit of $f(x)$ as $x$ approaches $x_0$ is equal to $+\infty$ (resp. $-\infty$) if for every $M > 0$, there exists $\delta > 0$ such that for all $x \in E$ with $0 < |x - x_0| < \delta$, we have:
    \[
        f(x) > M \quad (\text{resp. } f(x) < -M)
    \]
    This is denoted by:
    \[
        \lim_{x \to x_0} f(x) = +\infty \quad (\text{resp. } \lim_{x \to x_0} f(x) = -\infty)
    \]
    In this case, it is said that the function $f$ has a vertical asymptote $x = x_0$.
\end{definition}

\begin{eg}
    Let's prove the following limit:
    \[
        \lim_{x \to 0} \frac{1}{x^2} = +\infty
    \]
    To show this, let $M > 0$ be given. We need to find $\delta > 0$ such that for all $x \in E$ with $0 < |x - 0| < \delta$, we have:
    \[
        \frac{1}{x^2} > M
    \]
    We have:
    \[
        \frac{1}{x^2} > M \quad \iff \quad |x| < \frac{1}{\sqrt{M}}
    \]
    Thus, we can choose $\delta = \frac{1}{\sqrt{M}}$. Therefore, for all $x \in E$ with $0 < |x| < \delta$, we have:
    \[
        \frac{1}{x^2} > M
    \]
    Thus, we conclude that:
    \[
        \lim_{x \to 0} \frac{1}{x^2} = +\infty
    \]
\end{eg}

\begin{eg}
    Let's show that the following limit does not exist:
    \[
        \lim_{x \to 0} \frac{1}{x}
    \]
    To show this, we can consider two sequences $(a_n)$ and $(b_n)$ defined by:
    \[
        a_n = \frac{1}{n} \quad \text{and} \quad b_n = -\frac{1}{n}
    \]
    for all $n \in \mathbb{N}^*$. We have:
    \[
        \lim_{n \to +\infty} a_n = \lim_{n \to +\infty} b_n = 0
    \]
    However, we have:
    \[
        \lim_{n \to +\infty} \frac{1}{a_n} = \lim_{n \to +\infty} n = +\infty
    \]
    and:
    \[
        \lim_{n \to +\infty} \frac{1}{b_n} = \lim_{n \to +\infty} -n = -\infty
    \]
    Since the two sequences $(\frac{1}{a_n})$ and $(\frac{1}{b_n})$ converge to different limits, by the caracterization of limits by sequences, we conclude that the limit $\lim_{x \to 0} \frac{1}{x}$ does not exist.
\end{eg}

\begin{definition}[Infite Limits at Infinity]
    Let $f: E \to F$ be a function defined on a neighborhood of $+\infty$ (resp. $-\infty$). It is said that the limit of $f(x)$ as $x$ approaches $+\infty$ (resp. $-\infty$) is equal to $+\infty$ (resp. $-\infty$) if for every $M > 0$, there exists $X > 0$ such that for all $x \in E$ with $x > X$ (resp. $x < -X$), we have:
    \[
        f(x) > M \quad (\text{resp. } f(x) < -M)
    \]
    This is denoted by:
    \[
        \lim_{x \to +\infty} f(x) = +\infty \quad (\text{resp. } \lim_{x \to -\infty} f(x) = -\infty)
    \]
\end{definition}
Note that all results shown for limits at finite points can be adapted to limits at infinity and vice versa.

\begin{eg}
    Some examples of infinite limits at infinity are:
    \[
        \lim_{x \to +\infty} x = +\infty, \quad \lim_{x \to -\infty} x = -\infty, \quad \lim_{x \to +\infty} e^x = +\infty, \quad \lim_{x \to -\infty} e^x = 0
    \]
\end{eg}

\subsection{Indeterminate Forms}
When computing limits, we can encounter some indeterminate forms such as:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $\frac{0}{0}$
    \item $\frac{\infty}{\infty}$
    \item $0 \cdot \infty$
    \item $\infty - \infty$
    \item $0^0$ (will be seen later)
    \item $1^\infty$ (will be seen later)
    \item $\infty^0$ (will be seen later)
\end{itemize}

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to \infty} (\sqrt{x^2 + 2x} - x)
    \]
    If we compute the limit directly, we get the indeterminate form $\infty - \infty$. To resolve this, we can multiply the expression by its conjugate:
    \[
        \lim_{x \to \infty} (\sqrt{x^2 + 2x} - x) \cdot \frac{\sqrt{x^2 + 2x} + x}{\sqrt{x^2 + 2x} + x} = \lim_{x \to \infty} \frac{(x^2 + 2x) - x^2}{\sqrt{x^2 + 2x} + x} = \lim_{x \to \infty} \frac{2x}{\sqrt{x^2 + 2x} + x}
    \]
    Thus, we can rewrite the limit as:
    \[
        \lim_{x \to \infty} \frac{2}{\sqrt{1 + \frac{2}{x}} + 1} = \frac{2}{\sqrt{1 + 0} + 1} = \frac{2}{2} = 1
    \]
\end{eg}

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to - \infty} (\sqrt{x^2 + 2x} + x)
    \]
    If we compute the limit directly, we get the indeterminate form $\infty - \infty$. To resolve this, we can multiply the expression by its conjugate:
    \[
        \lim_{x \to -\infty} (\sqrt{x^2 + 2x} + x) \cdot \frac{\sqrt{x^2 + 2x} - x}{\sqrt{x^2 + 2x} - x} = \lim_{x \to -\infty} \frac{(x^2 + 2x) - x^2}{\sqrt{x^2 + 2x} - x} = \lim_{x \to -\infty} \frac{2x}{\sqrt{x^2 + 2x} - x}
    \]
    Thus, we can rewrite the limit as:
    \[
        \lim_{x \to -\infty} \frac{2x}{\underbrace{\sqrt{x^2}}_{|x| = -x \ (x \to - \infty)} \sqrt{1 + \frac{1}{2}} - x} = = \lim_{x \to - \infty} \frac{2x}{-x \left(\sqrt{1 + \frac{2}{x}} + 1\right)} = -1
    \]
\end{eg}

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to 1} \frac{\sin\left(\frac{\pi x}{2}\right) + x^2 - 2x}{(x -1)^2}
    \]
    If we compute the limit directly, we get the indeterminate form $\frac{0}{0}$. To resolve this, we can make a change of variable $t = x -1$:
    \[
        \lim_{t \to 0} \frac{\sin\left(\frac{\pi (t + 1)}{2}\right) + (t + 1)^2 - 2(t + 1)}{t^2} = \lim_{t \to 0} \frac{\sin\left(\frac{\pi t}{2} + \frac{\pi}{2}\right) + t^2 + 2t + 1 - 2t - 2}{t^2}
    \]
    By using the trigonometric identity $\sin(\alpha + \frac{\pi}{2}) = \cos \alpha$, we get:
    \[
        = \lim_{t \to 0} \frac{\cos\left(\frac{\pi t}{2}\right) + t^2 -1}{t^2}
    \]
    Again, by using the trigonometric identity $\cos \alpha = 1 - 2(\sin \frac{\alpha}{2})^2$, we get:
    \[
        = \lim_{t \to 0} \frac{-2\left(\sin\frac{\pi t}{4}\right)^2 + t^2}{t^2} = \lim_{t \to 0} \left(1 - 2\underbrace{\frac{\left(\sin(\frac{\pi t}{4})\right)^2}{\left(\frac{\pi t}{4}\right)^2}}_{\to 1} \frac{\left(\frac{\pi t}{4}\right)^2}{t^2}\right) = 1 - \frac{\pi^2}{8}
    \]
\end{eg}

\subsection{Properties of Infinite Limits}
The properties of limits shown previously can be adapted to infinite limits. Here are some examples:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item If $\lim_{x \to x_0} f(x) = \pm\infty$ and $\lim_{x \to x_0} g(x) = \pm \infty$, then $\lim_{x \to x_0} (f(x) + g(x)) = \pm\infty$
    \item If $\lim_{x \to x_0} f(x) = +\infty$ and $\lim_{x \to x_0} g(x) = L \in \mathbb{R}^*$, then $\lim_{x \to x_0} (f(x) \cdot g(x)) = +\infty$ if $L > 0$ and $-\infty$ if $L < 0$
    \item If $\lim_{x \to x_0} f(x) = \pm\infty$ and $g(x)$ is bounded in a neighborhood of $x_0$, then $\lim_{x \to x_0} (f(x) \pm g(x)) = \pm\infty$
    \item If $\lim_{x \to x_0} f(x) = \pm \infty$, then $\lim_{x \to x_0} \frac{1}{f(x)} = 0$
    \item If $\lim_{x \to x_0} f(x) = 0$ and $f(x) \neq 0$ in a neighborhood of $x_0$, then $\lim_{x \to x_0} \frac{1}{f(x)} = + \infty$ if $f(x) > 0$ in a neighborhood of $x_0$, $-\infty$ if $f(x) < 0$ in a neighborhood of $x_0$ and does not exist otherwise
    \item If $\lim_{x \to x_0} = + \infty \ (- \infty)$ and $g(x) \geq f(x)$ ($g(x) \leq f(x)$) for all $x$ in a neighborhood of $x_0$, then $\lim_{x \to x_0} g(x) = + \infty$ ($- \infty$) (Squeeze Theorem)
\end{itemize}
Note that these properties are aslo valid for limits at infinity.

\subsection{Right and Left Limits}
\begin{definition}[Right and Left Limits]
    Let $f: E \to F$ be a function and $x_0 \in \mathbb{R}$ be an accumulation point of $E$. \\
    It is said that the \textbf{right limit} of $f(x)$ as $x$ approaches $x_0$ is equal to $L \in F$ if for every $\epsilon > 0$, there exists $\delta > 0$ such that for all $x \in E$ with $x_0 < x < x_0 + \delta$, we have:
    \[
        |f(x) - L| < \epsilon
    \]
    This is denoted by:
    \[
        \lim_{x \to x_0^+} f(x) = L
    \]
    It is said that the \textbf{left limit} of $f(x)$ as $x$ approaches $x_0$ is equal to $L \in F$ if for every $\epsilon > 0$, there exists $\delta > 0$ such that for all $x \in E$ with $x_0 - \delta < x < x_0$, we have:
    \[
        |f(x) - L| < \epsilon
    \]
    This is denoted by:
    \[
        \lim_{x \to x_0^-} f(x) = L
    \]
\end{definition}
Remark that a limit $\lim_{x \to x_0} f(x) = L$ exists if and only if both right and left limits exist and are equal:
\[
    \lim_{x \to x_0^+} f(x) = \lim_{x \to x_0^-} f(x) = L
\]

\begin{eg}
    Let's consider the function:
    \[
        f(x) = \begin{cases}
            1, & x \geq 0 \\
            -1, & x < 0
        \end{cases}
    \]
    Graphically, this function looks like:
    \begin{center}
        \begin{tikzpicture}[scale=1]
            \draw[->] (-3,0) -- (3,0) node[right] {$x$};
            \draw[->] (0,-1.5) -- (0,1.5) node[above] {$y$};
            \draw[thick,primary] (0,1) -- (2.5,1) node[right] {$f(x)$};
            \draw[thick,primary] (-2.5,-1) -- (0,-1);
        \end{tikzpicture}
    \end{center}
    We have:
    \[
        \lim_{x \to 0^+} f(x) = 1 \quad \text{and} \quad \lim_{x \to 0^-} f(x) = -1
    \]
    Thus, the limit $\lim_{x \to 0} f(x)$ does not exist.
\end{eg}

\begin{eg}
    Let consider the function $f(x) = \frac{1}{x}$. We have:
    \[
        \lim_{x \to 0^+} f(x) = +\infty \quad \text{and} \quad \lim_{x \to 0^-} f(x) = -\infty
    \]
    Graphically, this function looks like:
    \begin{center}
        \begin{tikzpicture}[scale=.8]
            \draw[->] (-3,0) -- (3,0) node[right] {$x$};
            \draw[->] (0,-3) -- (0,3) node[above] {$y$};
            
            \draw[thick,primary,domain=0.37:2.5,smooth,variable=\x] plot ({\x},{1/\x}) node[right] {$f(x)$};
            \draw[thick,primary,domain=-2.5:-0.37,smooth,variable=\x] plot ({\x},{1/\x});
        \end{tikzpicture}
    \end{center}
    Thus, the limit $\lim_{x \to 0} f(x)$ does not exist.
\end{eg}

\section{Exponential and Logarithmic Functions}
\begin{definition}[$e$]
    The number $e$ is defined as the unique positive number such that:
    \[
        \sum_{n = 0}^{+\infty} \frac{x^n}{n!} = e^x
    \]
    Which converges for all $x \in \mathbb{R}$ (D'Alembert's ratio test).
\end{definition}
By convention, we define $0! = 1$ and $0^0 = 1$. The following properties hold:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $e^{x + y} = e^x \cdot e^y$ for all $x, y \in \mathbb{R}$
    \item $e^{-x} = \frac{1}{e^x}$ for all $x \in \mathbb{R}$
    \item $e^x > 0$ for all $x \in \mathbb{R}$
\end{itemize}
\begin{proof}
    \textbf{1. Property}:
    \[
        e^x \cdot e^y = \left(\sum_{n = 0}^{+\infty} \frac{x^n}{n!}\right) \cdot \left(\sum_{m = 0}^{+\infty} \frac{y^m}{m!}\right) = \sum_{n = 0}^{+\infty} \sum_{m = 0}^{+\infty} \frac{x^n y^m}{n! m!}    
    \]
    By rearranging the terms, we get:
    \[
        = \sum_{k = 0}^{+\infty} \sum_{n = 0}^{k} \frac{x^n y^{k - n}}{n! (k - n)!} = \sum_{k = 0}^{+\infty} \frac{1}{k!} \sum_{n = 0}^{k} \binom{k}{n} x^n y^{k - n}
    \]
    By using the binomial theorem, we have:
    \[
        = \sum_{k = 0}^{+\infty} \frac{(x + y)^k}{k!} = e^{x + y}
    \]
    \textbf{2. Property}:
    \[
        e^x \cdot e^{-x} = e^{x - x} = e^0 = 1 \quad \implies \quad e^{-x} = \frac{1}{e^x}
    \]
    \textbf{3. Property}: Since all terms of the series defining $e^x$ are positive, we have:
    \[
        e^x = e^{\frac{x}{2}} \cdot e^{\frac{x}{2}} = \left(e^{\frac{x}{2}}\right)^2 > 0
    \]
\end{proof}

\subsection{Properties of Exponential Function}
These properties can be proved using the definition of $e^x$:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $\lim_{x \to \infty} e^x = \lim_{x \to \infty} \sum_{k = 0}^{\infty} \frac{x^k}{k!} \geq \lim_{x \to \infty} (1 + x) = + \infty$
    \item $\lim_{x \to -\infty} e^x = \lim_{y \to \infty} e^{-y} = \lim_{y \to \infty} \frac{1}{e^y} = 0$
    \item $e^x$ is strictly increasing on $\mathbb{R}$. Let $x_1, x_2 \in \mathbb{R}$ such that $x_1 < x_2$. We have:
    \[
        e^{x_2} - e^{x_1} = e^{x_1} (e^{x_2 - x_1} - 1) > 0
    \]
\end{itemize}    

\begin{eg}
    Let's prove the following limit:
    \[
        \lim_{x \to 0} \frac{e^x -1}{x} = 1
    \]
    To prove this, we can use the definition of $e^x$:
    \begin{align*}
        \left|\frac{e^x -1}{x} - 1\right|
        &= \left|\frac{\sum_{k = 0}^{\infty} \frac{x^k}{k!} - 1}{x} - 1\right| = \left|\frac{\sum_{k = 1}^{\infty} \frac{x^k}{k!}}{x} - 1\right| = \left|\sum_{k = 1}^{\infty} \frac{x^{k-1}}{k!} - 1\right|\\
        &= \left|\sum_{k = 2}^{\infty} \frac{x^{k-1}}{k!}\right| \\
        &\le \sum_{k = 2}^{\infty} \frac{|x|^{k-1}}{k!} = |x| \sum_{k = 2}^{\infty} \frac{|x|^{k-2}}{k!} \\
        &\le |x| \sum_{k = 2}^{\infty} \frac{|x|^{k-2}}{(k-2)!} = |x| \sum_{m = 0}^{\infty} \frac{|x|^{m}}{m!} = |x| e^{|x|}\\
    \end{align*}
    By the Squeeze Theorem, we conclude that:
    \[
        0 < \left|\frac{e^x -1}{x} -1\right| < \underbrace{|x| e^{|x|}}_{ \to 0} \quad \implies \quad \lim_{x \to 0} \frac{e^x -1}{x} = 1
    \]
\end{eg}
Remark that this limit $\lim_{x \to a} \frac{e^{g(x)} -1}{g(x)}$ also converges to $1$ for any function $g(x)$ such that $\lim_{x \to a} g(x) = 0$ and $g(x) \neq 0$ in the neighborhood of $x = a$.

\subsection{Logarithmic Function}
\begin{definition}[Natural Logarithm]
    The natural logarithm function $\ln: (0, +\infty) \to \mathbb{R}$ is defined as the inverse function of the exponential function $e^x: \mathbb{R} \to (0, +\infty)$.
\end{definition}

\subsection{Properties of Logarithmic Function}
These properties can be proved using the definition of $\ln x$:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $e^{\ln x} = x$ for all $x > 0$, $\ln(e^x) = x$ for all $x \in \mathbb{R}$
    \item $\ln(xy) = \ln x + \ln y$ for all $x, y > 0$
    \item $\ln\left(\frac{x}{y}\right) = \ln x - \ln y$ for all $x, y > 0$
    \item $\ln(x^r) = r \ln x$ for all $r \in \mathbb{R}$
    \item $\ln(1) = 0$, $\ln(e) = 1$
\end{itemize}

\section{List of Important Limits}
Here is a list of some important limits related to exponential and logarithmic functions:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $\lim_{x \to 0} \frac{e^x -1 }{x} = 1$
    \item $\lim_{x \to 0} \frac{\ln(1 + x)}{x} = 1$
    \item $\lim_{x \to 0} \left(1 + x\right)^{\frac{1}{x}} = e$
    \item $\lim_{x \to a} \frac{e^{t(x)} -1}{t(x)} = 1$ if $\lim_{x \to a} t(x) = 0$ and $t(x) \neq 0$ in a neighborhood of $x = a$
    \item $\lim_{x \to a} \frac{\ln(1 + t(x))}{t(x)} = 1$ if $\lim_{x \to a} t(x) = 0$ and $t(x) \neq 0$ in a neighborhood of $x = a$
    \item $\lim_{x \to a} \left(1 + t(x)\right)^{\frac{1}{t(x)}} = e$ if $\lim_{x \to a} t(x) = 0$ and $t(x) \neq 0$ in a neighborhood of $x = a$
\end{itemize} 

\section{Continuity}
\begin{definition}[Continuity at a Point]
    Let $f: E \to F$ be a function and $x_0 \in E$. The function $f$ is said to be continuous at point $x_0$ if:
    \[
        \lim_{x \to x_0} f(x) = f(x_0)
    \]
    % That is, for every $\epsilon > 0$, there exists $\delta > 0$ such that for all $x \in E$ with $|x - x_0| < \delta$, we have:
    % \[
    %     |f(x) - f(x_0)| < \epsilon
    % \]
\end{definition}
Remark that this condition could be spit into three parts:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item $f(x_0)$ is defined (i.e., $x_0 \in E$)
    \item $\lim_{x \to x_0} f(x)$ exists
    \item $\lim_{x \to x_0} f(x) = f(x_0)$
\end{itemize}
\begin{eg}
    The following examples illustrate the concept of continuity at a point:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $f(x) = x^p, p \in \mathbb{N}$ is continous at every point $x_0 \in \mathbb{R}$, since $\lim_{x \to a} x^p = a^p$ for all $a \in \mathbb{R}$.
        \item All polynomial functions are continuous at every point $x_0 \in \mathbb{R}$.
        \item Any rational function $\frac{P(x)}{Q(x)}$ is continuous at every point $x_0 \in \mathbb{R}$ such that $Q(x_0) \neq 0$.
        \item $f(x) = \sqrt[p]{x}$ is continuous at every point $x_0 \in (0, +\infty)$ for all $p \in \mathbb{N}^*$.
        \item $f(x) = \sin(x)$ and $g(x) = \cos(x)$ are continuous at every point $x_0 \in \mathbb{R}$.
    \end{itemize}
\end{eg}
\begin{theorem}[Cauchy's Criterion for Continuity]
    Let $f: E \to F$ be a function defined in a neighborhood of $x = x_0$ and at $x_0$. The function $f$ is continuous at point $x_0$ if and only if for every $\epsilon > 0$, there exists $\delta > 0$ such that for all $x, y \in E$ with $|x - x_0| < \delta$ and $|y - x_0| < \delta$, we have:
    \[
        |f(x) - f(y)| < \epsilon
    \]
\end{theorem}
\begin{proof}
    \textbf{($\Rightarrow$) Direction}: Assume that $f$ is continuous at point $x_0$. Let $\epsilon > 0$ be given. By the definition of continuity, there exists $\delta > 0$ such that for all $x \in E$ with $|x - x_0| < \delta$, we have:
    \[
        |f(x) - f(x_0)| < \frac{\epsilon}{2}
    \]
    Now, let $x, y \in E$ such that $|x - x_0| < \delta$ and $|y - x_0| < \delta$. We have:
    \[
        |f(x) - f(y)| = |f(x) - f(x_0) + f(x_0) - f(y)| \leq |f(x) - f(x_0)| + |f(y) - f(x_0)| < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
    \]
    Thus, the condition is satisfied. \\
    \textbf{($\Leftarrow$) Direction}: Assume that for every $\epsilon > 0$, there exists $\delta > 0$ such that for all $x, y \in E$ with $|x - x_0| < \delta$ and $|y - x_0| < \delta$, we have:
    \[
        |f(x) - f(y)| < \epsilon
    \]
    To show that $f$ is continuous at point $x_0$, let $\epsilon > 0$ be given. By assumption, there exists $\delta > 0$ such that for all $x, y \in E$ with $|x - x_0| < \delta$ and $|y - x_0| < \delta$, we have:
    \[
        |f(x) - f(y)| < \epsilon
    \]
    In particular, for any $x \in E$ with $|x - x_0| < \delta$, we can choose $y = x_0$. Thus, we have:
    \[
        |f(x) - f(x_0)| < \epsilon
    \]
    Therefore, by the definition of continuity, we conclude that $f$ is continuous at point $x_0$.
\end{proof}

\begin{definition}[Continuity on the left and on the right]
    Let $f: E \to F$ be a function and $x_0 \in E$. The function $f$ is said to be continuous on the \textbf{right} at point $x_0$ if:
    \[
        \lim_{x \to x_0^+} f(x) = f(x_0)
    \]
    The function $f$ is said to be continuous on the \textbf{left} at point $x_0$ if:
    \[
        \lim_{x \to x_0^-} f(x) = f(x_0)
    \]
\end{definition}
Remark that $f$ is continuous at point $x_0$ if and only if it is continuous on the left and on the right at point $x_0$.

\begin{eg}
    Let's study the continuity of a function $f$ in $x = 0$. Consider the function:
    \[
        f(x) = \begin{cases}
            2x + 1, & x \geq 0 \\
            \frac{\sin x}{x}, & x < 0
        \end{cases}
    \]
    Let's compute the right limit:
    \[
        \lim_{x \to 0^+} f(x) = \lim_{x \to 0^+} (2x + 1) = 1
    \]
    Let's compute the left limit:
    \[
        \lim_{x \to 0^-} f(x) = \lim_{x \to 0^-} \frac{\sin x}{x} = 1
    \]
    Since both limits are the same, we conclude that $f$ is continuous at point $x = 0$.
\end{eg}

\begin{eg}
    Let's study the continuity of a function $f$ in $x = 0$. Consider the function:
    \[
        f(x) = \begin{cases}
            2x + 1, & x > 0 \\
            2, & x = 0 \\
            \frac{\sin x}{x}, & x < 0
        \end{cases}
    \]
    Let's compute the right limit:
    \[
        \lim_{x \to 0^+} f(x) = \lim_{x \to 0^+} (2x + 1) = 1
    \]
    Let's compute the left limit:
    \[
        \lim_{x \to 0^-} f(x) = \lim_{x \to 0^-} \frac{\sin x}{x} = 1
    \]
    Since both limits are the same but different from $f(0) = 2$, we conclude that $f$ is not continuous at point $x = 0$.
\end{eg}

\subsection{Operations on Continuous Functions}
Let $f, g: E \to F$ be two functions continuous at point $x_0 \in E$. The following operations produce new functions that are also continuous at point $x_0$:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item The sum function $h(x) = \alpha f(x) + \beta g(x)$ is continuous at point $x_0$ (for any $\alpha, \beta \in \mathbb{R}$).
    \item The product function $h(x) = f(x) \cdot g(x)$ is continuous at point $x_0$.
    \item The quotient function $h(x) = \frac{f(x)}{g(x)}$ is continuous at point $x_0$ if $g(x_0) \neq 0$.
    \item If $f: E \to F$ and $g: G \to H$, $f(E) \subset G$, $f$ is continuous at point $x_0 \in E$ and $g$ is continuous at point $f(x_0) \in G$, then the composition function $h(x) = g(f(x))$ is continuous at point $x_0$.
\end{itemize}
Remark that if $(g \circ f)$ is continuous at point $x_0$, it does not necessarily imply that $f$ is continuous at point $x_0$ or that $g$ is continuous at point $f(x_0)$.

\begin{eg}
    Let $f$ be a function defined as:
    \[
        f(x) = \frac{\sqrt{e^{2x^2 + \sin x} + 8x^2} + \ln (x^2 + 1)}{x^4 + 8}
    \]
    We can see that $f$ is a composition of continuous functions (polynomial, exponential, square root, logarithm) and thus it is continuous for all $x \in \mathbb{R}$.
\end{eg}

\begin{eg}
    Let consider the functions:
    \[
    f(x) = \begin{cases}
        |x|, & x \neq 0 \\
        -1, & x = 0
    \end{cases} \quad \text{and} \quad g(x) = \begin{cases}
        0, & x \geq -1 \\
        1, & x < -1
    \end{cases}
    \]
    We have:
    \[
        (g \circ f)(x) = g(f(x)) = 0 \quad \text{for all} \ x \in \mathbb{R}
    \]
    Thus, $(g \circ f)(x)$ is continuous at point $x = 0$, but $f(x)$ is not continuous at point $x = 0$ and $g(x)$ is not continuous at point $f(0) = -1$.
\end{eg}

% TEACHER: "a l'exanen il y a toujours une questions sur un prolongement par continuti, il faut juste faire les limites et voir si elles sont gales, si oui on peut prolonger la fonction en ce point en mettant la valeur de la limite"
\subsection{Extension by Continuity}
\begin{definition}[Extension by Continuity]
    Let $f: E \to F$ be a function and $x_0$ be an accumulation point of $E$ but not in $E$. If the limit $\lim_{x \to x_0} f(x) = L$ exists, we can define a new function $\tilde{f}: E \cup \{x_0\} \to F$ as:
    \[
        \tilde{f}(x) = \begin{cases}
            f(x), & x \in E \\
            L, & x = x_0
        \end{cases}
    \]
    The function $\tilde{f}$ is called the extension by continuity of $f$ at point $x_0$.
\end{definition}

\begin{eg}
    Let's consider the function:
    \[
        f(x) = x \sin\left(\frac{1}{x}\right)
    \]
    defined on $E = \mathbb{R} \setminus \{0\}$. We have:
    \[
        \lim_{x \to 0} f(x) = \lim_{x \to 0} x \sin\left(\frac{1}{x}\right) = 0
    \]
    Thus, we can define the extension by continuity of $f$ at point $x_0 = 0$ as:
    \[
        \tilde{f}(x) = \begin{cases}
            x \sin\left(\frac{1}{x}\right), & x \neq 0 \\
            0, & x = 0
        \end{cases}
    \]
    The function $\tilde{f}$ is continuous on $\mathbb{R}$.
\end{eg}

\section{Continuous Functions on Intervals}
\begin{definition}[Continuous Function on an Interval]
    A function $f: I \to F$ where $I$ is an open, non-empty interval, is said to be continuous on the interval $I$ if it is continuous at every point $x_0 \in I$ i.e. for $f: [a, b] \to F$ to be continuous on $[a, b]$, it must be continuous on $(a, b)$, continuous on the right at point $a$ and continuous on the left at point $b$.
\end{definition}

\begin{eg}
    Let's consider the function:
    \[
        f(x) = \begin{cases}
            \frac{1}{2x + 3}, & x \neq -\frac{3}{2} \\
            0, & x = -\frac{3}{2}
        \end{cases}
    \]
    The function $f$ is continuous on the interval $\left(-\infty, -\frac{3}{2}\right)$ and $\left(-\frac{3}{2}, +\infty\right)$ but not continuous at point $x = -\frac{3}{2}$ since the limit $\lim_{x \to -\frac{3}{2}} f(x)$ does not exist.
\end{eg}

\begin{theorem}
    Let $a < b \in \mathbb{R}$ and $f: [a, b] \to F$ be a continuous function on the bounded and closed interval $[a, b]$. Then, $f$ reaches its minimum and maximum on $[a, b]$, i.e., there exist $c, d \in [a, b]$ such that:
    \[
        f(c) = \min_{x \in [a, b]} f(x) \quad \text{and} \quad f(d) = \max_{x \in [a, b]} f(x)
    \]
\end{theorem}
% TODO: maybe add graphs to illustrate why all the requirements of the theorem are necessary (bounded, closed interval, continuity) - w9c2 @ 55:00

\begin{theorem}[Indeterminate Value Theorem]
    Let $a < b \in \mathbb{R}$ and $f: [a, b] \to \mathbb{R}$ be a continuous function on the bounded and closed interval $[a, b]$. Then, for every $L$ in $[f(a), f(b)]$, there exists at least one $c \in [a, b]$ such that:
    \[
        f(c) = L
    \]
    i.e.:
    \[
        f([a, b]) = [\min_{x \in [a, b]} f(x), \max_{x \in [a, b]} f(x)]
    \]
\end{theorem}
Remark the following:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item If $f(a) \cdot f(b) < 0$, then there exists at least one $c \in [a, b]$ such that $f(c) = 0$.
    \item Given an open interval $I$ and a strictly monotonic, continuous function $f: I \to \mathbb{R}$, the image $f(I)$ is an open interval.
    \item For all injective and continuous function on an interval $I$, the function is strictly monotonic.
    \item For all bijective and continuous function on an interval $I$, the inverse function is also continuous and strictly monotonic on the interval $f(I)$.
\end{itemize}

\begin{eg}
    Let's consider the following graph:
    \begin{center}
        \begin{tikzpicture}
            \draw[->] (-0.25,0) -- (4.7,0) node[right] {$x$};
            \draw[->] (0,-2.5) -- (0,2.5) node[above] {$y$};
            \draw[thick,primary,domain=0:3.7,smooth,variable=\x] plot ({\x},{(\x - 2)*(\x - 2) - 2}) node[right] {$f(x)$};

            \draw[primary] (0.1,2) -- (-0.1,2) node[left] {$\max_{[a,b]} f(x)$};
            \draw[primary, dashed] (2,-2) -- (-0.1,-2) node[left] {$\min_{[a,b]} f(x)$};
            \draw[primary] (0.1,-2) -- (-0.1,-2);

            \node[below left, primary] at (0,0) {$a$};
            \draw[dashed, primary] (3.7, 0.89) -- (3.7,0) node[below, primary] {$b$};

            \draw[dashed, secondary] (4, -1) -- (-0.1,-1) node[left, secondary] {$L$};

            \draw[dashed, secondary] (3, -1) -- (3,0) node[above, secondary] {$c_1$};
            \draw[dashed, secondary] (1, -1) -- (1,0) node[above, secondary] {$c_2$};
        \end{tikzpicture}
    \end{center}
    We have $f: [a, b] \to \mathbb{R}$ continuous on $[a, b]$. According to the Indeterminate Value Theorem, there exist at least a point (in this case two) $c_1, c_2 \in [a, b]$ such that $f(c_1) = f(c_2) = L$ for any $L \in [\min_{x \in [a, b]} f(x), \max_{x \in [a, b]} f(x)]$.
\end{eg}

\begin{eg}
    Let's show that the equation $\sin x + \frac{1}{x - 4} = 0$ has at least two solution in the interval $[0, \pi]$. We first notice that the function is continuous on the interval since it is the sum of two continuous functions on $[0, \pi]$ and the interval is bounded and closed, thus we can use the Intermediate Value Theorem. We have:
    \[
        f(0) = \sin 0 + \frac{1}{0 - 4} = -\frac{1}{4} < 0
    \]
    and
    \[
        f(\pi) = \sin \pi + \frac{1}{\pi - 4} = 0 + \frac{1}{\pi - 4} < 0
    \]
    Since both values are negative, we need to find a point where the function is positive. Let's try $x = \frac{\pi}{2}$:
    \[
        f\left(\frac{\pi}{2}\right) = \sin \frac{\pi}{2} + \frac{1}{\frac{\pi}{2} - 4} = 1 + \frac{1}{\frac{\pi}{2} - 4} > 0
    \]
    Thus, by the Intermediate Value Theorem, there exists at least one $c_1 \in (0, \frac{\pi}{2})$ such that $f(c_1) = 0$ and at least one $c_2 \in (\frac{\pi}{2}, \pi)$ such that $f(c_2) = 0$. Therefore, the equation $\sin x + \frac{1}{x - 4} = 0$ has at least two solutions in the interval $[0, \pi]$.
\end{eg}

\section{Exercices}
This section gathers a selection of exercises related to Chapter \thechapter, taken from weekly assignments, past exams, textbooks, and other sources. The origin of each exercise will be indicated at its beginning.

\begin{exercise}[Quizz of Lecture 13]
    Let $f(x) = 2 \sin(1 - x^2)$ on the biggest interval where it is bijective and containing $x= 1$. Then the set of the definition of $f^{-1}$ and its image is:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $f^{-1}: \left[-2 \sin(1), 2 \sin(1)\right] \to \left[\sqrt{1 - \frac{\pi}{2}}, \sqrt{1 + \frac{\pi}{2}}\right]$
        \item $f^{-1}: \left[0, 2 \sin(1)\right] \to \left[0, \sqrt{1 + \pi}\right]$
        \item $f^{-1}: \left[-2, 2 \sin(1)\right] \to \left[0, \sqrt{1 + \frac{\pi}{2}}\right]$
        \item $f^{-1}: \left[-2, 2 \sin(1)\right] \to \left[-\sqrt{1 + \frac{\pi}{2}}, \sqrt{1 + \frac{\pi}{2}}\right]$
        \item $f^{-1}: \left[-2 \sin(1), 0\right] \to \left[0, \sqrt{1 + \pi}\right]$
    \end{itemize}
    \Answer
    The correct answer is the 3rd proposition because we have:
    \[
        f(x) = 2 \sin(1 - x^2) \quad \implies \quad - \frac{\pi}{2} + 2k\pi \leq 1 - x^2 \leq \frac{\pi}{2} + 2k\pi
    \]
    for $k = 0$ since $x = 1$ is in the interval. Thus:
    \[
        1 - \frac{\pi}{2} \leq x^2 \leq 1 + \frac{\pi}{2} \quad \iff \quad 0 \leq x^2 \leq 1 + \frac{\pi}{2}
    \]
    since $1 - \frac{\pi}{2} < 0$ and $x^2 \geq 0$. Therefore:
    \[
        0 \leq x \leq \sqrt{1 + \frac{\pi}{2}}
    \]
    Thus the domain of $f(x)$ is $\left[0, \sqrt{1 + \frac{\pi}{2}}\right] = E$ for it to be bijective and containing $x = 1$. Furthermore, we see that $f$ is decreasing on $E$ thus:
    \[
        \inf_{x \in E} f(x) = f\left(\sqrt{1 + \frac{\pi}{2}}\right) = -2 \quad \text{and} \quad \sup_{x \in E} f(x) = f(0) = 2 \sin(1)
    \]
    Therefore, the image of $f$ is $\left[-2, 2 \sin(1)\right]$.
\end{exercise}

\begin{exercise}[Quizz of Lecture 14]
    Let's compute the limit:
    \[
        \lim_{x \to 0} \frac{\sqrt{1 + x^2}}{\sqrt{1 + \left(\sin\frac{1}{x}\right)^2} + \sqrt{1 - \left(\sin\frac{1}{x}\right)^2}}
    \]
    \Answer
    Since $x^2 \geq 0$, we have:
    \[
        1 \leq \sqrt{1 + x^2} \leq \underbrace{1 + \frac{1}{2}x^2}_{\to 1 \ (x \to 0)}
    \]
    Thus by the Sandwich Theorem, we conclude that $\lim_{x \to 0} \sqrt{1 + x^2} = 1$. Let's take two sequences $(a_k) = \frac{1}{\pi k}$ and $(b_k) = \frac{1}{\frac{\pi}{2} + 2\pi k}$ such that $\lim_{k \to \infty} a_k = b_k = 0$. We have:
    \[
        \lim_{k \to \infty} \left(\sqrt{1 + \left(\sin \frac{1}{a_k}\right)^2} + \sqrt{1 - \left(\sin \frac{1}{a_k}\right)^2}\right) = \lim_{k \to \infty} \left(\sqrt{1 + \underbrace{(\sin a_k)^2}_{= 0}} + \sqrt{1 - \underbrace{(\sin a_k)^2}_{= 0}}\right)
    \]
    \[
        \lim_{k \to \infty} (1 + 1) = 2
    \]
    and:
    \[
        \lim_{k \to \infty} \left(\sqrt{1 + \left(\sin \frac{1}{b_k}\right)^2} + \sqrt{1 - \left(\sin \frac{1}{b_k}\right)^2}\right) = \lim_{k \to \infty} \left(\sqrt{1 + \underbrace{(\sin b_k)^2}_{= 1}} + \sqrt{1 - \underbrace{(\sin b_k)^2}_{= 1}}\right)
    \]
    \[
        \lim_{k \to \infty} (\sqrt{2} + 0) = \sqrt{2}
    \]
    Thus:
    \[
        \lim_{k \to \infty} f(a_k) = \frac{1}{2} \quad \text{and} \quad \lim_{k \to \infty} f(b_k) = \frac{1}{\sqrt{2}}
    \]
    Since the two limits are different, we conclude that the limit $\lim_{x \to 0} f(x)$ does not exist.
\end{exercise}