\chapter{Differential Calculus}

\section{Differentiability of Functions}

\begin{definition}[Derivative]
    It is said that $f$ is differentiable at a point $x_0 \in I$ if the following limit exists and is finite:
    \[
        f'(x_0) = \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0}.
    \]
    The value $f'(x_0)$ is called the derivative of $f$ at the point $x_0$.
\end{definition}
Remark that if $f$ is differentiable at $x_0$, then it can be written as:
\[
    f(x) = f(x_0) + f'(x_0)(x - x_0) + r(x)
\]
where the remainder term $r(x) = f(x) - f(x_0) - f'(x_0)(x - x_0)$ satisfies:
\[    \lim_{x \to x_0} \frac{r(x)}{x - x_0} = \lim_{x \to x_0} \left(\underbrace{\frac{f(x) - f(x_0)}{x - x_0}}_{\to f'(x_0)} - f'(x_0)\right) = 0. \]

\begin{definition}[Linear Approximation]
    Let $f$ be a function that is differentiable at a point $x_0 \in I$. The linear approximation of $f$ at the point $x_0$ is defined by:
    \[
        f(x) = f(x_0) + a(x - x_0) + r(x).
    \]
    where $a \in \mathbb{R}$. The value $a$ is called the slope of the linear approximation and is equal to the derivative of $f$ at the point $x_0$, i.e. $a = f'(x_0)$.
\end{definition}


\begin{eg}
    Let's compute the derivative of the function $f(x) = x^2$. Let $x_0 \in \mathbb{R}$. We have:
    \[
        f'(x_0) = \lim_{x \to x_0} \frac{x^2 - x_0^2}{x - x_0} = \lim_{x \to x_0} \frac{(x - x_0)(x + x_0)}{x - x_0} = \lim_{x \to x_0} (x + x_0) = 2x_0.
    \]
    Therefore, the derivative of the function $f(x) = x^2$ is given by $f'(x) = 2x$ for all $x \in \mathbb{R}$.
\end{eg}

\begin{eg}
    Let's compute the derivative of the function $f(x) = \cos x$. Let $x_0 \in \mathbb{R}$. We have:
    \begin{align*}
        f'(x_0) &= \lim_{x \to x_0} \frac{\cos x - \cos x_0}{x - x_0} = \lim_{x \to x_0} \frac{\cos(x_0 + (x - x_0)) - \cos x_0}{x - x_0} \\
        &= \lim_{x \to x_0} \frac{\cos x_0 \cos(x - x_0) - \sin x_0 \sin(x - x_0) - \cos x_0}{x - x_0} \\
        &= \lim_{x \to x_0} \left[\frac{-\sin x_0 \cdot \sin (x - x_0)}{x-x_0} + \frac{\cos x_0 (\cos(x - x_0) - 1)}{x - x_0}\right] \\
        &= \lim_{x \to x_0} \left[-\sin x_0  \cdot \underbrace{\frac{\sin(x-x_0)}{x-x_0}}_{\to 1} + \cos x_0 \cdot \underbrace{\frac{\left(-2 \sin^2 \left(\frac{x - x_0}{2}\right)\right)}{\left(\frac{x - x_0}{2}\right)^2}}_{\to -2} \cdot \underbrace{\frac{\left(\frac{x - x_0}{2}\right)^2}{x - x_0}}_{\to 0}\right] \\
        &= -\sin x_0.
    \end{align*}
    Therefore, the derivative of the function $f(x) = \cos x$ is given by $f'(x) = -\sin x$ for all $x \in \mathbb{R}$.
\end{eg}

% \begin{eg}
%     Let's compute the derivative of the function $f(x) = \sin x$. Let $x_0 \in \mathbb{R}$. We have:
%     \[
%         f'(x_0) = \lim_{h \to 0} \frac{\sin(x_0 + h) - \sin x_0}{h} = \lim_{h \to 0} \frac{\sin x_0 \cos h + \cos x_0 \sin h - \sin x_0}{h}.
%     \]
%     Using the limits $\lim_{h \to 0} \frac{\sin h}{h} = 1$ and $\lim_{h \to 0} \cos h = 1$, we get:
%     \[
%         f'(x_0) = \sin x_0 \cdot 0 + \cos x_0 \cdot 1 = \cos x_0.
%     \]
%     Therefore, the derivative of the function $f(x) = \sin x$ is given by $f'(x) = \cos x$ for all $x \in \mathbb{R}$.
% \end{eg}

\begin{definition}[Derivative Function]
    Let $f: E \to \mathbb{R}$ be a function differentiable on a set $D(f') \subset E$. The derivative function of $f$ is defined as the function $f': D(f') \to \mathbb{R}$ such that:
    \[
        f(x) = f'(x) \quad \forall x \in D(f').
    \]
\end{definition}

\begin{theorem}
    A derivable function $f$ at a point $x_0 \in I$ is continuous at $x_0$.
\end{theorem}
\begin{proof}
    Let $f: I \to \mathbb{R}$ be a function derivable at a point $x_0 \in I$. We have:
    \[
        \lim_{x \to x_0} f(x) = \lim_{x \to x_0} \left[f(x_0) + f'(x_0)(x - x_0) + r(x)\right] = f(x_0) + f'(x_0) \cdot 0 + \lim_{x \to x_0} r(x) = f(x_0).
    \]
    Therefore, the function $f$ is continuous at the point $x_0$.
\end{proof}
Remark that the converse is not true: a function can be continuous at a point but not derivable at that point.

\subsection{Operations on Derivatives}
Let $f, g: I \to \mathbb{R}$ be two functions derivable at a point $x_0 \in I$. The following operations can be performed on the derivatives of $f$ and $g$ at the point $x_0$:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item The function $\alpha f + \beta g$ is derivable at the point $x_0$ and:
    \[        (\alpha f + \beta g)'(x_0) = \alpha f'(x_0) + \beta g'(x_0). \]
    \item The function $f \cdot g$ is derivable at the point $x_0$ and:
    \[        (f \cdot g)'(x_0) = f'(x_0) \cdot g(x_0) + f(x_0) \cdot g'(x_0). \]
    \item If $g(x_0) \neq 0$, then the function $\frac{f}{g}$ is derivable at the point $x_0$ and:
    \[        \left(\frac{f}{g}\right)'(x_0) = \frac{f'(x_0) \cdot g(x_0) - f(x_0) \cdot g'(x_0)}{(g(x_0))^2}. \]
\end{itemize}
\begin{proof}
    Let's prove each of these properties one by one. \\
    \textbf{1. Linearity} We have:
    \begin{align*}
        (\alpha f + \beta g)'(x_0) &= \lim_{x \to x_0} \frac{\alpha f(x) + \beta g(x) - \alpha f(x_0) - \beta g(x_0)}{x - x_0} \\
        &= \lim_{x \to x_0} \left[\alpha \cdot \frac{f(x) - f(x_0)}{x - x_0} + \beta \cdot \frac{g(x) - g(x_0)}{x - x_0}\right] \\
        &= \alpha f'(x_0) + \beta g'(x_0).
    \end{align*}
    \textbf{2. Product Rule} We have:
    \begin{align*}
        (f \cdot g)'(x_0) &= \lim_{x \to x_0} \frac{f(x)g(x) - f(x_0)g(x_0)}{x - x_0} \\
        &= \lim_{x \to x_0} \frac{f(x)g(x) - f(x)g(x_0) + f(x)g(x_0) - f(x_0)g(x_0)}{x - x_0} \\
        &= \lim_{x \to x_0} \left[f(x) \cdot \frac{g(x) - g(x_0)}{x - x_0} + g(x_0) \cdot \frac{f(x) - f(x_0)}{x - x_0}\right] \\
        &= f(x_0) \cdot g'(x_0) + g(x_0) \cdot f'(x_0).
    \end{align*}
    \textbf{3. Quotient Rule} We have:
    \begin{align*}
        \left(\frac{f}{g}\right)'(x_0) &= \lim_{x \to x_0} \frac{\frac{f(x)}{g(x)} - \frac{f(x_0)}{g(x_0)}}{x - x_0} \\
        &= \lim_{x \to x_0} \frac{f(x)g(x_0) - f(x_0)g(x)}{(x - x_0) g(x) g(x_0)} \\
        &= \lim_{x \to x_0} \frac{g(x_0) [f(x) - f(x_0)] - f(x_0) [g(x) - g(x_0)]}{(x - x_0) g(x) g(x_0)} \\
        &= \frac{g(x_0) f'(x_0) - f(x_0) g'(x_0)}{(g(x_0))^2}.
    \end{align*}
\end{proof}

\subsection{Derivatives of Usual Functions}
\vskip0.3cm
\begin{center}
    \begin{tabular}{p{0.45\textwidth} | p{0.45\textwidth}}
        \\ {\centering \textbf{$f(x)$} \par} & {\centering \textbf{$f'(x)$} \par} \\ \\ \hline \\
        {\centering $c$ (constant function) \par} & {\centering $0$ \par} \\ \\
        {\centering$x^n$ ($n \in \mathbb{N}$) \par} & {\centering $n x^{n-1}$ \par} \\ \\
        {\centering$e^x$ \par} & {\centering $e^x$ \par} \\ \\
        % {\centering$\ln x$ \par} & {\centering $\frac{1}{x}$ \par} \\ \\
        {\centering$\sin x$ \par} & {\centering $\cos x$ \par} \\ \\
        {\centering$\cos x$ \par} & {\centering $-\sin x$ \par} \\ \\
        {\centering$\tan x$ \par} & {\centering $\sec^2 x = \frac{1}{(\cos x)^2}$ \par} \\ \\
        % {\centering$\arcsin x$ \par} & {\centering $\frac{1}{\sqrt{1 - x^2}}$ \par} \\ \\
        % {\centering$\arccos x$ \par} & {\centering $-\frac{1}{\sqrt{1 - x^2}}$ \par} \\ \\
        % {\centering$\arctan x$ \par} & {\centering $\frac{1}{1 + x^2}$ \par} \\ \\
    \end{tabular}
\end{center}
\begin{proof}
    Let's prove some of these results. \\
    \textbf{2. $f(x) = x^n$:} Let $x_0 \in \mathbb{R}$. By recurrence on $n$: \\
    For $n = 0$, we have $f(x) = 1$ and $f'(x) = 0$. \\
    Assume that the result is true for some $n \in \mathbb{N}$. We have:
    \begin{align*}
        f'(x_0) &= \lim_{x \to x_0} \frac{x^{n+1} - x_0^{n+1}}{x - x_0} = \lim_{x \to x_0} \frac{(x - x_0)(x^n + x^{n-1}x_0 + \ldots + x_0^n)}{x - x_0} \\
        &= \lim_{x \to x_0} (x^n + x^{n-1} x_0 + \ldots + x_0^n) = (n + 1) x_0^n.
    \end{align*}
    \textbf{3. $f(x) = e^x$}: Let $x_0 \in \mathbb{R}$. We have:
    \begin{align*}
        f'(x_0) &= \lim_{x \to x_0} \frac{e^x - e^{x_0}}{x - x_0} = \lim_{x \to x_0} \frac{e^{x_0}(e^{x - x_0} - 1)}{x - x_0} \\
        &= e^{x_0} \cdot 1 = e^{x_0}.
    \end{align*}
    \textbf{6. $f(x) = \tan x $:} By the quotient rule, we have:
    \[        f'(x) = \left(\frac{\sin x}{\cos x}\right)' = \frac{\cos x \cdot \cos x - \sin x \cdot (-\sin x)}{(\cos x)^2} = \frac{(\cos x)^2 + (\sin x)^2}{(\cos x)^2} = \frac{1}{(\cos x)^2}. \]
\end{proof}

\subsection{Geometric Interpretation of the Derivative}
It can easily be seen from the definition of the derivative that the derivative at a point $x_0$ represents the slope of the tangent line $t(x)$ to the graph of the function $f$ at the point $(x_0, f(x_0))$.
\begin{center}
    \begin{tikzpicture}
        \draw[->] (-0.25,0) -- (4.25,0) node[right] {$x$};
        \draw[->] (0,-0.25) -- (0,4.25) node[above] {$y$};

        \draw[thick,primary,domain=0.25:4,smooth,variable=\x] plot ({\x},{1/\x}) node[above right] {$f(x)$};
        \draw[thick,secondary,domain=-0.25:1.75,smooth,variable=\x] plot ({\x},{-1.78*(\x - 0.75) + 1.33}) node[right] {$t(x)$};

        \draw[dashed, primary] (0.75,1.33) -- (0.75,0) node[below] {$x_0$};
    \end{tikzpicture}
\end{center}

\begin{definition}[Equation of the Tangent Line]
    Let $f: I \to \mathbb{R}$ be a function differentiable at a point $x_0 \in I$. The equation of the tangent line $t(x)$ to the graph of $f$ at the point $(x_0, f(x_0))$ is given by:
    \[
        t(x) = f(x_0) + f'(x_0)(x - x_0)
    \]
\end{definition}

\subsection{Right and Left Derivatives}
\begin{definition}[Right and Left Derivatives]
    Let $f: I \to \mathbb{R}$ be a function and $x_0 \in I$. \\
    \textbf{The right derivative} of $f$ at the point $x_0$ is defined as:
    \[
        f'_+(x_0) = \lim_{x \to x_0^+} \frac{f(x) - f(x_0)}{x - x_0}
    \]
    \textbf{The left derivative} of $f$ at the point $x_0$ is defined as:
    \[
        f'_-(x_0) = \lim_{x \to x_0^-} \frac{f(x) - f(x_0)}{x - x_0}
    \]
\end{definition}
Remark that if the right and left derivatives at a point $x_0$ exist and are equal, then the function is derivable at that point and the derivative is equal to the common value of the right and left derivatives, i.e.:
\[    f'(x_0) = f'_+(x_0) = f'_-(x_0) \]

\begin{eg}
    Let's show that the function $f(x) = |x|$ is not derivable at the point $x_0 = 0$ (even if it is continuous on $\mathbb{R}$). We have:
    \[
        f'(0) = \lim_{x \to 0} \frac{|x| - |0|}{x - 0} = \lim_{x \to 0} \frac{|x|}{x}
    \]
    We compute the left-hand limit and the right-hand limit:
    \[
        \lim_{x \to 0^-} \frac{|x|}{x} = \lim_{x \to 0^-} \frac{-x}{x} = -1
    \]
    \[
        \lim_{x \to 0^+} \frac{|x|}{x} = \lim_{x \to 0^+} \frac{x}{x} = 1
    \]
    Since the left-hand limit and the right-hand limit are not equal, the limit $\lim_{x \to 0} \frac{|x|}{x}$ does not exist. Therefore, the function $f(x) = |x|$ is not derivable at the point $x_0 = 0$.
\end{eg}

\subsection{Infinite Derivative}
\begin{definition}[Infinite Derivative]
    Let $f: I \to \mathbb{R}$ be a function and $x_0 \in I$. It is said that $f$ has an infinite derivative at the point $x_0$ if:
    \[
        \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0} = \pm \infty
    \]
\end{definition}
Remark that if a function has an infinite derivative at a point $x_0$, then the tangent line to the graph of the function at that point is vertical.

\begin{eg}
    Let $f(x) = \sqrt[3]{x}$. We are going to show that $f$ has an infinite derivative at the point $x_0 = 0$. We have:
    \[
        \lim_{x \to 0} \frac{\sqrt[3]{x} - \sqrt[3]{0}}{x - 0} = \lim_{x \to 0} \frac{\sqrt[3]{x}}{x} = \lim_{x \to 0} \frac{1}{\sqrt[3]{x^2}}
    \]
    We compute the left-hand limit and the right-hand limit:
    \[        \lim_{x \to 0^-} \frac{1}{\sqrt[3]{x^2}} = +\infty \]
    \[        \lim_{x \to 0^+} \frac{1}{\sqrt[3]{x^2}} = +\infty \]
    Since the left-hand limit and the right-hand limit are equal, the limit $\lim_{x \to 0} \frac{\sqrt[3]{x}}{x}$ is $+\infty$. Therefore, we can say that $f$ has an infinite derivative at the point $x_0 = 0$. Therefore, the tangent line to the graph of the function at the point $(0, 0)$ is vertical.
\end{eg}

\subsection{Derivative of Composite Functions}
\begin{theorem}[Chain Rule]
    Let $f: I \to \mathbb{R}$ and $g: J \to \mathbb{R}$ be two functions such that $f(I) \subset J$. If $f$ is derivable at a point $x_0 \in I$ and $g$ is derivable at the point $f(x_0) \in J$, then the composite function $g \circ f: I \to \mathbb{R}$ is derivable at the point $x_0$ and:
    \[        (g \circ f)'(x_0) = g'(f(x_0)) \cdot f'(x_0) \]
\end{theorem}
\begin{proof}
    Let $f: I \to \mathbb{R}$ and $g: J \to \mathbb{R}$ be two functions such that $f(I) \subset J$. We have:
    \begin{align*}
        (g \circ f)'(x_0) &= \lim_{x \to x_0} \frac{g(f(x)) - g(f(x_0))}{x - x_0} \\
        &= \lim_{x \to x_0} \left[\frac{g(f(x)) - g(f(x_0))}{f(x) - f(x_0)} \cdot \frac{f(x) - f(x_0)}{x - x_0}\right] \\
        &= g'(f(x_0)) \cdot f'(x_0)
    \end{align*}
\end{proof}

\begin{eg}
    Let's compute the derivative of the function $f(x) = e^{2x^2 + \sin x}$. We have:
    \[        f'(x) = e^{2x^2 + \sin x} \cdot (4x + \cos x) \]
\end{eg}

\begin{eg}
    Let's compute the derivative of the function $f(x) = \frac{\sin x}{e^x - e^{-x}}$. We have:
    \[        f'(x) = \frac{\cos x (e^x - e^{-x}) - \sin x (e^x + e^{-x})}{(e^x - e^{-x})^2} \]  
\end{eg}

\subsection{Derivative of Inverse Functions}
\begin{theorem}
    Let $f: I \to F$ a bijective function that is continuous and differentiable at a point $x_0 \in I$ such that $f'(x_0) \neq 0$. Then, the inverse function $f^{-1}: F \to I$ is differentiable at the point $y_0 = f(x_0)$ and:
    \[        (f^{-1})'(y_0) = \frac{1}{f'(x_0)} \]
\end{theorem}
\begin{proof}
    Let $f: I \to F$ a bijective function that is continuous and differentiable at a point $x_0 \in I$ such that $f'(x_0) \neq 0$. We have:
    \begin{align*}
        (f^{-1})'(y_0) &= \lim_{y \to y_0} \frac{f^{-1}(y) - f^{-1}(y_0)}{y - y_0} = \lim_{y \to y_0} \frac{f^{-1}(y) - f^{-1}(f(x_0))}{y - f(x_0)} \\
        &= \lim_{x \to x_0} \frac{x - x_0}{f(x) - f(x_0)} = \frac{1}{\lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0}} = \frac{1}{f'(x_0)}
    \end{align*}
\end{proof}
Remark that if $f: I \to F$ and $f^{-1}: F \to I$ two reciprocal and continuous function that are differentiable at every point of their respective domains, such that $f'(f^{-1}(x)) \neq 0$, then:
\[    (f^{-1})'(x) = \frac{1}{f'(f^{-1}(x))} = \left. \frac{1}{f'(y)}{y = f^{-1}(x)}\right|_{y = f^{-1}(x)} \quad \forall x \in F \]

\begin{eg}
    Let's compute the derivative of the function $f(x) = \arccos x$ ($[-1, 1] \to [0, \pi]$) by its inverse function $\cos x$ ($[0, \pi] \to [-1, 1]$). We have:
    \[        f'(x) = \frac{1}{\left. (\cos y)'\right|_{y = \arccos x}} = \frac{1}{-\sin(\arccos x)} = -\frac{1}{\sqrt{1 - (\cos (\arccos x))^2}} = -\frac{1}{\sqrt{1 - x^2}} \]
    Remark that $\sin y = \pm \sqrt{1 - \cos^2 y}$ and since $y \in [0, \pi]$, we have $\sin y \geq 0$ thus we choose the positive root.
\end{eg}

\begin{eg}
    Let's compute the derivative of the function $f(x) = \ln x$ ($\mathbb{R}^+ \to \mathbb{R}$) by its inverse function $e^x$ ($\mathbb{R} \to \mathbb{R}^+$). We have:
    \[        f'(x) = \frac{1}{\left. (e^y)'\right|_{y = \ln x}} = \frac{1}{e^{\ln x}} = \frac{1}{x} \]
\end{eg}
Remark that $a^x = e^{x \ln a}$ for all $x \in \mathbb{R}$ and $a > 0, a \neq 1$. Thus, by the chain rule, we have:
\[    (a^x)' = (e^{x \ln a})' = e^{x \ln a} \cdot \ln a = a^x \ln a \]
and by the inverse function theorem, we have:
\[    (\log_a x)' = \frac{1}{(a^{\log_a x})'|_{x = \log_a x}} = \frac{1}{a^{\log_a x} \cdot \ln a} = \frac{1}{x \ln a} \]
\begin{eg}
    Let $r \in \mathbb{R}$ and $f: \mathbb{R}^*_+ \to \mathbb{R}^*_+$ defined by $f(x) = x^r = e^{r \ln x}$. We have:
    \[        f'(x) = \left(e^{r \ln x}\right)' = e^{r \ln x} \cdot \frac{r}{x} = r x^{r - 1} \]
\end{eg}

% \begin{eg}
%     Let's compute the derivative of the function $f(x) = f_1(x)^{f_2(x)}$. We have:
%     \[
%         f(x) = e^{f_2(x) \ln (f_1(x))}
%     \]
%     where $f_1(x) > 0$, thus, by the chain rule, we get:
%     \[        f'(x) = f_1(x)^{f_2(x)} \cdot \left[f_2'(x) \ln (f_1(x)) + f_2(x) \cdot \frac{f_1'(x)}{f_1(x)}\right] \]
% \end{eg}
\begin{theorem}
    Let $f_1(x) > 0$ and $f_2(x)$ be two differentiable functions on an interval $I$. The function $f: I \to \mathbb{R}$ defined by $f(x) = f_1(x)^{f_2(x)}$ is differentiable on $I$ and:
    \[        f'(x) = f_1(x)^{f_2(x)} \cdot \left(f_2 (x) \cdot \ln(f_1(x))\right)' \]
\end{theorem}
\begin{proof}
    Let $f_1(x) > 0$ and $f_2(x)$ be two differentiable functions on an interval $I$. We have:
    \[
        f(x) = f_1(x)^{f_2(x)} = e^{f_2(x) \ln (f_1(x))}
    \]
    Thus, by the chain rule, we get:
    \[
        f'(x) = \left(e^{f_2(x) \ln (f_1(x))}\right)' = e^{f_2(x) \ln (f_1(x))} \cdot \left(f_2 (x) \cdot \ln(f_1(x))\right)' = f_1(x)^{f_2(x)} \cdot \left(f_2 (x) \cdot \ln(f_1(x))\right)' 
    \]
\end{proof}
More generally if $\ln(f(x))$ is defined, then:
\[    (\ln f(x))' = \frac{f'(x)}{f(x)} \implies f'(x) = f(x) \cdot (\ln f(x))' \]

\begin{eg}
    Let's compute the derivative of the function $f(x) = x^{x^x}$. By the logarithm rule, we have:
    \[
        f(x) = e^{x^x \ln x} = e^{e^{\ln x \cdot x} \ln x}
    \]
    Thus, by the chain rule, we get:
    \[        f'(x) = \left(e^{e^{\ln x \cdot x} \ln x}\right)' = x^{x^x} x^x \cdot \left[(\ln x)^2 + \ln x + \frac{1}{x}\right] \]
\end{eg}

\section{Hyperbolic Functions}
\begin{definition}[Sine and Cosine Hyperbolic Functions]
    The hyperbolic sine function is defined as follows:
    \[
        \sinh x = \frac{e^x - e^{-x}}{2}, \quad \text{odd} \ \forall x \in \mathbb{R}
    \]
    Graphically, the hyperbolic sine function can be represented as:
    \begin{center}
        \begin{tikzpicture}[scale=0.8]
            \draw[->] (-3.5,0) -- (3.5,0) node[right] {$x$};
            \draw[->] (0,-3) -- (0,3) node[above] {$y$};

            \draw[thick,primary,domain=-1.7:1.7,smooth,variable=\x] plot ({\x},{(exp(\x) - exp(-\x))/2}) node[above right] {$\sinh x$};
        \end{tikzpicture}
    \end{center}
    The hyperbolic cosine function is defined as follows:
    \[
        \cosh x = \frac{e^x + e^{-x}}{2}, \quad \text{even} \ \forall x \in \mathbb{R}
    \]
    Graphically, the hyperbolic cosine function can be represented as:
    \begin{center}
        \begin{tikzpicture}[scale=1]
            \draw[->] (-3,0) -- (3,0) node[right] {$x$};
            \draw[->] (0,-0.2) -- (0,3) node[above] {$y$};

            \draw[thick,primary,domain=-1.7:1.7,smooth,variable=\x] plot ({\x},{(exp(\x) + exp(-\x))/2}) node[above right] {$\cosh x$};
        \end{tikzpicture}
    \end{center}
\end{definition}
Remark that this definition is similar to the definition of the cosine and sine functions using Euler's formula:
\[    \sin x = \frac{e^{ix} - e^{-ix}}{2i}, \quad \cos x = \frac{e^{ix} + e^{-ix}}{2} \]
Some properties of hyperbolic functions are similar to those of trigonometric functions. For example, we have:
\[    \cosh^2 x - \sinh^2 x = 1 \quad \text{(similar to } \cos^2 x + \sin^2 x = 1\text{)} \]

\subsection{Derivatives of Hyperbolic Functions}
\begin{theorem}
    The derivatives of the hyperbolic sine and cosine functions are given by:
    \[        (\sinh x)' = \cosh x, \quad (\cosh x)' = \sinh x \quad \forall x \in \mathbb{R} \]
\end{theorem}
\begin{proof}
    We have:
    \[        (\sinh x)' = \left(\frac{e^x - e^{-x}}{2}\right)' = \frac{e^x + e^{-x}}{2} = \cosh x \]
    \[        (\cosh x)' = \left(\frac{e^x + e^{-x}}{2}\right)' = \frac{e^x - e^{-x}}{2} = \sinh x \]
\end{proof}

\subsection{Tangent and Cotangent Hyperbolic Functions}
\begin{definition}[Tangent and Cotangent Hyperbolic Functions]
    The hyperbolic tangent function is defined as follows:
    \[        \tanh x = \frac{\sinh x}{\cosh x} = \frac{e^x - e^{-x}}{e^x + e^{-x}} \quad \forall x \in \mathbb{R} \]
    Graphically, the hyperbolic tangent function can be represented as:
    \begin{center}
        \begin{tikzpicture}[scale=1]
            \draw[->] (-3,0) -- (3,0) node[right] {$x$};
            \draw[->] (0,-1.5) -- (0,1.5) node[above] {$y$};
            \draw[thick,primary,domain=-2.8:2.8,smooth,variable=\x] plot ({\x},{(exp(\x) - exp(-\x))/(exp(\x) + exp(-\x))}) node[right] {$\tanh x$};
        \end{tikzpicture}
    \end{center}
    The hyperbolic cotangent function is defined as follows:
    \[        \coth x = \frac{\cosh x}{\sinh x} = \frac{e^x + e^{-x}}{e^x - e^{-x}} \quad \forall x \in \mathbb{R}^* \]
    Graphically, the hyperbolic cotangent function can be represented as:
    \begin{center}
        \begin{tikzpicture}[scale=0.8]
            \draw[->] (-3,0) -- (3,0) node[right] {$x$};
            \draw[->] (0,-4) -- (0,4) node[above] {$y$};
            \draw[thick,primary,domain=-2.8:-0.3,smooth,variable=\x] plot ({\x},{(exp(\x) + exp(-\x))/(exp(\x) - exp(-\x))}) node[below left] {$\coth x$};
            \draw[thick,primary,domain=0.3:2.8,smooth,variable=\x] plot ({\x},{(exp(\x) + exp(-\x))/(exp(\x) - exp(-\x))});
        \end{tikzpicture}
    \end{center}
\end{definition}

\subsection{Arguments of Hyperbolic Functions}
\begin{definition}[Arguments of Hyperbolic Functions]
    The inverse hyperbolic sine function is defined as follows:
    \[        \operatorname{arsinh} x = \ln\left(x + \sqrt{x^2 + 1}\right) \quad \forall x \in \mathbb{R} \]
    The inverse hyperbolic cosine function is defined as follows:
    \[        \operatorname{arcosh} x = \ln\left(x + \sqrt{x^2 - 1}\right) \quad \forall x \geq 1 \]
    The inverse hyperbolic tangent function is defined as follows:
    \[        \operatorname{artanh} x = \frac{1}{2} \ln\left(\frac{1 + x}{1 - x}\right) \quad \forall x \in (-1, 1) \]
\end{definition}

\begin{theorem}
    The derivatives of the inverse hyperbolic functions are given by:
    \[        (\operatorname{arsinh} x)' = \frac{1}{\sqrt{x^2 + 1}}, \quad (\operatorname{arcosh} x)' = \frac{1}{\sqrt{x^2 - 1}}, \quad (\operatorname{artanh} x)' = \frac{1}{1 - x^2} \]
\end{theorem}
\begin{proof}
    We have:
    \[        (\operatorname{arsinh} x)' = \left(\ln\left(x + \sqrt{x^2 + 1}\right)\right)' = \frac{1 + \frac{x}{\sqrt{x^2 + 1}}}{x + \sqrt{x^2 + 1}} = \frac{\sqrt{x^2 + 1} + x}{\sqrt{x^2 + 1} (x + \sqrt{x^2 + 1})} = \frac{1}{\sqrt{x^2 + 1}} \]
    \[        (\operatorname{arcosh} x)' = \left(\ln\left(x + \sqrt{x^2 - 1}\right)\right)' = \frac{1 + \frac{x}{\sqrt{x^2 - 1}}}{x + \sqrt{x^2 - 1}} = \frac{\sqrt{x^2 - 1} + x}{\sqrt{x^2 - 1} (x + \sqrt{x^2 - 1})} = \frac{1}{\sqrt{x^2 - 1}} \]
    \[        (\operatorname{artanh} x)' = \left(\frac{1}{2} \ln\left(\frac{1 + x}{1 - x}\right)\right)' = \frac{1}{2} \cdot \frac{\frac{1}{1 + x} + \frac{1}{1 - x}}{\frac{1 + x}{1 - x}} = \frac{1}{2} \cdot \frac{(1 - x) + (1 + x)}{(1 - x)(1 + x)} = \frac{1}{1 - x^2} \]
\end{proof}

\section{Derivatives of Higher Order}
\begin{definition}[Higher Order Derivatives]
    Let $f: I \to \mathbb{R}$ be a function. If $f$ is differentiable on $I$, then the derivative function $f' : I \to \mathbb{R}$ exists. If $f'$ is also differentiable on $I$, then the second derivative function $f'' : I \to \mathbb{R}$ exists, and so on. The $n$-th derivative of $f$, denoted by $f^{(n)}$, is defined recursively as follows:
    \[        f^{(0)}(x) = f(x), \quad f^{(n)}(x) = (f^{(n-1)})'(x) \quad \forall n \in \mathbb{N}^* \]
\end{definition}

\begin{eg}
    Let's compute the n-th derivative of the function $f(x) = x^n$ where $n \in \mathbb{N}^*$. We have:
    \[        f'(x) = n x^{n-1}, \quad f''(x) = n (n-1) x^{n-2}, \quad \ldots, \quad f^{(n)}(x) = n! \]
    and for all $k > n$, we have:
    \[        f^{(k)}(x) = 0 \]
\end{eg}

\begin{definition}[Class $C^n$ Functions]
    Let $n \in \mathbb{N} \cup \{\infty\}$. A function $f: I \to \mathbb{R}$ is said to be of class $C^n$ on the interval $I$ if it is $n$ times differentiable on $I$ and its $n$-th derivative $f^{(n)}$ is continuous on $I$.
\end{definition}
Remark that the sine and cosine functions are of class $C^{\infty}$ on $\mathbb{R}$ ($C^{\infty}(\mathbb{R})$) since they are differentiable any number of times and all their derivatives are continuous on $\mathbb{R}$.

\section{Properties of Differentiable Functions}
\begin{theorem}
    If a function $f: E \to F$ is differentiable at a point $x_0 \in E$, such that $f$ has a local extremum at $x_0$, then:
    \[        f'(x_0) = 0 \]
\end{theorem}
\begin{proof}
    Let $f: E \to F$ be a function differentiable at a point $x_0 \in E$, such that $f$ has a local extremum at $x_0$. We have:
    \[
        f'(x_0) = \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0}
    \]
    If $f$ has a local maximum at $x_0$, then for all $x$ in a neighborhood of $x_0$, we have $f(x) \leq f(x_0)$. Thus, for $x > x_0$, we have:
    \[
        \frac{f(x) - f(x_0)}{x - x_0} \leq 0
    \]
    and for $x < x_0$, we have:
    \[
        \frac{f(x) - f(x_0)}{x - x_0} \geq 0
    \]
    Therefore, the right-hand limit is less than or equal to 0 and the left-hand limit is greater than or equal to 0. Since the limit exists, both limits must be equal, thus:
    \[
        f'(x_0) = 0
    \]
    The same reasoning applies if $f$ has a local minimum at $x_0$.
\end{proof}
Remark that the converse is not true, i.e., if $f'(x_0) = 0$, it does not imply that $f$ has a local extremum at $x_0$. For example, the function $f(x) = x^3$ has a derivative equal to 0 at the point $x_0 = 0$, but it does not have a local extremum at that point.

\begin{definition}[Critical Point]
    Let $f: E \to F$ be a function differentiable at a point $x_0 \in E$. The point $x_0$ is called a critical point of $f$ if:
    \[        f'(x_0) = 0 \]
\end{definition}
Remark that the extrema of a function $f: [a,b] \to \mathbb{R}$ can only occur at:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item Critical points
    \item Points where the function is not differentiable
    \item Endpoints of the interval ($x = a$ or $x = b$)
\end{itemize}

\subsection{Rolle's Theorem}

\begin{theorem}[Rolle's Theorem]
    Let $f: [a,b] \to \mathbb{R}$ be a function that is continuous on $[a,b]$ and differentiable on $(a,b)$ such that:
    \[        f(a) = f(b) \]
    Then, there exists at least one point $c \in (a,b)$ such that:
    \[        f'(c) = 0 \]
\end{theorem}
\begin{proof}
    Let $f: [a,b] \to \mathbb{R}$ be a function that is continuous on $[a,b]$ and differentiable on $(a,b)$ such that $f(a) = f(b)$. By the Extreme Value Theorem, $f$ attains its maximum and minimum values on the interval $[a,b]$. Let $M$ be the maximum value of $f$ on $[a,b]$ and let $m$ be the minimum value of $f$ on $[a,b]$: \\
    \textbf{If $M = m$}, then $f$ is constant on $[a,b]$, and thus $f'(x) = 0$ for all $x \in (a,b)$. \\
    \textbf{If $M \neq m$}, then either the maximum or minimum value is attained at some point $c \in (a,b)$. Without loss of generality, assume that the maximum value is attained at some point $c \in (a,b)$. Since $f$ is differentiable at $c$ and has a local maximum at that point, by the previous theorem, we have:
    \[        f'(c) = 0 \]
\end{proof}
Remark that if $f: (a, b) \to \mathbb{R}$ is a function that is differentiable and $\lim_{x \to a^+} f(x) = \lim_{x \to b^-} f(x) = \alpha$ where $\alpha \in \mathbb{R} \cup {+ \infty}$, then there exists at least one point $c \in (a,b)$ such that $f'(c) = 0$.

\begin{eg}
    Let's apply Rolle's theorem. Let $f: \mathbb{R} \to \mathbb{R}$ a function of class $C^{\infty}$ such that:
    \[
        f(0) = f(4) = 0 \quad \text{and} \quad f'(0) = f'(4) = 0
    \]
    Let's show that the equation $f''(x) = 0$ has at least two solutions in the interval $(0,4)$. \\
    Since $f(0) = f(4) = 0$, by Rolle's theorem, there exists at least one point $c_1 \in (0,4)$ such that:
    \[        f'(c_1) = 0
    \]
    Since $f'(0) = f'(4) = 0$, by Rolle's theorem, there exists at least one point $c_2 \in (0,4)$ such that:
    \[        f''(c_2) = 0
    \]
    Now, we have two cases:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item If $c_1 < c_2$, then by Rolle's theorem, there exists at least one point $c_3 \in (c_1, c_2)$ such that:
        \[            f''(c_3) = 0
        \]
        \item If $c_1 > c_2$, then by Rolle's theorem, there exists at least one point $c_3 \in (c_2, c_1)$ such that:
        \[            f''(c_3) = 0
        \]
    \end{itemize}
    Therefore, in both cases, we have found at least two points $c_2$ and $c_3$ in the interval $(0,4)$ such that:
    \[        f''(c_2) = 0 \quad \text{and} \quad f''(c_3) = 0
    \]
\end{eg}

\begin{eg}
    Let's apply Rolle's theorem on the function:
    \[
        f(x) = \begin{cases}
            x^2 \cos\left(\frac{1}{x}\right), &x \neq 0 \\
            0, &x = 0
        \end{cases}
    \]
    defined on the interval $[-\frac{1}{2}, \frac{1}{2}]$. We first need to show that $f$ is continuous on $[-\frac{1}{2}, \frac{1}{2}]$:
    \[
        \lim_{x \to 0} f(x) = \lim_{x \to 0} x^2 \cos\left(\frac{1}{x}\right) = 0 = f(0)
    \]
    Thus, $f$ is continuous on $[-\frac{1}{2}, \frac{1}{2}]$. Next, we need to show that $f$ is differentiable on $(-\frac{1}{2}, \frac{1}{2})$. For $x \neq 0$, we have:
    \[        f'(x) = 2x \cos\left(\frac{1}{x}\right) + x^2 \sin\left(\frac{1}{x}\right) \cdot \frac{1}{x^2} = 2x \cos\left(\frac{1}{x}\right) + \sin\left(\frac{1}{x}\right) \]
    For $x = 0$, we have:
    \[        f'(0) = \lim_{x \to 0} \frac{f(x) - f(0)}{x - 0} = \lim_{x \to 0} \frac{x^2 \cos\left(\frac{1}{x}\right)}{x} = \lim_{x \to 0} x \cos\left(\frac{1}{x}\right) = 0
    \]
    Thus, $f$ is differentiable on $(-\frac{1}{2}, \frac{1}{2})$. Finally, we have:
    \[        f\left(-\frac{1}{2}\right) = \left(-\frac{1}{2}\right)^2 \cos(-2) = \frac{1}{4} \cos(2) \]
    \[        f\left(\frac{1}{2}\right) = \left(\frac{1}{2}\right)^2 \cos(2) = \frac{1}{4} \cos(2) \]
    Since $f\left(-\frac{1}{2}\right) = f\left(\frac{1}{2}\right)$, by Rolle's theorem, there exists at least one point $c \in \left(-\frac{1}{2}, \frac{1}{2}\right)$ such that:
    \[        f'(c) = 0
    \]
    Let's solve the equation $f'(c) = 0$:
    \[        2c \cos\left(\frac{1}{c}\right) + \sin\left(\frac{1}{c}\right) = 0 \]
    Let $t = \frac{1}{c}$. Thus, we have:
    \[        \sin t + \frac{2}{t} \cos t = 0 \]
    Since if $\cos t = 0$ then $\sin t \neq 0$, thus $\cos t = 0$ cannot be a solution, thus allowing us to divide by $\cos t$, we have:
    \[        \tan t = -\frac{2}{t} \]
    We have that $x \in (-\frac{1}{2}, \frac{1}{2}) - \{0\}$, thus $t \in (-\infty, -2) \cup (2, +\infty)$. The function $\tan t$ is continuous and periodic with period $\pi$, and the function $-\frac{2}{t}$ is continuous and strictly increasing on the intervals $(-\infty, 0)$ and $(0, +\infty)$. Therefore, the equation $\tan t = -\frac{2}{t}$ has an infinite number of solutions in the intervals $(-\infty, -2)$ and $(2, +\infty)$. Thus, the equation $f'(c) = 0$ has an infinite number of solutions in the interval $\left(-\frac{1}{2}, \frac{1}{2}\right)$.
    % TODO: maybe add graph of tan y = -2/y
\end{eg}
Remark that $f(x)$ is derivable on $(-\frac{1}{2}, \frac{1}{2})$ but $f$ is not of class $C^1((-\frac{1}{2}, \frac{1}{2}))$ since $f'$ is not continuous at $x = 0$ (due to the term $\sin(\frac{1}{x})$).

\subsection{Mean Value Theorem}
\begin{theorem}[Mean Value Theorem]
    Let $f: [a,b] \to \mathbb{R}$ be a function that is continuous on $[a,b]$ and differentiable on $(a,b)$. Then, there exists at least one point $c \in (a,b)$ such that:
    \[        f'(c) = \frac{f(b) - f(a)}{b - a} \]
\end{theorem}
\begin{proof}
    Remark that the line passing through the points $(a, f(a))$ and $(b, f(b))$ has the equation:
    \[        y = f(a) + \frac{f(b) - f(a)}{b - a} (x - a) \]
    Let's define the function $g: [a,b] \to \mathbb{R}$ as follows:
    \[        g(x) = f(x) - \left[f(a) + \frac{f(b) - f(a)}{b - a} (x - a)\right] \]
    The function $g$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Moreover, we have:
    \[        g(a) = f(a) - f(a) = 0 \]
    \[        g(b) = f(b) - \left[f(a) + \frac{f(b) - f(a)}{b - a} (b - a)\right] = f(b) - f(b) = 0 \]
    Thus, by Rolle's theorem, there exists at least one point $c \in (a,b)$ such that:
    \[        g'(c) = 0 \]
    We have:
    \[        g'(x) = f'(x) - \frac{f(b) - f(a)}{b - a} \]
    Therefore, we have:
    \[        g'(c) = f'(c) - \frac{f(b) - f(a)}{b - a} = 0 \implies f'(c) = \frac{f(b) - f(a)}{b - a} \]
    % TODO: maybe add graph - w11c1 @ 29:40
\end{proof}
Remark that the Mean Value Theorem generalizes Rolle's theorem. Indeed, if $f(a) = f(b)$, then:
\[    f'(c) = \frac{f(b) - f(a)}{b - a} = 0 \]
The following holds as a direct consequence of the Mean Value Theorem:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item Let $f: [a,b] \to \mathbb{R}$ continuous on $[a,b]$, differentiable on $(a,b)$ and such that $f'(x) = 0$ for all $x \in (a,b)$. Then, $f$ is constant on $[a,b]$.
    \item Let $f(x)$ and $g(x)$ be two continuous functions on $[a,b]$ and differentiable on $(a,b)$ such that $f'(x) = g'(x)$ for all $x \in (a,b)$. Then, there exists a constant $C \in \mathbb{R}$ such that:
    \[        f(x) = g(x) + C \quad \forall x \in [a,b] \]
    \item If $f'(x) \geq 0$ (respectively $f'(x) \leq 0$) for all $x \in (a,b)$, then $f$ is increasing (respectively decreasing) on $[a,b]$.
    \item If $f'(x) > 0$ (respectively $f'(x) < 0$) for all $x \in (a,b)$, then $f$ is strictly increasing (respectively strictly decreasing) on $[a,b]$.
\end{itemize}
\begin{proof}
    Let's prove the third and fourth points. Let $f: [a,b] \to \mathbb{R}$ be a function continuous on $[a,b]$ and differentiable on $(a,b)$. \\
    \textbf{Third point:} If $f'(x) \geq 0$ for all $x \in (a,b)$, then for any $x_1, x_2 \in [a,b]$ such that $x_1 < x_2$, by the Mean Value Theorem, there exists at least one point $c \in (x_1, x_2)$ such that:
    \[        f'(c) = \frac{f(x_2) - f(x_1)}{x_2 - x_1} \]
    Since $f'(c) \geq 0$, we have:
    \[        \frac{f(x_2) - f(x_1)}{x_2 - x_1} \geq 0 \implies f(x_2) - f(x_1) \geq 0 \implies f(x_2) \geq f(x_1) \]
    Thus, $f$ is increasing on $[a,b]$. \\
    \textbf{Fourth point:} If $f'(x) > 0$ for all $x \in (a,b)$, then for any $x_1, x_2 \in [a,b]$ such that $x_1 < x_2$, by the Mean Value Theorem, there exists at least one point $c \in (x_1, x_2)$ such that:
    \[        f'(c) = \frac{f(x_2) - f(x_1)}{x_2 - x_1} \]
    Since $f'(c) > 0$, we have:
    \[        \frac{f(x_2) - f(x_1)}{x_2 - x_1} > 0 \implies f(x_2) - f(x_1) > 0 \implies f(x_2) > f(x_1) \]
    Thus, $f$ is strictly increasing on $[a,b]$.
\end{proof}
Remark that the converse of the third and fourth points is not necessarily true. For example, the function $f(x) = x^3$ is strictly increasing on $\mathbb{R}$, but its derivative $f'(x) = 3x^2$ is equal to 0 at $x = 0$.

\begin{eg}
    Let's show that the equation $f(x) = \ln x + e^x = 0$ ($f \in C^{\infty}((0, \infty))$) has exactly one solution, $x > 0$. Let's first show the existence of a solution. We have:
    \[        f(e) = \ln(e) + e^e = 1 + e^e > 0 \]
    \[        f(e^{-100}) = \ln(e^{-100}) + e^{e^{-100}} = -100 + e^{e^{-100}} < 0 \]
    Since $f$ is continuous on $(0, \infty)$, by the Intermediate Value Theorem, there exists at least one point $c \in (e^{-100}, e)$ such that:
    \[        f(c) = 0 \]
    Now to show the uniqueness, let's suppose that there are two distinct solutions $c_1, c_2 \in (0, \infty)$ such that $f(c_1) = f(c_2) = 0$ and $c_1 < c_2$. By the Mean Value Theorem, there exists at least one point $d \in (c_1, c_2)$ such that:
    \[        f'(d) = \frac{f(c_2) - f(c_1)}{c_2 - c_1} = \frac{0 - 0}{c_2 - c_1} = 0 \]
    However, we have:
    \[        f'(x) = \frac{1}{x} + e^x > 0 \quad \forall x \in (0, \infty) \]
    This is a contradiction. Therefore, the equation $f(x) = 0$ has exactly one solution $x > 0$.
\end{eg}

\subsection{L'Hôpital's Rule}
\begin{theorem}
    Let $f, g: [a,b] \to \mathbb{R}$ such that they are continuous on $[a,b]$, differentiable on $(a,b)$ and $g'(x) \neq 0$ on $(a,b)$. Then, there exists at least one point $c \in (a,b)$ such that:
    \[        \frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)} \]
\end{theorem}

\begin{theorem}[L'Hôpital's Rule]
    Let $f, g: \{x \in I, x_0 \neq x\} \to \mathbb{R}$ be two functions differentiable on $I - \{x_0\}$. Suppose that:
    \[
        g(x) \neq 0, g'(x) \neq 0 \quad \forall x \in I - \{x_0\}
    \]
    and:
    \[        \lim_{x \to x_0} f(x) = \lim_{x \to x_0} g(x) \in \{0, +\infty, -\infty\} \]
    If the limit:
    \[        \lim_{x \to x_0} \frac{f'(x)}{g'(x)} = L \in \mathbb{R} \cup \{\pm \infty\} \]
    exists, then:
    \[        \lim_{x \to x_0} \frac{f(x)}{g(x)} = L \]
\end{theorem}
\begin{proof}
    Let's only prove the case where:
    \[        \lim_{x \to x_0} f(x) = \lim_{x \to x_0} g(x) = 0 \]
    Then, $g(x_0) = f(x_0) = 0$ and both $f,g$ are continuous on the right of $x_0$. For any $x \in (a,b)$, by the previous theorem, there exists at least one point $c(x) \in (a,x)$ such that:
    \[        \frac{f'(c(x))}{g'(c(x))} = \frac{f(x) - f(x_0)}{g(x) - g(x_0)} = \frac{f(x)}{g(x)} \]
    Thus, we have:
    \[        \frac{f(x)}{g(x)} = \frac{f'(c(x))}{g'(c(x))} \]
    Taking the limit as $x \to x_0$, we have $c(x) \to x_0$, thus:
    \[        \lim_{x \to x_0} \frac{f(x)}{g(x)} = \lim_{c(x) \to x_0} \frac{f'(c(x))}{g'(c(x))} = L \]
\end{proof}
Remark that:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item L'Hôpital's Rule can also be applied when $x_0 = \pm \infty$.
    \item If the limit $\lim_{x \to x_0} \frac{f'(x)}{g'(x)}$ does not exist, it does not imply that $\lim_{x \to x_0} \frac{f(x)}{g(x)}$ does not exist.
\end{itemize}

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to \infty} \frac{x - \sin x}{x + \sin x}
    \]
    We have:
    \[        \lim_{x \to \infty} \frac{x - \sin x}{x + \sin x} = \lim_{x \to \infty} \frac{1 - \frac{\sin x}{x}}{1 + \frac{\sin x}{x}} = 1 \]
    Alternatively, we can apply L'Hôpital's Rule. We have:
    \[        \lim_{x \to \infty} \frac{x - \sin x}{x + \sin x} \overset{\text{BH}}{=} \lim_{x \to \infty} \frac{1 - \cos x}{1 + \cos x} \]
    However, this limit does not exist since $\cos x$ oscillates between $-1$ and $1$ as $x \to \infty$. Thus, L'Hôpital's Rule cannot be applied in this case.
\end{eg}

\begin{eg}
    Let compute the limit:
    \[
        \lim_{x \to \infty} \frac{\ln x}{x^{\alpha}} \quad \alpha > 0
    \]
    We have:
    \[        \lim_{x \to \infty} \frac{\ln x}{x^{\alpha}} \overset{\text{BH}}{=} \lim_{x \to \infty} \frac{\frac{1}{x}}{\alpha x^{\alpha - 1}} = \lim_{x \to \infty} \frac{1}{\alpha x^{\alpha}} = 0 \quad \forall \alpha > 0
    \]
\end{eg}

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to 0} \frac{\ln (1 + x)}{x} 
    \]
    We have:
    \[        \lim_{x \to 0} \frac{\ln (1 + x)}{x} \overset{\text{BH}}{=} \lim_{x \to 0} \frac{\frac{1}{1 + x}}{1} = 1
    \]
\end{eg}

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to 0} \left(\cos (2x)\right)^{\frac{3}{x^2}}
    \]
    We have:
    \[        \lim_{x \to 0} \left(\cos (2x)\right)^{\frac{3}{x^2}} = \lim_{x \to 0} e^{\frac{3}{x^2} \ln(\cos(2x))} = e^{\lim_{x \to 0} \frac{3 \ln(\cos(2x))}{x^2}} \]
    Let's show that we can apply L'Hôpital's Rule to compute the limit in the exponent. We have:
    \[        g(x) = x^2 \quad \implies \quad g(x) \neq 0 \ \text{in a neighborhood of } 0
    \]
    \[        g'(x) = 2x \quad \implies \quad g'(x) \neq 0 \ \text{in a neighborhood of } 0
    \]
    We also have:
    \[        \lim_{x \to 0} 3 \ln(\cos(2x)) = 3 \ln(1) = 0 = \lim_{x \to 0} x^2
    \]
    Thus, we have an indeterminate form of type $\frac{0}{0}$ and we can apply L'Hôpital's Rule. We have:
    \[        \lim_{x \to 0} \frac{3 \ln(\cos(2x))}{x^2} \overset{\text{BH}}{=} \lim_{x \to 0} \frac{\frac{3}{\cos (2x)} \cdot \left(- \sin(2x)\right) \cdot 2}{2x} = \lim_{x \to 0} \frac{\sin(2x)}{2x} \cdot \frac{-6}{\cos(2x)} = -6
    \]
    Therefore, we have:
    \[        \lim_{x \to 0} \left(\cos (2x)\right)^{\frac{3}{x^2}} = e^{-6} \]
\end{eg}

\section{Taylor's Series}
\begin{theorem}[Taylor's Theorem]
    Let $f: I \to \mathbb{R}$ be a function of class $C^{n + 1}$ on the interval $I$ and let $x_0 \in I$. Then, for any $x \in I$, there exists a point $c \in (x_0, x)$ such that:
    \[
        f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2!}(x - x_0)^2 + \ldots + \frac{f^{(n)}(x_0)}{n!}(x - x_0)^n + R_n(x)
    \]
    where:
    \[        R_n(x) = \frac{f^{(n+1)}(c)}{(n+1)!}(x - x_0)^{n+1}
    \]
\end{theorem}
Remark that the term without the remainder $R_n(x)$ is called the Taylor polynomial of degree $n$ of the function $f$ at the point $x_0$. The remainder term $R_n(x)$ represents the error made when approximating $f(x)$ by its Taylor polynomial of degree $n$. \\
Note the following:
\[
    T_f'(x) \Big|_{x = x_0} = f'(x_0) \quad \text{and} \quad T_f''(x) \Big|_{x = x_0} = f''(x_0) \quad \ldots \quad T_f^{(n)}(x) \Big|_{x = x_0} = f^{(n)}(x_0)
\]
Making the Taylor polynomial $T_f(x)$ of degree $n$ at the point $x_0$ the best approximation of $f(x)$ in the sense that it matches the first $n$ derivatives of $f$ at the point $x_0$. 

\begin{eg}
    Let's compute the Taylor polynomial of $f(x) = \sin x$ at the point $x_0 = 0$. We have:
    \[
        f'(x) = \cos x \quad f''(x) = -\sin x \quad f'''(x) = -\cos x \quad f^{(4)}(x) = \sin x \quad \ldots
    \]
    and:
    \[
        f'(0) = 1 \quad f''(0) = 0 \quad f'''(0) = -1 \quad f^{(4)}(0) = 0 \quad \ldots
    \]
    Thus, the Taylor polynomial of degree $n$ at the point $x_0 = 0$ is given by:
    \[
        \sin x = \frac{1}{1!}(x - 0) + \frac{-1}{3!}(x - 0)^3 + \frac{1}{5!}(x - 0)^5 + \ldots + \frac{(-1)^{n}}{(2n + 1)!}(x - 0)^{2n + 1} + R_{2n + 1}(x)
    \]
    Or more compactly:
    \[        \sin x = \sum_{k=0}^{n} \frac{(-1)^{k}}{(2k + 1)!} (x - 0)^{2k + 1} + R_{2n + 1}(x)
    \]
\end{eg}

\begin{eg}
    Let's compute the Taylor polynomial of $f(x) = \cos x$ at the point $x_0 = 0$. We have:
    \[        f'(x) = -\sin x \quad f''(x) = -\cos x \quad f'''(x) = \sin x \quad f^{(4)}(x) = \cos x \quad \ldots
    \]
    and:
    \[        f'(0) = 0 \quad f''(0) = -1 \quad f'''(0) = 0 \quad f^{(4)}(0) = 1 \quad \ldots
    \]
    Thus, the Taylor polynomial of degree $n$ at the point $x_0 = 0$ is given by:
    \[        \cos x = 1 + \frac{-1}{2!}(x - 0)^2 + \frac{1}{4!}(x - 0)^4 + \ldots + \frac{(-1)^{n}}{(2n)!}(x - 0)^{2n} + R_{2n}(x)
    \]
    Or more compactly:
    \[        \cos x = \sum_{k=0}^{n} \frac{(-1)^{k}}{(2k)!} (x - 0)^{2k} + R_{2n}(x)
    \]
    Another method to obtain the Taylor polynomial of $\cos x$ is to use the Taylor polynomial of $\sin x$ and the relation:
    \[        \frac{d}{dx}(\sin x) = \cos x
    \]
    Since:
    \[
        \sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \ldots + \frac{(-1)^{n}}{(2n + 1)!} x^{2n + 1} + R_{2n + 1}(x)
    \]
    If we differentiate this expression term by term, we obtain:
    \[        \cos x = 1 - \frac{3x^2}{3!} + \frac{5x^4}{5!} - \ldots + \frac{(-1)^{n}(2n + 1)x^{2n}}{(2n + 1)!} + R_{2n + 1}'(x)
    \]
    Simplifying the coefficients, we get:
    \[        \cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \ldots + \frac{(-1)^{n}}{(2n)!} x^{2n} + R_{2n + 1}'(x)
    \]
\end{eg}
Remark that the Taylor polynomial of degree $n$ at the point $x_0 = 0$ is also called the Maclaurin polynomial of degree $n$.

\subsection{Expansions}
\begin{definition}[Expansion]
    Let $f: E \to F$ be a function defined in a neighborhood of $x_0 \in E$. We say that $f$ admits an expansion of order $n$ at the point $x_0$ if there exist constants $a_0, a_1, \ldots, a_n \in F$ and a function $\epsilon: E \to F$ such that for all $x \in E, x \neq x_0$, the following holds:
    \[        f(x) = a_0 + a_1 (x - x_0) + \ldots + a_n (x - x_0)^n + (x - x_0)^n \epsilon(x)
    \]
    with:
    \[        \lim_{x \to x_0} \epsilon(x) = 0
    \]
\end{definition}
Note that $R_n(x) = (x - x_0)^n \epsilon(x)$ is called the reminder of the expansion, and $P_n(x) = a_0 + a_1 (x - x_0) + \ldots + a_n (x - x_0)^n$ is called the polynomial part of the expansion.

\begin{theorem}
    If $f: E \to F$ admits an expansion of order $n$ at the point $x_0 \in E$, then the expansion is unique.
\end{theorem}
\begin{proof}
    Let's suppose that $f$ admits two expansions of order $n$ at the point $x_0$:
    \[        f(x) = a_0 + a_1 (x - x_0) + \ldots + a_n (x - x_0)^n + (x - x_0)^n \epsilon_1(x)
    \]
    and:
    \[        f(x) = b_0 + b_1 (x - x_0) + \ldots + b_n (x - x_0)^n + (x - x_0)^n \epsilon_2(x)
    \]
    with:
    \[        \lim_{x \to x_0} \epsilon_1(x) = 0 \quad \text{and} \quad \lim_{x \to x_0} \epsilon_2(x) = 0
    \]
    Let $k$ be the smallest integer such that $a_k \neq b_k$. Thus, we have:
    \[
        f(x) - f(x) = (a_k - b_k)(x - x_0)^k + (a_{k+1} - b_{k+1})(x - x_0)^{k+1} + \ldots + (x - x_0)^n (\epsilon_1(x) - \epsilon_2(x))
    \]
    Dividing both sides by $(x - x_0)^k$, we get:
    \[
        0 = (a_k - b_k) + (a_{k+1} - b_{k+1})(x - x_0) + \ldots + (x - x_0)^{n - k} (\epsilon_1(x) - \epsilon_2(x))
    \]
    Taking the limit as $x \to x_0$, we have:
    \[        0 = (a_k - b_k) + 0 + \ldots + 0 = a_k - b_k
    \]
    This is a contradiction since we assumed that $a_k \neq b_k$. Therefore, the expansion is unique.
\end{proof}
Let's show that if $f: I \to \mathbb{R}$ is a function of class $C^{n + 1}$ on the interval $I$, then Taylor's theorem implies that $f$ admits an expansion of order $n$ at any point $x_0 \in I$ i.e. if the reminder of Taylor's theorem can be written as:
\[
    R_n(x) = (x - x_0)^n \epsilon(x)
\]
with:
\[    \lim_{x \to x_0} \epsilon(x) = 0
\]
Then Taylor's theorem is an expansion. With Taylor's theorem, the reminder is:
\[
    R_n(x) = \frac{f^{(n+1)}(c)}{(n+1)!}(x - x_0)^{n+1}
\]
Let $\epsilon (x) = \frac{f^{(n+1)}(c)}{(n+1)!}(x - x_0)$. Thus by computing the limit:
\[
    \lim_{x \to x_0} \left|\frac{f^{(n+1)}(c)}{(n+1)!}(x - x_0)\right| \leq \lim_{x \to x_0} M \left|\frac{(x - x_0)}{(n + 1)!}\right| = 0
\]
Remark that:
\begin{itemize}[itemsep=1pt,label=$\circ$]
    \item to use Taylor's theorem, $f$ only needs to be of class $C^{n}$ and the remainder can be expressed as $R_n(x) = (x - x_0)^n \epsilon(x)$ (Note that the proof above does not hold, but the result is still valid. The proof for this argument is much more complicated).
    \item $f$ might admit an expansion even if Taylor's theorem cannot be applied.
\end{itemize}

\begin{eg}
    Let's compute the expension of $f(x) = e^x$ at the point $x_0 = 0$. We have:
    \[        f^{(n)}(x) = e^x \quad \implies \quad f^{(n)}(0) = 1
    \]
    Thus, the expansion of $f$ at the point $x_0 = 0$ is given by:
    \[        e^x = 1 + \frac{1}{1!} x + \frac{1}{2!} x^2 + \ldots + \frac{1}{n!} x^n + R_n(x)
    \]
    with:
    \[        R_n(x) = \frac{e^c}{(n+1)!} x^{n+1}
    \]
    where $c$ is a point between $0$ and $x$. Thus, we have:
    \[        \epsilon(x) = \frac{e^c}{(n+1)!} x \quad \implies \quad \lim_{x \to 0} \epsilon(x) = 0
    \]
    Therefore, the expansion of $f$ at the point $x_0 = 0$ is:
    \[        e^x = 1 + \frac{1}{1!} x + \frac{1}{2!} x^2 + \ldots + \frac{1}{n!} x^n + (x - 0)^n \epsilon(x)
    \]
\end{eg}

\begin{eg}
    Let's compute a expansion for $f(x) = \frac{1}{1 - x}$ of order $n$ at the point $x_0 = 0$. We could compute the expansion using Taylor's theorem, but a easier way would be to transform it into a geometric series. Let $|x| < 1$, we have:
    \[
        \frac{1}{1-x} = \sum_{n = 0}^{\infty} x^n
    \]
    thus we have:
    \[
        \frac{1}{1-x} = \underbrace{1 + x + x^2 + \ldots + x^n}_{P_n(x)} + \underbrace{x^{n + 1} + x^{n + 2} + \ldots}_{x^n \epsilon(x)}
    \]
    with:
    \[
        x^n \epsilon(x) = x^{n + 1} + x^{n + 2} + \ldots = x^{n + 1} \left(1 + x + x^2 + \ldots\right) = x^{n + 1} \cdot \sum_{k = 0}^{\infty} x^k = x^{n + 1} \cdot \frac{1}{1 - x}
    \]
    and thus the limit of $\epsilon(x)$ as $x \to 0$ is:
    \[        \lim_{x \to 0} \epsilon(x) = \lim_{x \to 0} \frac{1}{1 - x} = 0 \]
    Therefore, the expansion of $f$ at the point $x_0 = 0$ is:
    \[        \frac{1}{1-x} = 1 + x + x^2 + \ldots + x^n + (x - 0)^n \epsilon(x)
    \]
\end{eg}

\begin{eg}
    Let's compute the expansion of $f(x) = \sin(x^2)$ of order $n$ at the point $x_0 = 0$. We have:
    \[
        \sin x = \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2k + 1)!} x^{2k + 1}
    \]
    Thus, we have:
    \[        \sin(x^2) = \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2k + 1)!} (x^2)^{2k + 1} = \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2k + 1)!} x^{4k + 2}
    \]
\end{eg}

\begin{eg}
    Let's compute the limit:
    \[
        \lim_{x \to 0} \frac{\sin x}{x}
    \]
    Using the expansion of $\sin x$ at the point $x_0 = 0$, we have:
    \[        \sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \ldots + \frac{(-1)^{n}}{(2n + 1)!} x^{2n + 1} + R_{2n + 1}(x)
    \]
    Thus, we have:
    \begin{align*}
        \frac{\sin x}{x} &= \frac{x - \frac{x^3}{3!} + \frac{x^5}{5!} - \ldots + \frac{(-1)^{n}}{(2n + 1)!} x^{2n + 1} + R_{2n + 1}(x)}{x} \\
        &= 1 - \frac{x^2}{3!} + \frac{x^4}{5!} - \ldots + \frac{(-1)^{n}}{(2n + 1)!} x^{2n} + \frac{R_{2n + 1}(x)}{x}
    \end{align*}
    Taking the limit as $x \to 0$, we have:
    \[        \lim_{x \to 0} \frac{\sin x}{x} = 1 - 0 + 0 - \ldots + 0 + 0 = 1
    \]
\end{eg}

\section{Function Study}
Recall that if $f: I \to F$ is differentiable on $I$ and $f$ has a local extremum at $x_0 \in I$, then:
\[    f'(x_0) = 0
\]

\begin{theorem}[Sufficient Condition for Local Extremum]
    Let $f: I \to F$ be a function of class $C^n$ on $I$ such that for some even $n \in \mathbb{N}$:
    \[
        f'(x_0) = f''(x_0) = \ldots = f^{(n-1)}(x_0) = 0 \quad \text{and} \quad f^{(n)}(x_0) \neq 0
    \]
    Then:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item if $f^{(n)}(x_0) > 0$, then $f$ has a local minimum at $x_0$.
        \item if $f^{(n)}(x_0) < 0$, then $f$ has a local maximum at $x_0$.
    \end{itemize}
\end{theorem}
\begin{proof}
    Let's prove the first point. The second point can be proved similarly. Since $f$ is of class $C^n$ on $I$, by Taylor's theorem, for any $x \in I$, there exists a point $c$ between $x_0$ and $x$ such that:
    \[
        f(x) = f(x_0) + \frac{f^{(n)}(c)}{n!} (x - x_0)^n
    \]
    Since $f^{(n)}$ is continuous at $x_0$ and $f^{(n)}(x_0) > 0$, there exists a neighborhood $U$ of $x_0$ such that for all $c \in U$, we have:
    \[        f^{(n)}(c) > 0
    \]
    Thus, for any $x \in U$, we have:
    \[        f(x) - f(x_0) = \frac{f^{(n)}(c)}{n!} (x - x_0)^n > 0
    \]
    since $(x - x_0)^n \geq 0$ for even $n$. Therefore, $f(x) > f(x_0)$ for all $x \in U, x \neq x_0$. Thus, $f$ has a local minimum at $x_0$.
\end{proof}

\begin{definition}[Inflection Point]
    Let $f: E \to F$ be a function differentiable on $a \in E$. Let $l(x) = f(a) + f'(a)(x - a)$ be the tangent line to the graph of $f$ at the point $(a, f(a))$. Let's define the following function:
    \[
        \psi(x) = f(x) - l(x) = f(x) - f(a) - f'(a)(x - a)
    \]
    If $\psi(x)$ changes sign at $x = a$, then $(a, f(a))$ is called an inflection point of $f$.
\end{definition}

\begin{theorem}[Sufficient Condition for Inflection Point]
    Let $f: I \to F$ be a function of class $C^n$ on $I$ such that for some odd $n \in \mathbb{N}$ ($n > 1$):
    \[
        f''(x_0) = f'''(x_0) = \ldots = f^{(n-1)}(x_0) = 0 \quad \text{and} \quad f^{(n)}(x_0) \neq 0
    \]
    Then, $(x_0, f(x_0))$ is an inflection point of $f$.
\end{theorem}
\begin{proof}
    Since $f$ is of class $C^n$ on $I$, by Taylor's theorem, for any $x \in I$, there exists a point $c$ between $x_0$ and $x$ such that:
    \[
        f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{f^{(n)}(c)}{n!} (x - x_0)^n
    \]
    Thus, we have:
    \[
        \psi(x) = f(x) - f(x_0) - f'(x_0)(x - x_0) = \frac{f^{(n)}(c)}{n!} (x - x_0)^n
    \]
    Since $f^{(n)}$ is continuous at $x_0$ and $f^{(n)}(x_0) \neq 0$, there exists a neighborhood $U$ of $x_0$ such that for all $c \in U$, we have:
    \[        f^{(n)}(c) \neq 0
    \]
    Thus, for any $x \in U, x \neq x_0$, we have:
    \[        \psi(x) = \frac{f^{(n)}(c)}{n!} (x - x_0)^n
    \]
    Since $n$ is odd, $(x - x_0)^n$ changes sign as $x$ crosses $x_0$. Therefore, $\psi(x)$ changes sign at $x = x_0$. Thus, $(x_0, f(x_0))$ is an inflection point of $f$.
\end{proof}

\subsection{Convexity and Concavity}
\begin{definition}[Convexity]
    Let $f: I \to \mathbb{R}$ be a function defined on an interval $I$. It is said that $f$ is convex on $I$ if for any $a < b \in I$, the graph of $f(x)$ lies below the chord connecting the points $(a, f(a))$ and $(b, f(b))$. In other words, for any $x \in (a,b)$, we have:
    \[        f(x) \leq f(a) + \frac{f(b) - f(a)}{b - a} (x - a)
    \]
\end{definition}

\begin{definition}[Concavity]
    Let $f: I \to \mathbb{R}$ be a function defined on an interval $I$. It is said that $f$ is concave on $I$ if for any $a < b \in I$, the graph of $f(x)$ lies above the chord connecting the points $(a, f(a))$ and $(b, f(b))$. In other words, for any $x \in (a,b)$, we have:
    \[        f(x) \geq f(a) + \frac{f(b) - f(a)}{b - a} (x - a)
    \]
\end{definition}

\begin{theorem}
    Let $f: I \to \mathbb{R}$ be a function of class $C^2$ on the interval $I$. Then:
    \[
        f \text{ is convex on } I \quad \iff \quad f''(x) \geq 0 \text{ on } I \quad \iff \quad f'(x) \text{ is non-decreasing on } I
    \]
    \[
        f \text{ is concave on } I \quad \iff \quad f''(x) \leq 0 \text{ on } I \quad \iff \quad f'(x) \text{ is non-increasing on } I
    \]
\end{theorem}

\section{Exercices}
This section gathers a selection of exercises related to Chapter \thechapter, taken from weekly assignments, past exams, textbooks, and other sources. The origin of each exercise will be indicated at its beginning.

\begin{exercise}[Quizz of lecture 20]
    Let $f: \mathbb{R} \to \mathbb{R}$ be a function of class $C^1(\mathbb{R})$, such that $f'(x) > 5$ for all $x > 0$. Which of the following statements are true?
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $\exists x > 0$ such that $f(x) = 5$
        \item $\forall x > 0$ we have $f(x) > f(0) + 5x$
        \item $\forall x > 0$ we have $f(x) > 5 f(0)$
        \item $\exists x > 0$ such that $f(x) = 5 - f(0)$
        \item $\forall x > 0$ we have $f(x) - 5x > 0$
    \end{itemize}
    \Answer
    The correct statement is: \textbf{2.} \\
    By the Mean Value Theorem, for any $x > 0$, there exists at least one point $c \in (0,x)$ such that:
    \[        f'(c) = \frac{f(x) - f(0)}{x - 0} \]
    Since $f'(c) > 5$, we have:
    \[        \frac{f(x) - f(0)}{x} > 5 \implies f(x) - f(0) > 5x \implies f(x) > f(0) + 5x \]
\end{exercise}