\chapter{Probability}

\section{Basic Concepts}
\begin{definition}[Experiment]
    An experiment is any process that can be repeated and has a well-defined set of possible outcomes.
\end{definition}

\begin{definition}[Sample Space $S$]
    The sample space \( S \) of an experiment is the set of all possible outcomes of that experiment.
\end{definition}

\begin{definition}[Event]
    An event is any subset of the sample space \( S \). An event occurs if the outcome of the experiment is an element of that subset.
\end{definition}

\begin{eg}
    When throwing a pair of dice, the sample space is:
    \[
        S = \{(1,1), (1,2), \ldots, (6,6)\}
    \]
    An example event $E$ could be "the sum of the two dice is 3", then the set of outcomes in $E$ is:
    \[
        E = \{(1,2), (2,1)\}
    \]
    Thus the event $E$ occurs if the outcome of the experiment is either (1,2) or (2,1).
\end{eg}

\section{Laplace Definition of Probability}
\begin{definition}[Laplace]
    The Laplace definition of probability states that if all outcomes in the sample space are equally likely, the probability of an event \( E \) is given by:
    \[
        P(E) = \frac{|E|}{|S|}
    \]
    where \( |E| \) is the number of outcomes in event \( E \) and \( |S| \) is the total number of outcomes in the sample space.
\end{definition}

\begin{eg}
    Continuing with the previous example of throwing a pair of dice, the total number of outcomes in the sample space is \( |S| = 36 \). The event \( E \) (sum of the two dice is 3) has \( |E| = 2 \) outcomes. Therefore, the probability of event \( E \) occurring is:
    \[
        P(E) = \frac{|E|}{|S|} = \frac{2}{36} = \frac{1}{18}
    \]
\end{eg}

\begin{eg}
    If two dices are rolled after each other, what is the probability to roll at least one 6? \\
    Let $E$ be the event "at least one 6 is rolled", the the possible outcome set is:
    \[
        E = \{(6, 1...6), (1...6, 6)\}
    \]
    By the principle of inclusion-exclusion, we have:
    \[
        |E| = 6 + 6 - 1 = 11
    \]
    More explicitly:
    \[
        E = \{(6,1), (6,2), (6,3), (6,4), (6,5), (6,6), (1,6), (2,6), (3,6), (4,6), (5,6)\}
    \]
    Thus the probability of rolling at least one 6 is:
    \[
        P(E) = \frac{|E|}{|S|} = \frac{11}{36}
    \]
\end{eg}

\begin{theorem}
    Let $E$ be an event in the sample space $S$. The probability of the event $\bar{E} = S - E$, the complementary event of $E$, is given by:
    \[
        P(\bar{E}) = 1 - P(E)
    \]
\end{theorem}
\begin{proof}
    Since \( E \) and \( \bar{E} \) are complementary events, we have:
    \[
        |S| = |E| + |\bar{E}|
    \]
    Dividing both sides by \( |S| \), we get:
    \[
        1 = \frac{|E|}{|S|} + \frac{|\bar{E}|}{|S|}
    \]
    Therefore:
    \[
        P(E) + P(\bar{E}) = 1
    \]
    Rearranging gives:
    \[
        P(\bar{E}) = 1 - P(E)
    \]
\end{proof}

\begin{eg}
    A sequence of 10 bits is chosen randomly. What is the probability that at least one of these bits is 0? \\
    Let $E$ be the event "at least one bit is 0". The complementary event $\bar{E}$ is "all bits are 1". There is only one outcome in $\bar{E}$, which is the sequence "1111111111". The total number of possible outcomes in the sample space is \( |S| = 2^{10} = 1024 \). Thus, the probability of the complementary event is:
    \[
        P(\bar{E}) = \frac{1}{1024}
    \]
    Therefore, the probability of event \( E \) occurring is:
    \[
        P(E) = 1 - P(\bar{E}) = 1 - \frac{1}{1024} = \frac{1023}{1024}
    \]
\end{eg}

\begin{theorem}
    Let $E_1$ and $E_2$ be two events in the sample space $S$. The probability of the union of the two events is given by:
    \[
        P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)
    \]
\end{theorem}
\begin{proof}
    By the principle of inclusion-exclusion, we have:
    \[
        |E_1 \cup E_2| = |E_1| + |E_2| - |E_1 \cap E_2|
    \]
    Dividing both sides by \( |S| \), we get:
    \[
        \frac{|E_1 \cup E_2|}{|S|} = \frac{|E_1|}{|S|} + \frac{|E_2|}{|S|} - \frac{|E_1 \cap E_2|}{|S|}
    \]
    Therefore:
    \[
        P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)
    \]
\end{proof}

\begin{eg}
    What is the probability that a positive integer selected at random from the set of positive integers not exceeding $100$ is divisible by $2$ or $5$? \\
    Let $E_1$ be the event "the number is divisible by 2" and $E_2$ be the event "the number is divisible by 5". We have:
    \[
        |E_1| = 50, \quad |E_2| = 20, \quad |E_1 \cap E_2| = 10
    \]
    The total number of outcomes in the sample space is \( |S| = 100 \). Therefore, the probabilities are:
    \[
        P(E_1) = \frac{50}{100} = \frac{1}{2}, \quad P(E_2) = \frac{20}{100} = \frac{1}{5}, \quad P(E_1 \cap E_2) = \frac{10}{100} = \frac{1}{10}
    \]
    Using the formula for the union of two events, we have:
    \[
        P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2) = \frac{1}{2} + \frac{1}{5} - \frac{1}{10} = \frac{5}{10} + \frac{2}{10} - \frac{1}{10} = \frac{6}{10} = \frac{3}{5}
    \]
\end{eg}

\subsection{Limitation of the Laplace Definition}
The Laplace definition of probability is limited to situations where all outcomes in the sample space are equally likely. In many real-world scenarios, this assumption does not hold true, and outcomes may have different probabilities of occurring. In such cases, more advanced probability theories and models, such as Bayesian probability or frequentist probability, are used to accurately represent and analyze the likelihood of events.

\section{Probability Distributions}
\begin{definition}[Probability Distribution]
    Let $S$ be a sample space of an experiment with a finite or countable number of outcomes. Each outcome $s$ is assigned a probability $P(s)$ such that:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item $0 \leq P(s) \leq 1$ for every outcome $s$ in $S$.
        \item The sum of the probabilities of all outcomes in $S$ is equal to 1:
        \[
            \sum_{s \in S} P(s) = 1
        \]
    \end{itemize}
    The function $P: S \to [0, 1]$ that assigns probabilities to outcomes is called a probability distribution.
\end{definition}

\begin{eg}
    Probability distribution for the sum of two rolling dice. The set of possible outcomes for the sum ranges from 2 to 12, thus $S = \{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\}$. The probabilities for each possible sum are not equal, as some sums can be achieved through multiple combinations of dice rolls, for example:
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Sum of 2: (1,1) $\to$ 1 way
        \item Sum of 3: (1,2), (2,1) $\to$ 2 ways
        \item Sum of 4: (1,3), (2,2), (3,1) $\to$ 3 ways
        \item $\ldots$
        \item Sum of 7: (1,6), (2,5), (3,4), (4,3), (5,2), (6,1) $\to$ 6 ways
        \item $\ldots$
        \item Sum of 12: (6,6) $\to$ 1 way
    \end{itemize}
    There are a total of 36 possible outcomes when rolling two dice. Therefore, the probabilities for each sum can be calculated as the number of ways to achieve that sum divided by 36, for example:
    \[
        P(2) = \frac{1}{36}, \quad P(3) = \frac{2}{36}, \quad P(4) = \frac{3}{36}, \quad \ldots
    \]
\end{eg}

\begin{eg}
    What probabilities should we assign to the outcomes $H$ (heads) and $T$ (tails) when the coin is biased so that heads come up twice as often as tails? \\
    Let $P(T) = x$. Since heads come up twice as often as tails, we have $P(H) = 2x$. According to the properties of probability distributions, we have:
    \[
        P(H) + P(T) = 1
    \]
    Substituting the values, we get:
    \[
        2x + x = 1 \quad \implies \quad 3x = 1 \quad \implies \quad x = \frac{1}{3}
    \]
    Therefore, the probabilities are:
    \[
        P(T) = \frac{1}{3}, \quad P(H) = \frac{2}{3}
    \]
\end{eg}

\begin{definition}[Uniform Distribution]
    Suppose that $S$ is a set with $n$ elements. The uniform distribution assigns the probability \( \frac{1}{n} \) to each element of \( S \). In other words, for every outcome \( s \) in \( S \):
    \[
        P(s) = \frac{1}{n}
    \]
\end{definition}

\begin{definition}[Probability of an Event]
    Let \( E \) be an event in the sample space \( S \) with a probability distribution \( P \). The probability of the event \( E \) is given by:
    \[
        P(E) = \sum_{s \in E} P(s)
    \]
    where the sum is taken over all outcomes \( s \) that belong to the event \( E \).
\end{definition}

\begin{eg}
    Suppose that a die is biased so that $3$ appears twice as often as any other number, but that the other five outcomes are equally likely. What is the probability that an odd number is rolled? \\
    Let $P(1) = P(5) = P(2) = P(4) = P(6) = x$ and $P(3) = 2x$. According to the properties of probability distributions, we have:
    \[
        P(1) + P(2) + P(3) + P(4) + P(5) + P(6) = 1
    \]
    Substituting the values, we get:
    \[
        x + x + 2x + x + x + x = 1 \quad \implies \quad 7x = 1 \quad \implies \quad x = \frac{1}{7}
    \]
    Therefore, the probabilities are:
    \[
        P(1) = P(5) = P(2) = P(4) = P(6) = \frac{1}{7}, \quad P(3) = \frac{2}{7}
    \]
    The event "rolling an odd number" consists of the outcomes \( \{1, 3, 5\} \). Thus, the probability of rolling an odd number is:
    \[
        P(\{1, 3, 5\}) = P(1) + P(3) + P(5) = \frac{1}{7} + \frac{2}{7} + \frac{1}{7} = \frac{4}{7}
    \]
\end{eg}

\begin{theorem}
    The probability of the complement of an event \( E \) in a sample space \( S \) with a probability distribution \( P \) is given by:
    \[
        P(\bar{E}) = 1 - P(E)
    \]
\end{theorem}
\begin{proof}
    Since \( E \) and \( \bar{E} \) are complementary events, we have:
    \[
        S = E \cup \bar{E} \quad \text{and} \quad E \cap \bar{E} = \emptyset
    \]
    Therefore, the probability of the sample space is:
    \[
        P(S) = P(E) + P(\bar{E}) = 1
    \]
    Rearranging gives:
    \[
        P(\bar{E}) = 1 - P(E)
    \]
\end{proof}

\begin{theorem}
    Let $E_1$ and $E_2$ be two events in the sample space $S$ with a probability distribution $P$. The probability of the union of the two events is given by:
    \[
        P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)
    \]
\end{theorem}
\begin{proof}
    By the principle of inclusion-exclusion, we have:
    \[
        P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)
    \]
\end{proof}

\begin{theorem}
    If $E_1, E_2, \ldots, E_n$ are mutually exclusive events in the sample space $S$ with a probability distribution $P$, then the probability of the union of these events is given by:
    \[
        P\left(\bigcup_{i=1}^{n} E_i\right) = \sum_{i=1}^{n} P(E_i)
    \]
\end{theorem}

\section{Conditional Probability}
\begin{definition}[Conditional Probability]
    The conditional probability of an event \( A \) given that another event \( B \) has occurred is denoted by \( P(A|B) \) and is defined as:
    \[
        P(A|B) = \frac{P(A \cap B)}{P(B)} \quad \text{provided that } P(B) > 0
    \]
\end{definition}

\begin{eg}
    When you roll a dice, what is the probability that the outcome is even?
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Without any additional knowledge, the probability is:
        \[
            P(\text{even}) = \frac{3}{6} = \frac{1}{2}
        \]
        \item If we know that the outcome is less than or equal to 3, then the possible outcomes are \( \{1, 2, 3\} \). The only even number in this set is 2. Therefore, the conditional probability is:
        \[
            P(\text{even} | \text{outcome} \leq 3) = \frac{\frac{1}{6}}{\frac{1}{2}} = \frac{1}{3}
        \]
    \end{itemize}
\end{eg}

\begin{eg}
    When you toss a coin $6$ times, what is the probability that the last toss is heads?
    \begin{itemize}[itemsep=1pt,label=$\circ$]
        \item Without any additional knowledge, the probability is:
        \[
            P(H) = \frac{2^5}{2^6} = \frac{1}{2}
        \]
        \item If we know that the first $5$ tosses were all heads. We have that:
        \[
            E \cap F = \{(H, H, H, H, H, H)\}
        \]
        and:
        \[
            F = \{(H, H, H, H, H, H), (H, H, H, H, H, T)\}
        \]
        Thus:
        \[
            P(F) = \frac{2^1}{2^6} = \frac{1}{2^5}
        \]
        Therefore, the conditional probability is:
        \[
            P(E|F) = \frac{P(E \cap F)}{P(F)} = \frac{\frac{1}{2^6}}{\frac{1}{2^5}} = \frac{1}{2}
        \]
    \end{itemize}
\end{eg}